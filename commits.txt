diff --git a/.gitignore b/.gitignore
index 8afd3293..819a0556 100644
--- a/.gitignore
+++ b/.gitignore
@@ -36,3 +36,4 @@ packages/*/coverage/
 # Generated files
 packages/cli/src/generated/
 .integration-tests/
+.aider*
diff --git a/.vscode/mcp.json b/.vscode/mcp.json
new file mode 100644
index 00000000..1e7506a8
--- /dev/null
+++ b/.vscode/mcp.json
@@ -0,0 +1,7 @@
+{
+    "servers": {
+        "gemini-cli": {
+            "url": "http://localhost:8765/mcp"
+        }
+    }
+}
\ No newline at end of file
diff --git a/README-GEMINI-MCP.md b/README-GEMINI-MCP.md
new file mode 100644
index 00000000..1f115660
--- /dev/null
+++ b/README-GEMINI-MCP.md
@@ -0,0 +1,308 @@
+# Gemini CLI - MCP / OpenAI Bridge Server (`@google/gemini-mcp-server`)
+
+`@google/gemini-mcp-server` is a versatile companion application designed to serve as a powerful extension for the `gemini-cli` ecosystem. It primarily fulfills two core roles:
+
+1.  **MCP (Model-Context Protocol) Server**: It hosts and exposes `gemini-cli`'s powerful built-in tools (e.g., `google_web_search`, file system tools) via a standard, discoverable protocol. This allows the core `gemini-cli` model to invoke these tools as needed.
+
+2.  **OpenAI-Compatible API Bridge**: It provides an endpoint compatible with the OpenAI Chat Completions API. This enables any third-party tool or application that supports the OpenAI API (such as [Open WebUI](https://github.com/open-webui/open-webui)) to seamlessly interact with the underlying Gemini model of `gemini-cli`, including full support for streaming responses.
+
+## Core Design Philosophy
+
+The server is built on a principle of **minimal modification and maximum reuse**. It is not a reimplementation of `gemini-cli`'s features but is instead intelligently built on top of the `@google/gemini-cli-core` package.
+
+By reusing the `Config` and `GeminiClient` classes from the core package, the `mcp-server` inherits all of the essential business logic, tool execution capabilities, and configuration management of the main CLI. This design ensures behavioral consistency and simplifies maintenance and extension.
+
+## Features
+
+-   **Hosts Native `gemini-cli` Tools**: Exposes the built-in tools (file system operations, web fetching, web search, etc.) to the `gemini-cli` model via the MCP protocol.
+-   **OpenAI API Compatibility**: Provides `/v1/chat/completions` and `/v1/models` endpoints, allowing third-party applications to interact with the Gemini model as if it were an OpenAI service.
+-   **Streaming Support**: Fully supports streaming responses, pushing real-time generation results from the Gemini model to the client via Server-Sent Events (SSE).
+-   **Flexible Model Configuration**: Allows a separate, default LLM model to be configured via an environment variable specifically for tools hosted by the server (e.g., for summarizing search results).
+-   **Inherited Configuration & Authentication**: Automatically uses the same settings and authentication state as your main `gemini-cli` setup.
+-   **Forced YOLO Mode**: Runs in a permanent "YOLO" mode, automatically approving all tool calls for streamlined, non-interactive use by other applications.
+
+## Architecture & Interaction Flow
+
+The `mcp-server` operates as a standalone component within the `gemini-cli` ecosystem with the following interaction flow:
+
+1.  **Configuration Loading**: On startup, the server loads user and workspace `settings.json` files and reads environment variables, just like the main `gemini-cli` application, to initialize an instance of the `@google/gemini-cli-core` `Config` class.
+2.  **Authentication**: The server **does not** handle its own authentication. It relies entirely on the established authentication state of `gemini-cli` (see the next section for details).
+3.  **MCP Service**: It starts an MCP server, which the `gemini-cli` can connect to when it needs to discover and execute tools.
+4.  **OpenAI Bridge**: It starts an Express web server that listens for API requests in the OpenAI format.
+5.  **Request Handling**:
+    -   When an OpenAI-formatted request is received, the server converts it into a format that `gemini-cli-core` can understand.
+    -   It uses the reused `Config` instance to get a `GeminiClient`.
+    -   A **new, isolated `GeminiChat` session** is created for each incoming API request to prevent conversation history from leaking between different clients.
+    -   The request is sent to the Gemini API via the `GeminiClient`.
+    -   If the Gemini API's response is streaming, the server transforms it into an OpenAI-compatible SSE stream; otherwise, it returns a complete JSON response.
+
+## Authentication Mechanism
+
+Crucially, the `mcp-server` **does not manage its own credentials**. It shares the exact same authentication mechanism as the main `gemini-cli` tool to ensure seamless and secure operation.
+
+The source of authentication credentials follows the identical priority and lookup logic as `gemini-cli`:
+
+-   **Cached Credentials**: If you have previously logged in through the interactive `gemini-cli` flow (e.g., `gcloud auth application-default login` or OAuth web login), the `mcp-server` will automatically use the cached credentials stored in `~/.config/gcloud` or `~/.gemini`.
+-   **Environment Variables**: The server will look for and use standard Google Cloud and Gemini environment variables, such as:
+    -   `GEMINI_API_KEY`
+    -   `GOOGLE_APPLICATION_CREDENTIALS`
+    -   `GOOGLE_CLOUD_PROJECT`
+
+This means that as long as your `gemini-cli` itself is configured correctly and works, the `mcp-server` will be authorized automatically, with no extra authentication steps required.
+
+## Important Security Note: YOLO Mode
+
+The `mcp-server` is designed for non-interactive, programmatic use. As such, it runs with a permanent **YOLO (You Only Live Once) Mode** enabled (`approvalMode: ApprovalMode.YOLO`).
+
+This means that any tool call requested by the model (e.g., `run_shell_command`, `replace`) will be **executed immediately without user confirmation**.
+
+**Warning:** Be extremely cautious when exposing this server to a network. Any client that can reach the server will be able to execute tools with the same permissions as the user running the server process.
+
+## Configuration Options
+
+You can configure the server's behavior via command-line arguments and environment variables.
+
+### Command-Line Arguments
+
+-   `--port=<number>`: Specifies the port for the server to listen on.
+    -   **Default**: `8765`
+-   `--debug`: Enables detailed debug logging to the console.
+
+### Environment Variables
+
+-   `GEMINI_TOOLS_DEFAULT_MODEL`: Sets a default LLM model specifically for tools hosted by the server (like `google_web_search`).
+    -   **Purpose**: When a tool needs to invoke an LLM during its execution (e.g., to summarize search results), it will use the model specified by this variable. This allows you to use a different (potentially faster or cheaper) model for tool execution than for the main chat.
+    -   **Example**: `GEMINI_TOOLS_DEFAULT_MODEL=gemini-1.5-flash`
+
+## Usage
+
+### 1. Installation & Build
+
+From the root of the `gemini-cli` project, ensure all dependencies are installed and then build the `mcp-server` package.
+
+```bash
+# From the project root
+npm install
+npm run build --workspace=@google/gemini-mcp-server
+```
+
+### 2. Starting the Server
+
+You can start the server using the `npm run start` command from the root directory, targeting the workspace.
+
+```bash
+# Start the server with default configuration
+npm run start --workspace=@google/gemini-mcp-server
+
+# Start on a different port with debug mode enabled
+npm run start --workspace=@google/gemini-mcp-server -- --port=9000 --debug
+
+# Use a faster model for tool calls
+GEMINI_TOOLS_DEFAULT_MODEL=gemini-1.5-flash npm run start --workspace=@google/gemini-mcp-server
+```
+
+When the server starts successfully, you will see output similar to this:
+
+```
+üöÄ Gemini CLI MCP Server and OpenAI Bridge are running on port 8765
+   - MCP transport listening on http://localhost:8765/mcp
+   - OpenAI-compatible endpoints available at http://localhost:8765/v1
+‚öôÔ∏è  Using default model for tools: gemini-2.5-pro
+```
+
+### 3. Testing the Endpoints
+
+You can use `curl` or any API client to test the server.
+
+**Test OpenAI Chat Completions (Streaming)**:
+
+```bash
+curl -N http://localhost:8765/v1/chat/completions \
+  -H "Content-Type: application/json" \
+  -d '{
+    "model": "gemini-pro",
+    "messages": [{"role": "user", "content": "Tell me a short story about a robot who learns to paint."}],
+    "stream": true
+  }'
+```
+
+**Test OpenAI Chat Completions (Non-Streaming)**:
+
+```bash
+curl http://localhost:8765/v1/chat/completions \
+  -H "Content-Type: application/json" \
+  -d '{
+    "model": "gemini-pro",
+    "messages": [{"role": "user", "content": "Why is the sky blue?"}],
+    "stream": false
+  }'
+```
+
+## Telemetry, Terms of Service, and Privacy
+
+### Telemetry
+
+The `@google/gemini-mcp-server` **does not introduce any new telemetry or data collection mechanisms**.
+
+It relies entirely on the OpenTelemetry (OTEL) system built into the `@google/gemini-cli-core` package. Therefore, all telemetry data (if enabled) will follow the main `gemini-cli` configuration and be sent to the destination specified in your `settings.json` file.
+
+For details on how to configure and use telemetry, please refer to the [main Gemini CLI telemetry documentation](../../docs/telemetry.md).
+
+### Terms of Service and Privacy Notice
+
+Your use of this server is governed by the Terms of Service and Privacy Policies corresponding to the `gemini-cli` account type you are using for authentication. As a bridge, `@google/gemini-mcp-server` does not collect, store, or process any additional data of its own.
+
+We strongly recommend you review the [main Gemini CLI Terms of Service and Privacy Notice documentation](../../docs/tos-privacy.md) for details applicable to your account.
+
+---
+
+### Developer Note: Regarding the package name `@google/gemini-mcp-server`
+
+Please note that the name of this package, `@google/gemini-mcp-server`, reflects its origin as an internal component of the official `google-gemini/gemini-cli` monorepo.
+
+-   **Internal Naming**: This naming is consistent internally within the source code and workspaces of the `gemini-cli` project.
+-   **Not for Independent Publication**: This package is **not intended to be published independently** to a public npm registry. If you fork this project and wish to publish your modified version, you **must** change the package name to your own scope (e.g., `@your-username/gemini-mcp-server`) to comply with npm's package naming policies and to avoid confusion.
+
+----
+
+# Gemini CLI - MCP / OpenAI Bridge Server (`@google/gemini-mcp-server`)
+
+`@google/gemini-mcp-server` ÊòØ‰∏Ä‰∏™Â§öÂäüËÉΩÁöÑÊúçÂä°Âô®Â∫îÁî®Á®ãÂ∫èÔºåÊó®Âú®‰Ωú‰∏∫ `gemini-cli` ÁîüÊÄÅÁ≥ªÁªüÁöÑÂº∫Â§ßÊâ©Â±ï„ÄÇÂÆÉ‰∏ªË¶ÅÊâøÊãÖ‰∏§‰∏™Ê†∏ÂøÉËßíËâ≤Ôºö
+
+1.  **MCP (Model-Context Protocol) ÊúçÂä°Âô®**: ÂÆÉ‰∏∫ `gemini-cli` ÊâòÁÆ°ÂíåÊö¥Èú≤‰∫Ü‰∏ÄÁ≥ªÂàóÂº∫Â§ßÁöÑÂÜÖÁΩÆÂ∑•ÂÖ∑Ôºà‰æãÂ¶Ç `google_web_search`ÔºâÔºåÂÖÅËÆ∏ `gemini-cli` ÁöÑÊ†∏ÂøÉÊ®°ÂûãÈÄöËøá‰∏Ä‰∏™Ê†áÂáÜÁöÑ„ÄÅÂèØÂèëÁé∞ÁöÑÂçèËÆÆÊù•Ë∞ÉÁî®Ëøô‰∫õÂ∑•ÂÖ∑„ÄÇ
+
+2.  **OpenAI ÂÖºÂÆπÁöÑ API Ê°•Êé•Âô®**: ÂÆÉÊèê‰æõ‰∫Ü‰∏Ä‰∏™‰∏é OpenAI Chat Completions API ÂÖºÂÆπÁöÑÁ´ØÁÇπ„ÄÇËøô‰ΩøÂæó‰ªª‰ΩïÊîØÊåÅ OpenAI API ÁöÑÁ¨¨‰∏âÊñπÂ∑•ÂÖ∑ÊàñÂ∫îÁî®Á®ãÂ∫èÔºà‰æãÂ¶Ç [Open WebUI](https://github.com/open-webui/open-webui)ÔºâÈÉΩÂèØ‰ª•Êó†ÁºùÂú∞‰∏é `gemini-cli` ÁöÑÂ∫ïÂ±Ç Gemini Ê®°ÂûãËøõË°å‰∫§‰∫íÔºåÂåÖÊã¨Âà©Áî®ÊµÅÂºèÂìçÂ∫î„ÄÇ
+
+## Ê†∏ÂøÉËÆæËÆ°ÁêÜÂøµ
+
+Ëøô‰∏™ÊúçÂä°Âô®ÁöÑÊ†∏ÂøÉËÆæËÆ°ÂéüÂàôÊòØ **ÊúÄÂ∞èÂåñ‰øÆÊîπÂíåÊúÄÂ§ßÂåñÂ§çÁî®**„ÄÇÂÆÉÂπ∂‰∏çÊòØÂØπ `gemini-cli` ÂäüËÉΩÁöÑÈáçÊñ∞ÂÆûÁé∞ÔºåËÄåÊòØÂ∑ßÂ¶ôÂú∞ÊûÑÂª∫Âú® `@google/gemini-cli-core` ÂåÖ‰πã‰∏ä„ÄÇ
+
+ÈÄöËøáÈáçÁî® `core` ÂåÖ‰∏≠ÁöÑ `Config` Âíå `GeminiClient` Á±ªÔºå`mcp-server` ÁªßÊâø‰∫Ü `gemini-cli` ÊâÄÊúâÁöÑÊ†∏ÂøÉ‰∏öÂä°ÈÄªËæë„ÄÅÂ∑•ÂÖ∑ÊâßË°åËÉΩÂäõÂíåÈÖçÁΩÆÁÆ°ÁêÜÊú∫Âà∂„ÄÇËøôÁßçËÆæËÆ°Á°Æ‰øù‰∫ÜË°å‰∏∫ÁöÑ‰∏ÄËá¥ÊÄßÔºåÂπ∂‰ΩøÂæóÁª¥Êä§ÂíåÊâ©Â±ïÂèòÂæóÊõ¥Âä†ÁÆÄÂçï„ÄÇ
+
+## ÂäüËÉΩÁâπÊÄß
+
+-   **ÊâòÁÆ°ÂéüÁîü `gemini-cli` Â∑•ÂÖ∑**: ÈÄöËøá MCP ÂçèËÆÆÔºåÂ∞Ü `gemini-cli` ÁöÑÂÜÖÁΩÆÂ∑•ÂÖ∑ÔºàÂ¶ÇÊñá‰ª∂Á≥ªÁªüÊìç‰Ωú„ÄÅÁΩëÈ°µÊäìÂèñ„ÄÅÁΩëÁªúÊêúÁ¥¢Á≠âÔºâÊö¥Èú≤Áªô `gemini-cli` Ê®°Âûã„ÄÇ
+-   **OpenAI API ÂÖºÂÆπÊÄß**: Êèê‰æõ `/v1/chat/completions` Âíå `/v1/models` Á´ØÁÇπÔºåÂÖÅËÆ∏Á¨¨‰∏âÊñπÂ∫îÁî®Á®ãÂ∫èÂÉè‰∏é OpenAI ÂØπËØù‰∏ÄÊ†∑‰∏é Gemini Ê®°Âûã‰∫§‰∫í„ÄÇ
+-   **ÊµÅÂºèÂìçÂ∫îÊîØÊåÅ**: ÂÆåÂÖ®ÊîØÊåÅÊµÅÂºèÂìçÂ∫îÔºåÂèØ‰ª•Â∞Ü Gemini Ê®°ÂûãÁöÑÂÆûÊó∂ÁîüÊàêÁªìÊûúÈÄöËøá SSE (Server-Sent Events) Êé®ÈÄÅÁªôÂÆ¢Êà∑Á´Ø„ÄÇ
+-   **ÁÅµÊ¥ªÁöÑÊ®°ÂûãÈÖçÁΩÆ**: ÂÖÅËÆ∏ÈÄöËøáÁéØÂ¢ÉÂèòÈáè‰∏∫ÊúçÂä°Âô®ÊâòÁÆ°ÁöÑÂ∑•ÂÖ∑ÔºàÂ¶Ç `google_web_search`ÔºâÈÖçÁΩÆ‰∏Ä‰∏™ÁâπÂÆöÁöÑ„ÄÅÁã¨Á´ãÁöÑÈªòËÆ§ LLM Ê®°Âûã„ÄÇ
+
+## Êû∂ÊûÑ‰∏é‰∫§‰∫íÊµÅÁ®ã
+
+`mcp-server` ‰Ωú‰∏∫ `gemini-cli` ÁîüÊÄÅÁ≥ªÁªü‰∏≠ÁöÑ‰∏Ä‰∏™Áã¨Á´ãÁªÑ‰ª∂ÔºåÂÖ∂‰∫§‰∫íÊµÅÁ®ãÂ¶Ç‰∏ãÔºö
+
+1.  **ÈÖçÁΩÆÂä†ËΩΩ**: ÊúçÂä°Âô®ÂêØÂä®Êó∂ÔºåÂÆÉ‰ºöÂÉè‰∏ª `gemini-cli` Â∫îÁî®‰∏ÄÊ†∑ÔºåÂä†ËΩΩÁî®Êà∑ÂíåÂ∑•‰ΩúÂå∫ÁöÑ `settings.json` Êñá‰ª∂ÔºåÂπ∂ËØªÂèñÁéØÂ¢ÉÂèòÈáèÊù•ÂàùÂßãÂåñ‰∏Ä‰∏™ `@google/gemini-cli-core` ÁöÑ `Config` ÂÆû‰æã„ÄÇ
+2.  **ËÆ§ËØÅ**: ÊúçÂä°Âô®**‰∏çÂ§ÑÁêÜ**Ëá™Â∑±ÁöÑËÆ§ËØÅÊµÅÁ®ã„ÄÇÂÆÉÂÆåÂÖ®‰æùËµñ‰∫é `gemini-cli` Â∑≤ÁªèÂª∫Á´ãÁöÑËÆ§ËØÅÁä∂ÊÄÅÔºàËØ¶ÊÉÖËßÅ‰∏ã‰∏ÄËäÇÔºâ„ÄÇ
+3.  **MCP ÊúçÂä°**: ÂÆÉÂêØÂä®‰∏Ä‰∏™ MCP ÊúçÂä°Âô®Ôºå`gemini-cli` Âú®ÈúÄË¶ÅÊó∂ÂèØ‰ª•ËøûÊé•Âà∞Ëøô‰∏™ÊúçÂä°Âô®Êù•ÂèëÁé∞ÂíåÊâßË°åÂ∑•ÂÖ∑„ÄÇ
+4.  **OpenAI Ê°•Êé•**: ÂÆÉÂêØÂä®‰∏Ä‰∏™ Express Web ÊúçÂä°Âô®ÔºåÁõëÂê¨ OpenAI Ê†ºÂºèÁöÑ API ËØ∑Ê±Ç„ÄÇ
+5.  **ËØ∑Ê±ÇÂ§ÑÁêÜ**:
+    -   ÂΩìÊî∂Âà∞‰∏Ä‰∏™ OpenAI Ê†ºÂºèÁöÑËØ∑Ê±ÇÊó∂ÔºåÊúçÂä°Âô®‰ºöÂ∞ÜÂÖ∂ËΩ¨Êç¢‰∏∫ `gemini-cli-core` ÂèØ‰ª•ÁêÜËß£ÁöÑÊ†ºÂºè„ÄÇ
+    -   ÂÆÉ‰ΩøÁî®Â§çÁî®ÁöÑ `Config` ÂÆû‰æãÊù•Ëé∑Âèñ‰∏Ä‰∏™ `GeminiClient`„ÄÇ
+    -   ÈÄöËøá `GeminiClient` Â∞ÜËØ∑Ê±ÇÂèëÈÄÅÁªô Gemini API„ÄÇ
+    -   Â¶ÇÊûú Gemini API ÁöÑÂìçÂ∫îÊòØÊµÅÂºèÁöÑÔºåÊúçÂä°Âô®‰ºöÂ∞ÜÂÖ∂ËΩ¨Êç¢‰∏∫ OpenAI ÂÖºÂÆπÁöÑ SSE ‰∫ã‰ª∂ÊµÅÔºõÂ¶ÇÊûúÊòØÈùûÊµÅÂºèÁöÑÔºåÂàôËøîÂõû‰∏Ä‰∏™ÂÆåÊï¥ÁöÑ JSON ÂìçÂ∫î„ÄÇ
+
+## ËÆ§ËØÅÊú∫Âà∂
+
+Ëá≥ÂÖ≥ÈáçË¶ÅÁöÑÊòØÔºå`mcp-server` **‰∏çÁÆ°ÁêÜËá™Â∑±ÁöÑËÆ§ËØÅÂá≠ÊçÆ**„ÄÇÂÆÉ‰∏é‰∏ª `gemini-cli` Â∑•ÂÖ∑ÂÖ±‰∫´Áõ∏ÂêåÁöÑËÆ§ËØÅÊú∫Âà∂Ôºå‰ª•Á°Æ‰øùÊó†ÁºùÂíåÂÆâÂÖ®ÁöÑÊìç‰Ωú„ÄÇ
+
+ËÆ§ËØÅÂá≠ÊçÆÁöÑÊù•Ê∫êÈÅµÂæ™‰∏é `gemini-cli` ÂÆåÂÖ®Áõ∏ÂêåÁöÑ‰ºòÂÖàÁ∫ßÂíåÊü•ÊâæÈÄªËæëÔºö
+
+-   **ÁºìÂ≠òÁöÑÂá≠ÊçÆ**: Â¶ÇÊûúÊÇ®‰πãÂâçÈÄöËøá `gemini-cli` ÁöÑ‰∫§‰∫íÂºèÊµÅÁ®ãÔºà‰æãÂ¶Ç `gcloud auth application-default login` Êàñ OAuth ÁΩëÈ°µÁôªÂΩïÔºâÁôªÂΩïËøáÔºå`mcp-server` ‰ºöËá™Âä®‰ΩøÁî®Â≠òÂÇ®Âú® `~/.config/gcloud` Êàñ `~/.gemini` ÁõÆÂΩï‰∏ãÁöÑÁºìÂ≠òÂá≠ÊçÆ„ÄÇ
+-   **ÁéØÂ¢ÉÂèòÈáè**: ÊúçÂä°Âô®‰ºöÊü•ÊâæÂπ∂‰ΩøÁî®Ê†áÂáÜÁöÑ Google Cloud Âíå Gemini ÁéØÂ¢ÉÂèòÈáèÔºå‰æãÂ¶ÇÔºö
+    -   `GEMINI_API_KEY`
+    -   `GOOGLE_APPLICATION_CREDENTIALS`
+    -   `GOOGLE_CLOUD_PROJECT`
+
+ËøôÊÑèÂë≥ÁùÄÔºåÂè™Ë¶ÅÊÇ®ÁöÑ `gemini-cli` Êú¨Ë∫´ÈÖçÁΩÆÊ≠£Á°Æ‰∏îÂèØ‰ª•Â∑•‰ΩúÔºå`mcp-server` Â∞±ËÉΩËá™Âä®Ëé∑ÂæóÊéàÊùÉÔºåÊó†ÈúÄ‰ªª‰ΩïÈ¢ùÂ§ñÁöÑËÆ§ËØÅÊ≠•È™§„ÄÇ
+
+## ÈÖçÁΩÆÈÄâÈ°π
+
+ÊÇ®ÂèØ‰ª•ÈÄöËøáÂëΩ‰ª§Ë°åÂèÇÊï∞ÂíåÁéØÂ¢ÉÂèòÈáèÊù•ÈÖçÁΩÆÊúçÂä°Âô®ÁöÑË°å‰∏∫„ÄÇ
+
+### ÂëΩ‰ª§Ë°åÂèÇÊï∞
+
+-   `--port=<number>`: ÊåáÂÆöÊúçÂä°Âô®ÁõëÂê¨ÁöÑÁ´ØÂè£„ÄÇ
+    -   **ÈªòËÆ§ÂÄº**: `8765`
+-   `--debug`: ÂêØÁî®ËØ¶ÁªÜÁöÑË∞ÉËØïÊó•ÂøóËæìÂá∫„ÄÇ
+
+### ÁéØÂ¢ÉÂèòÈáè
+
+-   `GEMINI_TOOLS_DEFAULT_MODEL`: ‰∏∫ÊúçÂä°Âô®ÊâòÁÆ°ÁöÑÂ∑•ÂÖ∑ÔºàÂ¶Ç `google_web_search`ÔºâËÆæÁΩÆ‰∏Ä‰∏™ÈªòËÆ§ÁöÑ LLM Ê®°Âûã„ÄÇ
+    -   **Áî®ÈÄî**: ÂΩì‰∏Ä‰∏™Â∑•ÂÖ∑Âú®ÊâßË°åËøáÁ®ã‰∏≠ÈúÄË¶ÅË∞ÉÁî® LLMÔºà‰æãÂ¶ÇÔºåÂØπÊêúÁ¥¢ÁªìÊûúËøõË°åÊÄªÁªìÔºâÊó∂ÔºåÂÆÉÂ∞Ü‰ΩøÁî®Ê≠§ÁéØÂ¢ÉÂèòÈáèÊåáÂÆöÁöÑÊ®°Âûã„ÄÇËøôÂÖÅËÆ∏ÊÇ®‰∏∫‰∏ªËÅäÂ§©ÂíåÂ∑•ÂÖ∑ÊâßË°å‰ΩøÁî®‰∏çÂêåÁöÑÊ®°ÂûãÔºå‰ªéËÄåÂèØËÉΩ‰ºòÂåñÊàêÊú¨ÂíåÈÄüÂ∫¶„ÄÇ
+    -   **Á§∫‰æã**: `GEMINI_TOOLS_DEFAULT_MODEL=gemini-1.5-flash`
+
+## ‰ΩøÁî®ÊñπÊ≥ï
+
+### 1. ÂÆâË£Ö‰∏éÊûÑÂª∫
+
+Âú® `gemini-cli` È°πÁõÆÁöÑÊ†πÁõÆÂΩï‰∏ãÔºåÁ°Æ‰øùÊâÄÊúâ‰æùËµñÂ∑≤ÂÆâË£ÖÔºåÂπ∂ÊûÑÂª∫ `mcp-server` ÂåÖ„ÄÇ
+
+```bash
+# Âú®È°πÁõÆÊ†πÁõÆÂΩïËøêË°å
+npm install
+npm run build --workspace=@google/gemini-mcp-server
+```
+
+### 2. ÂêØÂä®ÊúçÂä°Âô®
+
+ÊÇ®ÂèØ‰ª•‰ΩøÁî® `npm run start` ÂëΩ‰ª§Êù•ÂêØÂä®ÊúçÂä°Âô®„ÄÇ
+
+```bash
+# ÂêØÂä®ÊúçÂä°Âô®Ôºå‰ΩøÁî®ÈªòËÆ§ÈÖçÁΩÆ
+npm run start --workspace=@google/gemini-mcp-server
+
+# Âú®‰∏çÂêåÁ´ØÂè£‰∏äÂêØÂä®ÔºåÂπ∂ÂêØÁî®Ë∞ÉËØïÊ®°Âºè
+npm run start --workspace=@google/gemini-mcp-server -- --port=9000 --debug
+
+# ‰ΩøÁî®‰∏Ä‰∏™Êõ¥Âø´ÁöÑÊ®°ÂûãËøõË°åÂ∑•ÂÖ∑Ë∞ÉÁî®
+GEMINI_TOOLS_DEFAULT_MODEL=gemini-1.5-flash npm run start --workspace=@google/gemini-mcp-server
+```
+
+ÊúçÂä°Âô®ÊàêÂäüÂêØÂä®ÂêéÔºåÊÇ®Â∞ÜÁúãÂà∞Á±ª‰ºº‰ª•‰∏ãÁöÑËæìÂá∫Ôºö
+
+```
+üöÄ Gemini CLI MCP Server and OpenAI Bridge are running on port 8765
+   - MCP transport listening on http://localhost:8765/mcp
+   - OpenAI-compatible endpoints available at http://localhost:8765/v1
+‚öôÔ∏è  Using default model for tools: gemini-2.5-pro
+```
+
+### 3. ÊµãËØïÁ´ØÁÇπ
+
+ÊÇ®ÂèØ‰ª•‰ΩøÁî® `curl` Êàñ‰ªª‰Ωï API ÂÆ¢Êà∑Á´ØÊù•ÊµãËØïÊúçÂä°Âô®„ÄÇ
+
+**ÊµãËØï OpenAI Chat Completions (ÊµÅÂºè)**:
+
+```bash
+curl -N http://localhost:8765/v1/chat/completions \
+  -H "Content-Type: application/json" \
+  -d '{
+    "model": "gemini-pro",
+    "messages": [{"role": "user", "content": "Tell me a short story about a robot who learns to paint."}],
+    "stream": true
+  }'
+```
+
+## ÈÅ•Êµã„ÄÅÊúçÂä°Êù°Ê¨æÂíåÈöêÁßÅ
+
+### ÈÅ•Êµã (Telemetry)
+
+`@google/gemini-mcp-server` Êú¨Ë∫´**‰∏çÂºïÂÖ•‰ªª‰ΩïÊñ∞ÁöÑÈÅ•ÊµãÊàñÊï∞ÊçÆÊî∂ÈõÜÊú∫Âà∂**„ÄÇ
+
+ÂÆÉÂÆåÂÖ®‰æùËµñ‰∫é `@google/gemini-cli-core` ÂåÖ‰∏≠ÂÜÖÁΩÆÁöÑ OpenTelemetry (OTEL) Á≥ªÁªü„ÄÇÂõ†Ê≠§ÔºåÊâÄÊúâÁöÑÈÅ•ÊµãÊï∞ÊçÆÔºàÂ¶ÇÊûúÂêØÁî®ÔºâÈÉΩÂ∞ÜÈÅµÂæ™ `gemini-cli` ÁöÑ‰∏ªÈÖçÁΩÆÔºåÂπ∂Ë¢´ÂèëÈÄÅÂà∞ `settings.json` Êñá‰ª∂‰∏≠ÊåáÂÆöÁöÑÁõÆÊ†á„ÄÇ
+
+ÂÖ≥‰∫éÂ¶Ç‰ΩïÈÖçÁΩÆÂíå‰ΩøÁî®ÈÅ•ÊµãÔºåËØ∑ÂèÇÈòÖ[‰∏ª Gemini CLI ÈÅ•ÊµãÊñáÊ°£](../../docs/telemetry.md)„ÄÇ
+
+### ÊúçÂä°Êù°Ê¨æ (Terms of Service) ÂíåÈöêÁßÅÂ£∞Êòé (Privacy Notice)
+
+Êú¨ÊúçÂä°Âô®ÁöÑ‰ΩøÁî®ÂèóÂà∂‰∫éÊÇ®Áî®‰∫éËÆ§ËØÅÁöÑ `gemini-cli` Ë¥¶Êà∑Á±ªÂûãÊâÄÂØπÂ∫îÁöÑÊúçÂä°Êù°Ê¨æÂíåÈöêÁßÅÊîøÁ≠ñ„ÄÇ`@google/gemini-mcp-server` ‰Ωú‰∏∫‰∏Ä‰∏™Ê°•Êé•Â∑•ÂÖ∑ÔºåÊú¨Ë∫´‰∏çÊî∂ÈõÜ„ÄÅÂ≠òÂÇ®ÊàñÂ§ÑÁêÜÊÇ®ÁöÑ‰ªª‰ΩïÈ¢ùÂ§ñÊï∞ÊçÆ„ÄÇ
+
+Êàë‰ª¨Âº∫ÁÉàÂª∫ËÆÆÊÇ®Êü•ÈòÖ[‰∏ª Gemini CLI ÊúçÂä°Êù°Ê¨æÂíåÈöêÁßÅÂ£∞ÊòéÊñáÊ°£](../../docs/tos-privacy.md)‰ª•‰∫ÜËß£ÈÄÇÁî®‰∫éÊÇ®Ë¥¶Êà∑ÁöÑËØ¶ÁªÜ‰ø°ÊÅØ„ÄÇ
+
+---
+
+### ÂºÄÂèëËÄÖËØ¥ÊòéÔºöÂÖ≥‰∫éÂåÖÂêç `@google/gemini-mcp-server`
+
+ËØ∑Ê≥®ÊÑèÔºåÊú¨ÂåÖÁöÑÂêçÁß∞ `@google/gemini-mcp-server` ÂèçÊò†‰∫ÜÂÆÉ‰Ωú‰∏∫ÂÆòÊñπ `google-gemini/gemini-cli` ÁöÑforkÈ°πÁõÆÁöÑ monorepo ÂÜÖÈÉ®ÁªÑ‰ª∂ÁöÑÊù•Ê∫ê„ÄÇ
+
+-   **ÂÜÖÈÉ®ÂëΩÂêç**: Âú® `gemini-cli` È°πÁõÆÁöÑÊ∫ê‰ª£Á†ÅÂíåÂ∑•‰ΩúÂå∫‰∏≠ÔºåÊ≠§ÂëΩÂêçÊòØÂÜÖÈÉ®‰∏ÄËá¥ÁöÑ„ÄÇ
+-   **ÈùûÁã¨Á´ãÂèëÂ∏É**: Ê≠§ÂåÖ**‰∏ç‰ºö**‰Ωú‰∏∫‰∏Ä‰∏™Áã¨Á´ãÁöÑ„ÄÅÁâàÊú¨ÂåñÁöÑÂåÖÂèëÂ∏ÉÂà∞ÂÖ¨ÂÖ± npm registry ‰∏ä„ÄÇÂ¶ÇÊûúÊÇ® fork Êú¨È°πÁõÆÂπ∂Â∏åÊúõÁã¨Á´ãÂèëÂ∏ÉÊÇ®ÁöÑ‰øÆÊîπÁâàÊú¨ÔºåÊÇ®**ÂøÖÈ°ª**Â∞ÜÂåÖÂêçÊõ¥Êîπ‰∏∫ÊÇ®Ëá™Â∑±ÁöÑ scopeÔºà‰æãÂ¶Ç `@your-username/gemini-mcp-server`ÔºâÔºå‰ª•ÈÅµÂÆà npm ÁöÑÂåÖÂëΩÂêçËßÑËåÉÂπ∂ÈÅøÂÖçÊ∑∑Ê∑Ü„ÄÇ
diff --git a/package-lock.json b/package-lock.json
index 09eb6d96..16a307cc 100644
--- a/package-lock.json
+++ b/package-lock.json
@@ -910,6 +910,10 @@
       "resolved": "packages/core",
       "link": true
     },
+    "node_modules/@google/gemini-mcp-server": {
+      "resolved": "packages/mcp-server",
+      "link": true
+    },
     "node_modules/@google/genai": {
       "version": "1.6.0",
       "resolved": "https://registry.npmjs.org/@google/genai/-/genai-1.6.0.tgz",
@@ -1227,9 +1231,9 @@
       "license": "MIT"
     },
     "node_modules/@modelcontextprotocol/sdk": {
-      "version": "1.13.1",
-      "resolved": "https://registry.npmjs.org/@modelcontextprotocol/sdk/-/sdk-1.13.1.tgz",
-      "integrity": "sha512-8q6+9aF0yA39/qWT/uaIj6zTpC+Qu07DnN/lb9mjoquCJsAh6l3HyYqc9O3t2j7GilseOQOQimLg7W3By6jqvg==",
+      "version": "1.13.2",
+      "resolved": "https://registry.npmjs.org/@modelcontextprotocol/sdk/-/sdk-1.13.2.tgz",
+      "integrity": "sha512-Vx7qOcmoKkR3qhaQ9qf3GxiVKCEu+zfJddHv6x3dY/9P6+uIwJnmuAur5aB+4FDXf41rRrDnOEGkviX5oYZ67w==",
       "license": "MIT",
       "dependencies": {
         "ajv": "^6.12.6",
@@ -2202,6 +2206,17 @@
       "license": "MIT",
       "peer": true
     },
+    "node_modules/@types/body-parser": {
+      "version": "1.19.6",
+      "resolved": "https://registry.npmjs.org/@types/body-parser/-/body-parser-1.19.6.tgz",
+      "integrity": "sha512-HLFeCYgz89uk22N5Qg3dvGvsv46B8GLvKKo1zKG4NybA8U2DiEO3w9lqGg29t/tfLRJpJ6iQxnVw4OnB7MoM9g==",
+      "dev": true,
+      "license": "MIT",
+      "dependencies": {
+        "@types/connect": "*",
+        "@types/node": "*"
+      }
+    },
     "node_modules/@types/braces": {
       "version": "3.0.5",
       "resolved": "https://registry.npmjs.org/@types/braces/-/braces-3.0.5.tgz",
@@ -2232,6 +2247,16 @@
       "integrity": "sha512-OS//b51j9uyR3zvwD04Kfs5kHpve2qalQ18JhY/ho3voGYUTPLEG90/ocfKPI48hyHH8T04f7KEEbK6Ue60oZQ==",
       "license": "MIT"
     },
+    "node_modules/@types/connect": {
+      "version": "3.4.38",
+      "resolved": "https://registry.npmjs.org/@types/connect/-/connect-3.4.38.tgz",
+      "integrity": "sha512-K6uROf1LD88uDQqJCktA4yzL1YYAK6NgfsI0v/mTgyPKWsX1CnJ0XPSDhViejru1GcRkLWb8RlzFYJRqGUbaug==",
+      "dev": true,
+      "license": "MIT",
+      "dependencies": {
+        "@types/node": "*"
+      }
+    },
     "node_modules/@types/deep-eql": {
       "version": "4.0.2",
       "resolved": "https://registry.npmjs.org/@types/deep-eql/-/deep-eql-4.0.2.tgz",
@@ -2263,6 +2288,31 @@
       "dev": true,
       "license": "MIT"
     },
+    "node_modules/@types/express": {
+      "version": "5.0.3",
+      "resolved": "https://registry.npmjs.org/@types/express/-/express-5.0.3.tgz",
+      "integrity": "sha512-wGA0NX93b19/dZC1J18tKWVIYWyyF2ZjT9vin/NRu0qzzvfVzWjs04iq2rQ3H65vCTQYlRqs3YHfY7zjdV+9Kw==",
+      "dev": true,
+      "license": "MIT",
+      "dependencies": {
+        "@types/body-parser": "*",
+        "@types/express-serve-static-core": "^5.0.0",
+        "@types/serve-static": "*"
+      }
+    },
+    "node_modules/@types/express-serve-static-core": {
+      "version": "5.0.6",
+      "resolved": "https://registry.npmjs.org/@types/express-serve-static-core/-/express-serve-static-core-5.0.6.tgz",
+      "integrity": "sha512-3xhRnjJPkULekpSzgtoNYYcTWgEZkp4myc+Saevii5JPnHNvHMRlBSHDbs7Bh1iPPoVTERHEZXyhyLbMEsExsA==",
+      "dev": true,
+      "license": "MIT",
+      "dependencies": {
+        "@types/node": "*",
+        "@types/qs": "*",
+        "@types/range-parser": "*",
+        "@types/send": "*"
+      }
+    },
     "node_modules/@types/glob": {
       "version": "8.1.0",
       "resolved": "https://registry.npmjs.org/@types/glob/-/glob-8.1.0.tgz",
@@ -2297,6 +2347,13 @@
       "integrity": "sha512-pUY3cKH/Nm2yYrEmDlPR1mR7yszjGx4DrwPjQ702C4/D5CwHuZTgZdIdwPkRbcuhs7BAh2L5rg3CL5cbRiGTCQ==",
       "license": "MIT"
     },
+    "node_modules/@types/http-errors": {
+      "version": "2.0.5",
+      "resolved": "https://registry.npmjs.org/@types/http-errors/-/http-errors-2.0.5.tgz",
+      "integrity": "sha512-r8Tayk8HJnX0FztbZN7oVqGccWgw98T/0neJphO91KkmOzug1KkofZURD4UaD5uH8AqcFLfdPErnBod0u71/qg==",
+      "dev": true,
+      "license": "MIT"
+    },
     "node_modules/@types/json-schema": {
       "version": "7.0.15",
       "resolved": "https://registry.npmjs.org/@types/json-schema/-/json-schema-7.0.15.tgz",
@@ -2321,6 +2378,13 @@
         "@types/braces": "*"
       }
     },
+    "node_modules/@types/mime": {
+      "version": "1.3.5",
+      "resolved": "https://registry.npmjs.org/@types/mime/-/mime-1.3.5.tgz",
+      "integrity": "sha512-/pyBZWSLD2n0dcHE3hq8s8ZvcETHtEuF+3E7XVt0Ig2nvsVQXdghHVcEkIWjy9A0wKfTn97a/PSDYohKIlnP/w==",
+      "dev": true,
+      "license": "MIT"
+    },
     "node_modules/@types/mime-types": {
       "version": "2.1.4",
       "resolved": "https://registry.npmjs.org/@types/mime-types/-/mime-types-2.1.4.tgz",
@@ -2349,6 +2413,20 @@
       "integrity": "sha512-37i+OaWTh9qeK4LSHPsyRC7NahnGotNuZvjLSgcPzblpHB3rrCJxAOgI5gCdKm7coonsaX1Of0ILiTcnZjbfxA==",
       "license": "MIT"
     },
+    "node_modules/@types/qs": {
+      "version": "6.14.0",
+      "resolved": "https://registry.npmjs.org/@types/qs/-/qs-6.14.0.tgz",
+      "integrity": "sha512-eOunJqu0K1923aExK6y8p6fsihYEn/BYuQ4g0CxAAgFc4b/ZLN4CrsRZ55srTdqoiLzU2B2evC+apEIxprEzkQ==",
+      "dev": true,
+      "license": "MIT"
+    },
+    "node_modules/@types/range-parser": {
+      "version": "1.2.7",
+      "resolved": "https://registry.npmjs.org/@types/range-parser/-/range-parser-1.2.7.tgz",
+      "integrity": "sha512-hKormJbkJqzQGhziax5PItDUTMAM9uE2XXQmM37dyd4hVM+5aVl7oVxMVUiVQn2oCQFN/LKCZdvSM0pFRqbSmQ==",
+      "dev": true,
+      "license": "MIT"
+    },
     "node_modules/@types/react": {
       "version": "19.1.8",
       "resolved": "https://registry.npmjs.org/@types/react/-/react-19.1.8.tgz",
@@ -2376,6 +2454,29 @@
       "dev": true,
       "license": "MIT"
     },
+    "node_modules/@types/send": {
+      "version": "0.17.5",
+      "resolved": "https://registry.npmjs.org/@types/send/-/send-0.17.5.tgz",
+      "integrity": "sha512-z6F2D3cOStZvuk2SaP6YrwkNO65iTZcwA2ZkSABegdkAh/lf+Aa/YQndZVfmEXT5vgAp6zv06VQ3ejSVjAny4w==",
+      "dev": true,
+      "license": "MIT",
+      "dependencies": {
+        "@types/mime": "^1",
+        "@types/node": "*"
+      }
+    },
+    "node_modules/@types/serve-static": {
+      "version": "1.15.8",
+      "resolved": "https://registry.npmjs.org/@types/serve-static/-/serve-static-1.15.8.tgz",
+      "integrity": "sha512-roei0UY3LhpOJvjbIP6ZZFngyLKl5dskOtDhxY5THRSpO+ZI+nzJ+m5yUMzGrp89YRa7lvknKkMYjqQFGwA7Sg==",
+      "dev": true,
+      "license": "MIT",
+      "dependencies": {
+        "@types/http-errors": "*",
+        "@types/node": "*",
+        "@types/send": "*"
+      }
+    },
     "node_modules/@types/shell-quote": {
       "version": "1.7.5",
       "resolved": "https://registry.npmjs.org/@types/shell-quote/-/shell-quote-1.7.5.tgz",
@@ -11382,7 +11483,7 @@
       "version": "0.1.8",
       "dependencies": {
         "@google/genai": "^1.4.0",
-        "@modelcontextprotocol/sdk": "^1.11.0",
+        "@modelcontextprotocol/sdk": "^1.13.2",
         "@opentelemetry/api": "^1.9.0",
         "@opentelemetry/exporter-logs-otlp-grpc": "^0.52.0",
         "@opentelemetry/exporter-metrics-otlp-grpc": "^0.52.0",
@@ -11444,6 +11545,73 @@
           "optional": true
         }
       }
+    },
+    "packages/mcp-server": {
+      "name": "@google/gemini-mcp-server",
+      "version": "0.1.8",
+      "dependencies": {
+        "@google/gemini-cli": "*",
+        "@google/gemini-cli-core": "*",
+        "@modelcontextprotocol/sdk": "^1.13.2",
+        "express": "^5.1.0",
+        "openai": "^5.8.2",
+        "zod": "^3.23.8"
+      },
+      "bin": {
+        "gemini-mcp-server": "dist/index.js"
+      },
+      "devDependencies": {
+        "@types/express": "^5.0.3",
+        "typescript": "^5.3.3",
+        "vitest": "^3.1.1"
+      },
+      "engines": {
+        "node": ">=18"
+      }
+    },
+    "packages/mcp-server/node_modules/openai": {
+      "version": "5.8.2",
+      "resolved": "https://registry.npmjs.org/openai/-/openai-5.8.2.tgz",
+      "integrity": "sha512-8C+nzoHYgyYOXhHGN6r0fcb4SznuEn1R7YZMvlqDbnCuE0FM2mm3T1HiYW6WIcMS/F1Of2up/cSPjLPaWt0X9Q==",
+      "license": "Apache-2.0",
+      "bin": {
+        "openai": "bin/cli"
+      },
+      "peerDependencies": {
+        "ws": "^8.18.0",
+        "zod": "^3.23.8"
+      },
+      "peerDependenciesMeta": {
+        "ws": {
+          "optional": true
+        },
+        "zod": {
+          "optional": true
+        }
+      }
+    },
+    "packages/mcp-server/node_modules/ws": {
+      "version": "8.18.3",
+      "resolved": "https://registry.npmjs.org/ws/-/ws-8.18.3.tgz",
+      "integrity": "sha512-PEIGCY5tSlUt50cqyMXfCzX+oOPqN0vuGqWzbcJ2xvnkzkq46oOpz7dQaTDBdfICb4N14+GARUDw2XV2N4tvzg==",
+      "license": "MIT",
+      "optional": true,
+      "peer": true,
+      "engines": {
+        "node": ">=10.0.0"
+      },
+      "peerDependencies": {
+        "bufferutil": "^4.0.1",
+        "utf-8-validate": ">=5.0.2"
+      },
+      "peerDependenciesMeta": {
+        "bufferutil": {
+          "optional": true
+        },
+        "utf-8-validate": {
+          "optional": true
+        }
+      }
     }
   }
 }
diff --git a/packages/cli/package.json b/packages/cli/package.json
index ade14e16..db5093a3 100644
--- a/packages/cli/package.json
+++ b/packages/cli/package.json
@@ -5,6 +5,10 @@
   "repository": "google-gemini/gemini-cli",
   "type": "module",
   "main": "dist/index.js",
+  "exports": {
+    ".": "./dist/index.js",
+    "./public-api": "./dist/src/public-api.js"
+  },
   "bin": {
     "gemini": "dist/index.js"
   },
diff --git a/packages/cli/src/config/config.ts b/packages/cli/src/config/config.ts
index 552a8f67..99e1dd7d 100644
--- a/packages/cli/src/config/config.ts
+++ b/packages/cli/src/config/config.ts
@@ -161,6 +161,7 @@ export async function loadHierarchicalGeminiMemory(
   );
 }
 
+
 export async function loadCliConfig(
   settings: Settings,
   extensions: Extension[],
diff --git a/packages/cli/src/public-api.ts b/packages/cli/src/public-api.ts
new file mode 100644
index 00000000..49ecfdc3
--- /dev/null
+++ b/packages/cli/src/public-api.ts
@@ -0,0 +1,5 @@
+export { loadEnvironment } from './config/config.js';
+export { loadSettings, type Settings } from './config/settings.js';
+export { loadExtensions, type Extension } from './config/extension.js';
+export { loadSandboxConfig } from './config/sandboxConfig.js';
+export { getCliVersion } from './utils/version.js';
diff --git a/packages/core/package.json b/packages/core/package.json
index e4a3a334..b831cc43 100644
--- a/packages/core/package.json
+++ b/packages/core/package.json
@@ -24,7 +24,7 @@
   ],
   "dependencies": {
     "@google/genai": "^1.4.0",
-    "@modelcontextprotocol/sdk": "^1.11.0",
+    "@modelcontextprotocol/sdk": "^1.13.2",
     "@opentelemetry/api": "^1.9.0",
     "@opentelemetry/exporter-logs-otlp-grpc": "^0.52.0",
     "@opentelemetry/exporter-metrics-otlp-grpc": "^0.52.0",
diff --git a/packages/mcp-server/package.json b/packages/mcp-server/package.json
new file mode 100644
index 00000000..e3787a64
--- /dev/null
+++ b/packages/mcp-server/package.json
@@ -0,0 +1,39 @@
+{
+  "name": "@google/gemini-mcp-server",
+  "version": "0.1.8",
+  "type": "module",
+  "main": "dist/index.js",
+  "bin": {
+    "gemini-mcp-server": "dist/index.js"
+  },
+  "scripts": {
+    "build": "node ../../scripts/build_package.js",
+    "clean": "rm -rf dist",
+    "start": "node dist/index.js",
+    "debug": "node --inspect-brk dist/index.js",
+    "lint": "eslint . --ext .ts,.tsx",
+    "format": "prettier --write .",
+    "test": "vitest run",
+    "typecheck": "tsc --noEmit",
+    "prerelease:version": "node ../../scripts/bind_package_version.js",
+    "prerelease:deps": "node ../../scripts/bind_package_dependencies.js",
+    "prepack": "npm run build",
+    "prepublishOnly": "node ../../scripts/prepublish.js"
+  },
+  "dependencies": {
+    "@google/gemini-cli": "*",
+    "@google/gemini-cli-core": "*",
+    "@modelcontextprotocol/sdk": "^1.13.2",
+    "express": "^5.1.0",
+    "openai": "^5.8.2",
+    "zod": "^3.23.8"
+  },
+  "devDependencies": {
+    "@types/express": "^5.0.3",
+    "typescript": "^5.3.3",
+    "vitest": "^3.1.1"
+  },
+  "engines": {
+    "node": ">=18"
+  }
+}
diff --git a/packages/mcp-server/src/bridge/bridge.ts b/packages/mcp-server/src/bridge/bridge.ts
new file mode 100644
index 00000000..c8e1b5b1
--- /dev/null
+++ b/packages/mcp-server/src/bridge/bridge.ts
@@ -0,0 +1,228 @@
+import express, { Request, Response, NextFunction, Application } from 'express';
+import { McpServer } from '@modelcontextprotocol/sdk/server/mcp.js';
+import { StreamableHTTPServerTransport } from '@modelcontextprotocol/sdk/server/streamableHttp.js';
+import { z } from 'zod';
+import {
+  type Config,
+  type Tool as GcliTool,
+  type ToolResult,
+  GeminiChat,
+} from '@google/gemini-cli-core';
+import {
+  type CallToolResult,
+  isInitializeRequest,
+} from '@modelcontextprotocol/sdk/types.js';
+import {
+  type PartUnion,
+  type Tool,
+  type GenerateContentConfig,
+  type Content,
+} from '@google/genai';
+import { randomUUID } from 'node:crypto';
+
+const LOG_PREFIX = '[MCP SERVER]';
+
+// NEW: Êó•Âøó‰∏≠Èó¥‰ª∂
+const requestLogger = (req: Request, res: Response, next: NextFunction) => {
+  console.log(`${LOG_PREFIX} ‚¨áÔ∏è  Incoming Request: ${req.method} ${req.url}`);
+  console.log(`${LOG_PREFIX}    Headers:`, JSON.stringify(req.headers, null, 2));
+  if (req.body && Object.keys(req.body).length > 0) {
+    const bodyStr = JSON.stringify(req.body);
+    console.log(
+      `${LOG_PREFIX}    Body:`,
+      bodyStr.length > 300 ? bodyStr.substring(0, 300) + '...' : bodyStr,
+    );
+  }
+  next();
+};
+
+export class GcliMcpBridge {
+  private readonly config: Config;
+  private readonly cliVersion: string;
+  private readonly mcpServer: McpServer;
+
+  constructor(config: Config, cliVersion: string) {
+    this.config = config;
+    this.cliVersion = cliVersion;
+    this.mcpServer = new McpServer(
+      {
+        name: 'gemini-cli-mcp-server',
+        version: this.cliVersion,
+      },
+      { capabilities: { tools: { listChanged: true }, logging: {} } },
+    );
+  }
+
+  public async start(app: Application) {
+    await this.registerAllGcliTools();
+
+    // NEW: ‰ΩøÁî®Êó•Âøó‰∏≠Èó¥‰ª∂
+    app.use(requestLogger);
+
+    const transports: Record<string, StreamableHTTPServerTransport> = {};
+
+    app.all('/mcp', async (req: Request, res: Response) => {
+      const sessionId = req.headers['mcp-session-id'] as string | undefined;
+      let transport = sessionId ? transports[sessionId] : undefined;
+
+      if (!transport) {
+        if (isInitializeRequest(req.body)) {
+          console.log(
+            `${LOG_PREFIX} creating new transport for initialize request.`,
+          );
+          transport = new StreamableHTTPServerTransport({
+            sessionIdGenerator: () => randomUUID(),
+            onsessioninitialized: newSessionId => {
+              console.log(
+                `${LOG_PREFIX} ‚úÖ Session initialized with ID: ${newSessionId}`,
+              );
+              transports[newSessionId] = transport!;
+            },
+          });
+
+          transport.onclose = () => {
+            const sid = transport!.sessionId;
+            if (sid && transports[sid]) {
+              console.log(
+                `${LOG_PREFIX} üö™ Transport for session ${sid} closed.`,
+              );
+              delete transports[sid];
+            }
+          };
+
+          // Connect the new transport to the *existing* McpServer
+          await this.mcpServer.connect(transport);
+        } else {
+          console.error(
+            `${LOG_PREFIX} ‚ùå Bad Request: Missing or invalid session ID for non-initialize request.`,
+          );
+          res.status(400).json({
+            jsonrpc: '2.0',
+            error: {
+              code: -32000,
+              message: 'Bad Request: Missing or invalid session ID.',
+            },
+            id: null,
+          });
+          return;
+        }
+      } else {
+        console.log(
+          `${LOG_PREFIX}  reusing transport for session: ${sessionId}`,
+        );
+      }
+
+      try {
+        await transport.handleRequest(req, res, req.body);
+      } catch (e) {
+        console.error(`${LOG_PREFIX} üí• Error handling request:`, e);
+        if (!res.headersSent) {
+          res.status(500).end();
+        }
+      }
+    });
+  }
+
+  private async registerAllGcliTools() {
+    const toolRegistry = await this.config.getToolRegistry();
+    const allTools = toolRegistry.getAllTools();
+    for (const tool of allTools) {
+      this.registerGcliTool(tool);
+    }
+  }
+
+  private registerGcliTool(tool: GcliTool) {
+    const inputSchema = this.convertJsonSchemaToZod(tool.schema.parameters);
+
+    this.mcpServer.registerTool(
+      tool.name,
+      {
+        title: tool.displayName,
+        description: tool.description,
+        inputSchema: inputSchema,
+      },
+      async (args, extra) => {
+        const result = await tool.execute(args, extra.signal);
+        return this.convertGcliResultToMcpResult(result);
+      },
+    );
+  }
+
+
+  private convertJsonSchemaToZod(jsonSchema: any): any {
+    // Helper to convert a single JSON schema property to a Zod type.
+    // This is defined as an inner arrow function to recursively call itself for arrays
+    // and to call the outer function for nested objects via `this`.
+    const convertProperty = (prop: any): z.ZodTypeAny => {
+      if (!prop || !prop.type) {
+        return z.any();
+      }
+
+      switch (prop.type) {
+        case 'string':
+          return z.string().describe(prop.description || '');
+        case 'number':
+          return z.number().describe(prop.description || '');
+        case 'boolean':
+          return z.boolean().describe(prop.description || '');
+        case 'array':
+          // This is the key fix: recursively call the converter for `items`.
+          if (!prop.items) {
+            // A valid array schema MUST have `items`. Fallback to `any` if missing.
+            return z.array(z.any()).describe(prop.description || '');
+          }
+          return z
+            .array(convertProperty(prop.items))
+            .describe(prop.description || '');
+        case 'object':
+          // For nested objects, recursively call the main function to get the shape.
+          return z
+            .object(this.convertJsonSchemaToZod(prop))
+            .passthrough()
+            .describe(prop.description || '');
+        default:
+          return z.any();
+      }
+    };
+
+    // If no schema or properties, return an empty shape object.
+    if (!jsonSchema || !jsonSchema.properties) {
+      return {};
+    }
+
+    const shape: any = {};
+    for (const [key, prop] of Object.entries(jsonSchema.properties)) {
+      let fieldSchema = convertProperty(prop as any);
+
+      if (!jsonSchema.required || !jsonSchema.required.includes(key)) {
+        fieldSchema = fieldSchema.optional();
+      }
+      shape[key] = fieldSchema;
+    }
+    return shape; // Directly return the shape object.
+  }
+
+  private convertGcliResultToMcpResult(
+    gcliResult: ToolResult,
+  ): CallToolResult {
+    if (typeof gcliResult.llmContent === 'string') {
+      return { content: [{ type: 'text', text: gcliResult.llmContent }] };
+    }
+
+    const parts = Array.isArray(gcliResult.llmContent)
+      ? gcliResult.llmContent
+      : [gcliResult.llmContent];
+
+    const contentBlocks = parts.map((part: PartUnion) => {
+      if (typeof part === 'string') {
+        return { type: 'text' as const, text: part };
+      }
+      if ('text' in part && part.text) {
+        return { type: 'text' as const, text: part.text };
+      }
+      return { type: 'text' as const, text: '[Unsupported Part Type]' };
+    });
+
+    return { content: contentBlocks };
+  }
+}
diff --git a/packages/mcp-server/src/bridge/index.ts b/packages/mcp-server/src/bridge/index.ts
new file mode 100644
index 00000000..08ea4b6b
--- /dev/null
+++ b/packages/mcp-server/src/bridge/index.ts
@@ -0,0 +1 @@
+export { GcliMcpBridge } from './bridge.js';
diff --git a/packages/mcp-server/src/bridge/openai.ts b/packages/mcp-server/src/bridge/openai.ts
new file mode 100644
index 00000000..1660fe00
--- /dev/null
+++ b/packages/mcp-server/src/bridge/openai.ts
@@ -0,0 +1,139 @@
+import { Router, Request, Response } from 'express';
+import { type Config, GeminiChat } from '@google/gemini-cli-core';
+import { createOpenAIStreamTransformer } from './stream-transformer.js';
+import { type Content } from '@google/genai';
+import { WritableStream } from 'node:stream/web';
+import { randomUUID } from 'node:crypto';
+
+// ÂÆö‰πâ OpenAI ËØ∑Ê±Ç‰ΩìÁöÑÊé•Âè£
+interface OpenAIChatCompletionRequest {
+  model: string;
+  messages: Array<{ role: string; content: string }>;
+  stream?: boolean;
+}
+
+export function createOpenAIRouter(config: Config): Router {
+  const router = Router();
+  // Ê≥®ÊÑèÔºöÂü∫‰∫é bridge.ts ÁöÑÁé∞Êúâ‰ª£Á†ÅÔºåÊàë‰ª¨ÂÅáËÆæ getGeminiClient Âíå getContentGenerator ÊñπÊ≥ïÂ≠òÂú®„ÄÇ
+  const contentGenerator = config.getGeminiClient().getContentGenerator();
+
+  // OpenAI chat completions Á´ØÁÇπ
+  router.post(
+    '/chat/completions',
+    async (req: Request, res: Response) => {
+      const body = req.body as OpenAIChatCompletionRequest;
+
+      // Á°Æ‰øù stream ÈªòËÆ§‰∏∫ trueÔºåÈô§ÈùûÊòæÂºèËÆæÁΩÆ‰∏∫ false
+      const stream = body.stream !== false;
+
+      if (!body.messages || body.messages.length === 0) {
+        res.status(400).json({ error: 'messages is required' });
+        return;
+      }
+
+      // Â∞Ü OpenAI Ê†ºÂºèÁöÑ messages ËΩ¨Êç¢‰∏∫ Gemini Ê†ºÂºè
+      // Ê≥®ÊÑèÔºöËøôÈáåÁÆÄÂåñ‰∫ÜÂ§ÑÁêÜÔºåÂÆûÈôÖÂèØËÉΩÈúÄË¶ÅÂ§ÑÁêÜ system prompt Á≠â
+      const history: Content[] = body.messages.map(msg => ({
+        role: msg.role === 'assistant' ? 'model' : 'user',
+        parts: [{ text: msg.content }],
+      }));
+
+      const lastMessage = history.pop();
+      if (!lastMessage) {
+        res.status(400).json({ error: 'No message to send.' });
+        return;
+      }
+
+      try {
+        const oneShotChat = new GeminiChat(
+          config,
+          contentGenerator,
+          {}, // generationConfig
+          history,
+        );
+
+        if (stream) {
+          // --- ÊµÅÂºèÂìçÂ∫î ---
+          res.setHeader('Content-Type', 'text/event-stream');
+          res.setHeader('Cache-Control', 'no-cache');
+          res.setHeader('Connection', 'keep-alive');
+          res.flushHeaders(); // Á´ãÂç≥ÂèëÈÄÅÂ§¥‰ø°ÊÅØ
+
+          const geminiStream = await oneShotChat.sendMessageStream({
+            message: lastMessage.parts || [],
+          });
+          const openAIStream = createOpenAIStreamTransformer(body.model);
+
+          const writer = new WritableStream({
+            write(chunk) {
+              res.write(chunk);
+            },
+          });
+
+          const readableStream = new ReadableStream({
+            async start(controller) {
+              for await (const value of geminiStream) {
+                controller.enqueue(value);
+              }
+              controller.close();
+            },
+          });
+
+          await readableStream.pipeThrough(openAIStream).pipeTo(writer);
+
+          res.end();
+        } else {
+          // --- ÈùûÊµÅÂºèÂìçÂ∫îÔºà‰∏∫‰∫ÜÂÆåÊï¥ÊÄßÔºâ ---
+          const result = await oneShotChat.sendMessage({
+            message: lastMessage.parts || [],
+          });
+          const responseText =
+            result.candidates?.[0]?.content?.parts?.[0]?.text || '';
+
+          res.json({
+            id: `chatcmpl-${randomUUID()}`,
+            object: 'chat.completion',
+            created: Math.floor(Date.now() / 1000),
+            model: body.model,
+            choices: [
+              {
+                index: 0,
+                message: {
+                  role: 'assistant',
+                  content: responseText,
+                },
+                finish_reason: 'stop',
+              },
+            ],
+          });
+        }
+      } catch (error) {
+        console.error('[OpenAI Bridge] Error:', error);
+        const errorMessage =
+          error instanceof Error ? error.message : 'An unknown error occurred';
+        if (!res.headersSent) {
+          res.status(500).json({ error: errorMessage });
+        } else {
+          // Â¶ÇÊûúÂ§¥Â∑≤ÂèëÈÄÅÔºåÂè™ËÉΩÂ∞ùËØïÂÜôÂÖ•ÈîôËØØ‰ø°ÊÅØÂπ∂ÁªìÊùüÊµÅ
+          res.write(`data: ${JSON.stringify({ error: errorMessage })}\n\n`);
+          res.end();
+        }
+      }
+    },
+  );
+
+  // ÂèØ‰ª•Ê∑ªÂä† /v1/models Á´ØÁÇπ
+  router.get('/models', (req, res) => {
+    // ËøôÈáåÂèØ‰ª•ËøîÂõû‰∏Ä‰∏™Âõ∫ÂÆöÁöÑÊ®°ÂûãÂàóË°®ÔºåÊàñËÄÖ‰ªé config ‰∏≠Ëé∑Âèñ
+    res.json({
+      object: 'list',
+      data: [
+        { id: 'gemini-1.5-pro-latest', object: 'model', owned_by: 'google' },
+        { id: 'gemini-1.5-flash-latest', object: 'model', owned_by: 'google' },
+        { id: 'gemini-pro', object: 'model', owned_by: 'google' },
+      ],
+    });
+  });
+
+  return router;
+}
diff --git a/packages/mcp-server/src/bridge/stream-transformer.ts b/packages/mcp-server/src/bridge/stream-transformer.ts
new file mode 100644
index 00000000..e075edeb
--- /dev/null
+++ b/packages/mcp-server/src/bridge/stream-transformer.ts
@@ -0,0 +1,73 @@
+import { GenerateContentResponse } from '@google/genai';
+
+// ÂÆö‰πâ OpenAI ÊµÅÂºèÂìçÂ∫îÁöÑÂùóÁªìÊûÑ
+interface OpenAIDelta {
+  role?: 'user' | 'assistant' | 'system';
+  content?: string;
+}
+
+interface OpenAIChoice {
+  index: number;
+  delta: OpenAIDelta;
+  finish_reason: string | null;
+}
+
+interface OpenAIChunk {
+  id: string;
+  object: 'chat.completion.chunk';
+  created: number;
+  model: string;
+  choices: OpenAIChoice[];
+}
+
+/**
+ * ÂàõÂª∫‰∏Ä‰∏™ TransformStreamÔºåÂ∞Ü Gemini ÁöÑÊµÅÂºèÂìçÂ∫îÂùóËΩ¨Êç¢‰∏∫ OpenAI ÂÖºÂÆπÁöÑ SSE ‰∫ã‰ª∂„ÄÇ
+ * @param model - Ê≠£Âú®‰ΩøÁî®ÁöÑÊ®°ÂûãÂêçÁß∞ÔºåÁî®‰∫éÂ°´ÂÖÖ OpenAI ÂìçÂ∫î„ÄÇ
+ * @returns ‰∏Ä‰∏™ TransformStream ÂÆû‰æã„ÄÇ
+ */
+export function createOpenAIStreamTransformer(
+  model: string,
+): TransformStream<GenerateContentResponse, Uint8Array> {
+  const chatID = `chatcmpl-${crypto.randomUUID()}`;
+  const creationTime = Math.floor(Date.now() / 1000);
+  const encoder = new TextEncoder();
+  let isFirstChunk = true;
+
+  return new TransformStream({
+    transform(chunk, controller) {
+      const text = chunk.candidates?.[0]?.content?.parts?.[0]?.text;
+      if (!text) {
+        return;
+      }
+
+      const delta: OpenAIDelta = { content: text };
+      if (isFirstChunk) {
+        delta.role = 'assistant';
+        isFirstChunk = false;
+      }
+
+      const openAIChunk: OpenAIChunk = {
+        id: chatID,
+        object: 'chat.completion.chunk',
+        created: creationTime,
+        model: model,
+        choices: [
+          {
+            index: 0,
+            delta: delta,
+            finish_reason: null,
+          },
+        ],
+      };
+
+      // ÊåâÁÖß SSE Ê†ºÂºèÁºñÁ†Å
+      const sseString = `data: ${JSON.stringify(openAIChunk)}\n\n`;
+      controller.enqueue(encoder.encode(sseString));
+    },
+    flush(controller) {
+      // ÊµÅÁªìÊùüÊó∂ÔºåÂèëÈÄÅ [DONE] Ê∂àÊÅØ
+      const doneString = `data: [DONE]\n\n`;
+      controller.enqueue(encoder.encode(doneString));
+    },
+  });
+}
diff --git a/packages/mcp-server/src/index.ts b/packages/mcp-server/src/index.ts
new file mode 100644
index 00000000..668c7360
--- /dev/null
+++ b/packages/mcp-server/src/index.ts
@@ -0,0 +1,174 @@
+import {
+  Config,
+  ApprovalMode,
+  sessionId,
+  loadServerHierarchicalMemory,
+  FileDiscoveryService,
+  DEFAULT_GEMINI_MODEL,
+  DEFAULT_GEMINI_EMBEDDING_MODEL,
+  MCPServerConfig,
+  AuthType,
+} from '@google/gemini-cli-core';
+import {
+  loadSettings,
+  type Settings,
+  loadExtensions,
+  type Extension,
+  getCliVersion,
+  loadEnvironment,
+  loadSandboxConfig,
+} from '@google/gemini-cli/public-api';
+import { GcliMcpBridge } from './bridge/bridge.js';
+import { createOpenAIRouter } from './bridge/openai.js';
+import express from 'express';
+
+// Simple console logger for now
+const logger = {
+  // eslint-disable-next-line @typescript-eslint/no-explicit-any
+  warn: (...args: any[]) => console.warn('[WARN]', ...args),
+};
+
+function mergeMcpServers(
+  settings: Settings,
+  extensions: Extension[],
+): Record<string, MCPServerConfig> {
+  const mcpServers: Record<string, MCPServerConfig> = {
+    ...(settings.mcpServers || {}),
+  };
+  for (const extension of extensions) {
+    Object.entries(extension.config.mcpServers || {}).forEach(
+      ([key, server]) => {
+        if (mcpServers[key]) {
+          logger.warn(
+            `Skipping extension MCP config for server with key "${key}" as it already exists.`,
+          );
+          return;
+        }
+        mcpServers[key] = server;
+      },
+    );
+  }
+  return mcpServers;
+}
+
+async function startMcpServer() {
+  // 1. Áã¨Á´ãÁöÑ„ÄÅÁÆÄÂçïÁöÑÂèÇÊï∞Ëß£Êûê
+  const args = process.argv.slice(2);
+  const portArg = args.find(arg => arg.startsWith('--port='));
+  const port = portArg ? parseInt(portArg.split('=')[1], 10) : 8765;
+  const debugMode = args.includes('--debug');
+
+  if (isNaN(port)) {
+    console.error('Invalid port number provided. Use --port=<number>.');
+    process.exit(1);
+  }
+
+  console.log('Starting Gemini CLI in MCP Server Mode...');
+
+  // 2. Â§çÁî®ÈÖçÁΩÆÂä†ËΩΩÁöÑÊ†∏ÂøÉÈÉ®ÂàÜÔºå‰ΩÜÊâãÂä®ÊûÑÈÄ† Config
+  loadEnvironment(); // Âä†ËΩΩ .env Êñá‰ª∂
+  const workspaceRoot = process.cwd();
+  const settings = loadSettings(workspaceRoot);
+  const extensions = loadExtensions(workspaceRoot);
+  const cliVersion = await getCliVersion();
+
+  // 3. ÊâãÂä®ÊûÑÈÄ† ConfigParametersÔºåÁªïÂºÄ yargs
+  const fileDiscoveryService = new FileDiscoveryService(workspaceRoot);
+  const extensionContextFilePaths = extensions.flatMap(e => e.contextFiles);
+  const { memoryContent, fileCount } = await loadServerHierarchicalMemory(
+    workspaceRoot,
+    debugMode,
+    fileDiscoveryService,
+    extensionContextFilePaths,
+  );
+
+  const mockArgvForSandbox = {};
+  const sandboxConfig = await loadSandboxConfig(
+    settings.merged,
+    mockArgvForSandbox,
+  );
+
+  const mcpServers = mergeMcpServers(settings.merged, extensions);
+
+  const config = new Config({
+    sessionId: sessionId,
+    model: process.env.GEMINI_MODEL || DEFAULT_GEMINI_MODEL,
+    embeddingModel: DEFAULT_GEMINI_EMBEDDING_MODEL,
+    targetDir: workspaceRoot,
+    cwd: workspaceRoot,
+    debugMode: debugMode,
+    approvalMode: ApprovalMode.YOLO, // Âº∫Âà∂‰∏∫ YOLO Ê®°Âºè
+    sandbox: sandboxConfig,
+    userMemory: memoryContent,
+    geminiMdFileCount: fileCount,
+    fileDiscoveryService,
+    coreTools: settings.merged.coreTools,
+    excludeTools: settings.merged.excludeTools,
+    toolDiscoveryCommand: settings.merged.toolDiscoveryCommand,
+    toolCallCommand: settings.merged.toolCallCommand,
+    mcpServers: mcpServers,
+    extensionContextFilePaths,
+    showMemoryUsage: settings.merged.showMemoryUsage,
+    accessibility: settings.merged.accessibility,
+    telemetry: settings.merged.telemetry,
+    usageStatisticsEnabled: settings.merged.usageStatisticsEnabled,
+    fileFiltering: settings.merged.fileFiltering,
+    checkpointing: settings.merged.checkpointing?.enabled,
+    proxy:
+      process.env.HTTPS_PROXY ||
+      process.env.https_proxy ||
+      process.env.HTTP_PROXY ||
+      process.env.http_proxy,
+    bugCommand: settings.merged.bugCommand,
+  });
+
+  // Initialize Auth - this is critical to initialize the tool registry and gemini client
+  let selectedAuthType = settings.merged.selectedAuthType;
+  if (!selectedAuthType && !process.env.GEMINI_API_KEY) {
+    console.error(
+      'Auth missing: Please set `selectedAuthType` in .gemini/settings.json or set the GEMINI_API_KEY environment variable.',
+    );
+    process.exit(1);
+  }
+  selectedAuthType = selectedAuthType || AuthType.USE_GEMINI;
+  await config.refreshAuth(selectedAuthType);
+  console.log(`Using authentication method: ${selectedAuthType}`);
+
+  // Check for the custom tools model environment variable
+  const toolsDefaultModel = process.env.GEMINI_TOOLS_DEFAULT_MODEL;
+  if (toolsDefaultModel && toolsDefaultModel.trim() !== '') {
+    config.setModel(toolsDefaultModel.trim());
+    console.log(`üöÄ Using custom model for tools: ${toolsDefaultModel.trim()}`);
+  } else {
+    // Log the default model being used if the environment variable is not set
+    console.log(`‚öôÔ∏è  Using default model for tools: ${config.getModel()}`);
+  }
+
+  // 4. ÂàùÂßãÂåñÂπ∂ÂêØÂä® MCP Ê°•Êé•ÊúçÂä° Âíå OpenAI ÊúçÂä°
+  const mcpBridge = new GcliMcpBridge(config, cliVersion);
+
+  const app = express();
+  app.use(express.json());
+
+  // ÂêØÂä® MCP ÊúçÂä° (ËøôÊòØ GcliMcpBridge ÁöÑ‰∏ÄÈÉ®ÂàÜÔºåÊàë‰ª¨ÈúÄË¶ÅÊääÂÆÉÈõÜÊàêÂà∞‰∏ª app ‰∏≠)
+  await mcpBridge.start(app); // ‰øÆÊîπ start ÊñπÊ≥ï‰ª•Êé•Êî∂ express app ÂÆû‰æã
+
+  // ÂêØÂä® OpenAI ÂÖºÂÆπÁ´ØÁÇπ
+  const openAIRouter = createOpenAIRouter(config);
+  app.use('/v1', openAIRouter);
+
+  app.listen(port, () => {
+    console.log(
+      `üöÄ Gemini CLI MCP Server and OpenAI Bridge are running on port ${port}`,
+    );
+    console.log(`   - MCP transport listening on http://localhost:${port}/mcp`);
+    console.log(
+      `   - OpenAI-compatible endpoints available at http://localhost:${port}/v1`,
+    );
+  });
+}
+
+startMcpServer().catch(error => {
+  console.error('Failed to start Gemini CLI MCP Bridge:', error);
+  process.exit(1);
+});
diff --git a/packages/mcp-server/src/mcp-test-client.ts b/packages/mcp-server/src/mcp-test-client.ts
new file mode 100644
index 00000000..2a019d32
--- /dev/null
+++ b/packages/mcp-server/src/mcp-test-client.ts
@@ -0,0 +1,202 @@
+import { Client } from '@modelcontextprotocol/sdk/client/index.js';
+import { StreamableHTTPClientTransport } from '@modelcontextprotocol/sdk/client/streamableHttp.js';
+import {
+  ListToolsResultSchema,
+  type Notification,
+} from '@modelcontextprotocol/sdk/types.js'; // <--- ÂºïÂÖ• Notification Á±ªÂûã
+import { URL } from 'url';
+import { z } from 'zod';
+import OpenAI from 'openai';
+
+// Define the schema for a text content block, as it's not exported by the SDK.
+const TextContentBlockSchema = z.object({
+  type: z.literal('text'),
+  text: z.string(),
+});
+
+// --- ÈÖçÁΩÆ ---
+const SERVER_URL = 'http://localhost:8765/mcp';
+const LOG_PREFIX = '[TEST CLIENT]';
+// -------------
+
+function logWithPrefix(...args: unknown[]) {
+  console.log(LOG_PREFIX, ...args);
+}
+
+// --- Áå¥Â≠êË°•‰∏Å fetch ---
+const originalFetch = global.fetch;
+global.fetch = async (url, options) => {
+  logWithPrefix('‚û°Ô∏è  FETCHING:', options?.method || 'GET', url.toString());
+  if (options?.headers) {
+    logWithPrefix(
+      '   Headers:',
+      JSON.stringify(
+        Object.fromEntries((options.headers as Headers).entries()),
+        null,
+        2,
+      ),
+    );
+  }
+  if (options?.body) {
+    const bodyStr = options.body.toString();
+    logWithPrefix(
+      '   Body:',
+      bodyStr.length > 300 ? bodyStr.substring(0, 300) + '...' : bodyStr,
+    );
+  }
+
+  const response = await originalFetch(url, options);
+
+  logWithPrefix(
+    '‚¨ÖÔ∏è  RESPONSE:',
+    response.status,
+    response.statusText,
+    'from',
+    options?.method || 'GET',
+    url.toString(),
+  );
+  logWithPrefix(
+    '   Response Headers:',
+    JSON.stringify(Object.fromEntries(response.headers.entries()), null, 2),
+  );
+
+  const clonedResponse = response.clone();
+  clonedResponse
+    .text()
+    .then(text => {
+      if (text) {
+        logWithPrefix(
+          '   Response Body:',
+          text.length > 300 ? text.substring(0, 300) + '...' : text,
+        );
+      }
+    })
+    .catch(() => {});
+
+  return response;
+};
+// -----------------------
+
+async function runTestClient() {
+  logWithPrefix('üöÄ Starting MCP Test Client...');
+  logWithPrefix(`üéØ Target Server URL: ${SERVER_URL}`);
+
+  const client = new Client({
+    name: 'mcp-debug-client',
+    version: '1.0.0',
+  });
+
+  client.onerror = (error: Error) => {
+    console.error(`${LOG_PREFIX} üí• Client-level Error:`, error);
+  };
+
+  // --- ‰øÆÊ≠£ÁöÑÈÉ®ÂàÜ ---
+  // Â∞ÜÂèÇÊï∞Á±ªÂûã‰ªé JSONRPCMessage Êîπ‰∏∫ Notification
+  client.fallbackNotificationHandler = async (notification: Notification) => {
+    logWithPrefix(
+      `üì° Received Unhandled Notification:`,
+      JSON.stringify(notification, null, 2),
+    );
+  };
+  // -------------------
+
+  logWithPrefix('üöå Creating StreamableHTTPClientTransport...');
+  const transport = new StreamableHTTPClientTransport(new URL(SERVER_URL));
+
+  transport.onmessage = message => {
+    logWithPrefix('üì• Received Message:', JSON.stringify(message, null, 2));
+  };
+
+  try {
+    logWithPrefix('üîå Attempting to connect to server...');
+    await client.connect(transport);
+    logWithPrefix('‚úÖ Connection successful! Initialization complete.');
+    logWithPrefix('üîé Server Info:', client.getServerVersion());
+    logWithPrefix('üõ†Ô∏è Server Capabilities:', client.getServerCapabilities());
+  } catch (error) {
+    console.error(`${LOG_PREFIX} ‚ùå Failed to connect or initialize:`, error);
+    process.exit(1);
+  }
+
+  try {
+    logWithPrefix('üìã Requesting tool list...');
+    const result = await client.request(
+      { method: 'tools/list' },
+      ListToolsResultSchema,
+    );
+
+    logWithPrefix('‚úÖ Successfully received tool list response!');
+
+    if (result.tools && result.tools.length > 0) {
+      logWithPrefix(`üõ†Ô∏è Discovered ${result.tools.length} tools:`);
+      result.tools.forEach((tool, index) => {
+        logWithPrefix(`  ${index + 1}. Name: ${tool.name}`);
+        logWithPrefix(`     Title: ${tool.title || 'N/A'}`);
+        logWithPrefix(`     Description: ${tool.description || 'N/A'}`);
+        logWithPrefix(
+          `     Input Schema:`,
+          JSON.stringify(tool.inputSchema, null, 2),
+        );
+      });
+    } else {
+      logWithPrefix('‚ö†Ô∏è Server returned an empty list of tools.');
+    }
+  } catch (error) {
+    console.error(`${LOG_PREFIX} ‚ùå Failed to list tools:`, error);
+  } finally {
+    logWithPrefix('üëã Closing MCP connection...');
+    await client.close();
+    logWithPrefix('üö™ MCP Connection closed.');
+  }
+
+  // Now, test the OpenAI endpoint
+  await testOpenAIEndpoint();
+
+  logWithPrefix('‚úÖ Test finished.');
+}
+
+async function testOpenAIEndpoint() {
+  logWithPrefix('-----------------------------------');
+  logWithPrefix('üöÄ Testing OpenAI compatible endpoint...');
+
+  const openai = new OpenAI({
+    baseURL: 'http://localhost:8765/v1',
+    apiKey: 'not-needed', // The API key is not used by our local server
+  });
+
+  try {
+    const stream = await openai.chat.completions.create({
+      model: 'gemini-pro', // This can be any string, it's passed to the transformer
+      messages: [{ role: 'user', content: 'Why is the sky blue?' }],
+      stream: true,
+    });
+
+    let fullResponse = '';
+    logWithPrefix('‚úÖ Stream opened. Receiving response...');
+    for await (const chunk of stream) {
+      const content = chunk.choices[0]?.delta?.content || '';
+      fullResponse += content;
+      process.stdout.write(content);
+    }
+    console.log(''); // Newline after stream
+
+    if (fullResponse.toLowerCase().includes('scattering')) {
+      logWithPrefix('‚úÖ Validation successful: Response contains "scattering".');
+    } else {
+      console.error(
+        `${LOG_PREFIX} ‚ùå Validation failed: Response did not contain "scattering".`,
+      );
+    }
+  } catch (error) {
+    console.error(
+      `${LOG_PREFIX} ‚ùå Failed to call OpenAI endpoint:`,
+      error,
+    );
+  }
+  logWithPrefix('-----------------------------------');
+}
+
+runTestClient().catch(error => {
+  console.error(`${LOG_PREFIX} üö® Unhandled top-level error:`, error);
+  process.exit(1);
+});
diff --git a/packages/mcp-server/tsconfig.json b/packages/mcp-server/tsconfig.json
new file mode 100644
index 00000000..e0fb72ce
--- /dev/null
+++ b/packages/mcp-server/tsconfig.json
@@ -0,0 +1,18 @@
+{
+  "extends": "../../tsconfig.json",
+  "compilerOptions": {
+    "outDir": "dist",
+    "rootDir": "src"
+  },
+  "include": [
+    "src/**/*.ts"
+  ],
+  "references": [
+    {
+      "path": "../core"
+    },
+    {
+      "path": "../cli"
+    }
+  ]
+}
