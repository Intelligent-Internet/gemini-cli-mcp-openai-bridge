# Gemini CLI - MCP / OpenAI Bridge Server (`@gemini-community/gemini-mcp-server`)

`@gemini-community/gemini-mcp-server` is a versatile companion application designed to serve as a powerful extension for the `gemini-cli` ecosystem. It primarily fulfills two core roles:

1.  **MCP (Model-Context Protocol) Server**: It hosts and exposes `gemini-cli`'s powerful built-in tools (e.g., `google_web_search`, file system tools) via a standard, discoverable protocol. This allows the core `gemini-cli` model to invoke these tools as needed.

2.  **OpenAI-Compatible API Bridge**: It provides an endpoint compatible with the OpenAI Chat Completions API. This enables any third-party tool or application that supports the OpenAI API (such as [Open WebUI](https://github.com/open-webui/open-webui)) to seamlessly interact with the underlying Gemini model of `gemini-cli`, including full support for streaming responses.

## Core Design Philosophy

The server is built on a principle of **minimal modification and maximum reuse**. It is not a reimplementation of `gemini-cli`'s features but is instead intelligently built on top of the `@google/gemini-cli-core` package.

By reusing the `Config` and `GeminiClient` classes from the core package, the `mcp-server` inherits all of the essential business logic, tool execution capabilities, and configuration management of the main CLI. This design ensures behavioral consistency and simplifies maintenance and extension.

## Features

-   **Hosts Native `gemini-cli` Tools**: Exposes the built-in tools (file system operations, web fetching, web search, etc.) to the `gemini-cli` model via the MCP protocol.
-   **OpenAI API Compatibility**: Provides `/v1/chat/completions` and `/v1/models` endpoints, allowing third-party applications to interact with the Gemini model as if it were an OpenAI service.
-   **Streaming Support**: Fully supports streaming responses, pushing real-time generation results from the Gemini model to the client via Server-Sent Events (SSE).
-   **Flexible Model Configuration**: Allows a separate, default LLM model to be configured via an environment variable specifically for tools hosted by the server (e.g., for summarizing search results).
-   **Inherited Configuration & Authentication**: Automatically uses the same settings and authentication state as your main `gemini-cli` setup.
-   **Forced YOLO Mode**: Runs in a permanent "YOLO" mode, automatically approving all tool calls for streamlined, non-interactive use by other applications.

## Architecture & Interaction Flow

The `mcp-server` operates as a standalone component within the `gemini-cli` ecosystem with the following interaction flow:

1.  **Configuration Loading**: On startup, the server loads user and workspace `settings.json` files and reads environment variables, just like the main `gemini-cli` application, to initialize an instance of the `@google/gemini-cli-core` `Config` class.
2.  **Authentication**: The server **does not** handle its own authentication. It relies entirely on the established authentication state of `gemini-cli` (see the next section for details).
3.  **MCP Service**: It starts an MCP server, which the `gemini-cli` can connect to when it needs to discover and execute tools.
4.  **OpenAI Bridge**: It starts an Express web server that listens for API requests in the OpenAI format.
5.  **Request Handling**:
    -   When an OpenAI-formatted request is received, the server converts it into a format that `gemini-cli-core` can understand.
    -   It uses the reused `Config` instance to get a `GeminiClient`.
    -   A **new, isolated `GeminiChat` session** is created for each incoming API request to prevent conversation history from leaking between different clients.
    -   The request is sent to the Gemini API via the `GeminiClient`.
    -   If the Gemini API's response is streaming, the server transforms it into an OpenAI-compatible SSE stream; otherwise, it returns a complete JSON response.

## Authentication Mechanism

Crucially, the `mcp-server` **does not manage its own credentials**. It shares the exact same authentication mechanism as the main `gemini-cli` tool to ensure seamless and secure operation.

The source of authentication credentials follows the identical priority and lookup logic as `gemini-cli`:

-   **Cached Credentials**: If you have previously logged in through the interactive `gemini-cli` flow (e.g., `gcloud auth application-default login` or OAuth web login), the `mcp-server` will automatically use the cached credentials stored in `~/.config/gcloud` or `~/.gemini`.
-   **Environment Variables**: The server will look for and use standard Google Cloud and Gemini environment variables, such as:
    -   `GEMINI_API_KEY`
    -   `GOOGLE_APPLICATION_CREDENTIALS`
    -   `GOOGLE_CLOUD_PROJECT`

This means that as long as your `gemini-cli` itself is configured correctly and works, the `mcp-server` will be authorized automatically, with no extra authentication steps required.

## Important Security Note: YOLO Mode

The `mcp-server` is designed for non-interactive, programmatic use. As such, it runs with a permanent **YOLO (You Only Live Once) Mode** enabled (`approvalMode: ApprovalMode.YOLO`).

This means that any tool call requested by the model (e.g., `run_shell_command`, `replace`) will be **executed immediately without user confirmation**.

**Warning:** Be extremely cautious when exposing this server to a network. Any client that can reach the server will be able to execute tools with the same permissions as the user running the server process.

## Configuration Options

You can configure the server's behavior via command-line arguments and environment variables.

### Command-Line Arguments

-   `--port=<number>`: Specifies the port for the server to listen on.
    -   **Default**: `8765`
-   `--debug`: Enables detailed debug logging to the console.

### Environment Variables

-   `GEMINI_TOOLS_DEFAULT_MODEL`: Sets a default LLM model specifically for tools hosted by the server (like `google_web_search`).
    -   **Purpose**: When a tool needs to invoke an LLM during its execution (e.g., to summarize search results), it will use the model specified by this variable. This allows you to use a different (potentially faster or cheaper) model for tool execution than for the main chat.
    -   **Example**: `GEMINI_TOOLS_DEFAULT_MODEL=gemini-1.5-flash`

## Usage

### 1. Installation & Build

From the root of the `gemini-cli` project, ensure all dependencies are installed and then build the `mcp-server` package.

```bash
# From the project root
npm install
npm run build --workspace=@gemini-community/gemini-mcp-server
```

### 2. Starting the Server

You can start the server using the `npm run start` command from the root directory, targeting the workspace.

```bash
# Start the server with default configuration
npm run start --workspace=@gemini-community/gemini-mcp-server

# Start on a different port with debug mode enabled
npm run start --workspace=@gemini-community/gemini-mcp-server -- --port=9000 --debug

# Use a faster model for tool calls
GEMINI_TOOLS_DEFAULT_MODEL=gemini-1.5-flash npm run start --workspace=@gemini-community/gemini-mcp-server
```

When the server starts successfully, you will see output similar to this:

```
ğŸš€ Gemini CLI MCP Server and OpenAI Bridge are running on port 8765
   - MCP transport listening on http://localhost:8765/mcp
   - OpenAI-compatible endpoints available at http://localhost:8765/v1
âš™ï¸  Using default model for tools: gemini-2.5-pro
```

### 3. Testing the Endpoints

You can use `curl` or any API client to test the server.

**Test OpenAI Chat Completions (Streaming)**:

```bash
curl -N http://localhost:8765/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gemini-pro",
    "messages": [{"role": "user", "content": "Tell me a short story about a robot who learns to paint."}],
    "stream": true
  }'
```

**Test OpenAI Chat Completions (Non-Streaming)**:

```bash
curl http://localhost:8765/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gemini-pro",
    "messages": [{"role": "user", "content": "Why is the sky blue?"}],
    "stream": false
  }'
```

## Telemetry, Terms of Service, and Privacy

### Telemetry

The `@gemini-community/gemini-mcp-server` **does not introduce any new telemetry or data collection mechanisms**.

It relies entirely on the OpenTelemetry (OTEL) system built into the `@google/gemini-cli-core` package. Therefore, all telemetry data (if enabled) will follow the main `gemini-cli` configuration and be sent to the destination specified in your `settings.json` file.

For details on how to configure and use telemetry, please refer to the [main Gemini CLI telemetry documentation](../../docs/telemetry.md).

### Terms of Service and Privacy Notice

Your use of this server is governed by the Terms of Service and Privacy Policies corresponding to the `gemini-cli` account type you are using for authentication. As a bridge, `@gemini-community/gemini-mcp-server` does not collect, store, or process any additional data of its own.

We strongly recommend you review the [main Gemini CLI Terms of Service and Privacy Notice documentation](../../docs/tos-privacy.md) for details applicable to your account.

---

### Developer Note: Regarding the package name `@gemini-community/gemini-mcp-server`

Please note that the name of this package, `@gemini-community/gemini-mcp-server`, indicates that it is a community-maintained package.

-   **Community Driven**: This package is part of the Gemini Community effort to extend the capabilities of `gemini-cli`.
-   **Publication**: This package may be published to a public npm registry under the `@gemini-community` scope. If you fork this project and wish to publish your own modified version, you **must** change the package name to your own scope (e.g., `@your-username/gemini-mcp-server`) to comply with npm's package naming policies and to avoid confusion.

----

# Gemini CLI - MCP / OpenAI Bridge Server (`@gemini-community/gemini-mcp-server`)

`@gemini-community/gemini-mcp-server` æ˜¯ä¸€ä¸ªå¤šåŠŸèƒ½çš„æœåŠ¡å™¨åº”ç”¨ç¨‹åºï¼Œæ—¨åœ¨ä½œä¸º `gemini-cli` ç”Ÿæ€ç³»ç»Ÿçš„å¼ºå¤§æ‰©å±•ã€‚å®ƒä¸»è¦æ‰¿æ‹…ä¸¤ä¸ªæ ¸å¿ƒè§’è‰²ï¼š

1.  **MCP (Model-Context Protocol) æœåŠ¡å™¨**: å®ƒä¸º `gemini-cli` æ‰˜ç®¡å’Œæš´éœ²äº†ä¸€ç³»åˆ—å¼ºå¤§çš„å†…ç½®å·¥å…·ï¼ˆä¾‹å¦‚ `google_web_search`ï¼‰ï¼Œå…è®¸ `gemini-cli` çš„æ ¸å¿ƒæ¨¡å‹é€šè¿‡ä¸€ä¸ªæ ‡å‡†çš„ã€å¯å‘ç°çš„åè®®æ¥è°ƒç”¨è¿™äº›å·¥å…·ã€‚

2.  **OpenAI å…¼å®¹çš„ API æ¡¥æ¥å™¨**: å®ƒæä¾›äº†ä¸€ä¸ªä¸ OpenAI Chat Completions API å…¼å®¹çš„ç«¯ç‚¹ã€‚è¿™ä½¿å¾—ä»»ä½•æ”¯æŒ OpenAI API çš„ç¬¬ä¸‰æ–¹å·¥å…·æˆ–åº”ç”¨ç¨‹åºï¼ˆä¾‹å¦‚ [Open WebUI](https://github.com/open-webui/open-webui)ï¼‰éƒ½å¯ä»¥æ— ç¼åœ°ä¸ `gemini-cli` çš„åº•å±‚ Gemini æ¨¡å‹è¿›è¡Œäº¤äº’ï¼ŒåŒ…æ‹¬åˆ©ç”¨æµå¼å“åº”ã€‚

## æ ¸å¿ƒè®¾è®¡ç†å¿µ

è¿™ä¸ªæœåŠ¡å™¨çš„æ ¸å¿ƒè®¾è®¡åŸåˆ™æ˜¯ **æœ€å°åŒ–ä¿®æ”¹å’Œæœ€å¤§åŒ–å¤ç”¨**ã€‚å®ƒå¹¶ä¸æ˜¯å¯¹ `gemini-cli` åŠŸèƒ½çš„é‡æ–°å®ç°ï¼Œè€Œæ˜¯å·§å¦™åœ°æ„å»ºåœ¨ `@google/gemini-cli-core` åŒ…ä¹‹ä¸Šã€‚

é€šè¿‡é‡ç”¨ `core` åŒ…ä¸­çš„ `Config` å’Œ `GeminiClient` ç±»ï¼Œ`mcp-server` ç»§æ‰¿äº† `gemini-cli` æ‰€æœ‰çš„æ ¸å¿ƒä¸šåŠ¡é€»è¾‘ã€å·¥å…·æ‰§è¡Œèƒ½åŠ›å’Œé…ç½®ç®¡ç†æœºåˆ¶ã€‚è¿™ç§è®¾è®¡ç¡®ä¿äº†è¡Œä¸ºçš„ä¸€è‡´æ€§ï¼Œå¹¶ä½¿å¾—ç»´æŠ¤å’Œæ‰©å±•å˜å¾—æ›´åŠ ç®€å•ã€‚

## åŠŸèƒ½ç‰¹æ€§

-   **æ‰˜ç®¡åŸç”Ÿ `gemini-cli` å·¥å…·**: é€šè¿‡ MCP åè®®ï¼Œå°† `gemini-cli` çš„å†…ç½®å·¥å…·ï¼ˆå¦‚æ–‡ä»¶ç³»ç»Ÿæ“ä½œã€ç½‘é¡µæŠ“å–ã€ç½‘ç»œæœç´¢ç­‰ï¼‰æš´éœ²ç»™ `gemini-cli` æ¨¡å‹ã€‚
-   **OpenAI API å…¼å®¹æ€§**: æä¾› `/v1/chat/completions` å’Œ `/v1/models` ç«¯ç‚¹ï¼Œå…è®¸ç¬¬ä¸‰æ–¹åº”ç”¨ç¨‹åºåƒä¸ OpenAI å¯¹è¯ä¸€æ ·ä¸ Gemini æ¨¡å‹äº¤äº’ã€‚
-   **æµå¼å“åº”æ”¯æŒ**: å®Œå…¨æ”¯æŒæµå¼å“åº”ï¼Œå¯ä»¥å°† Gemini æ¨¡å‹çš„å®æ—¶ç”Ÿæˆç»“æœé€šè¿‡ SSE (Server-Sent Events) æ¨é€ç»™å®¢æˆ·ç«¯ã€‚
-   **çµæ´»çš„æ¨¡å‹é…ç½®**: å…è®¸é€šè¿‡ç¯å¢ƒå˜é‡ä¸ºæœåŠ¡å™¨æ‰˜ç®¡çš„å·¥å…·ï¼ˆå¦‚ `google_web_search`ï¼‰é…ç½®ä¸€ä¸ªç‰¹å®šçš„ã€ç‹¬ç«‹çš„é»˜è®¤ LLM æ¨¡å‹ã€‚

## æ¶æ„ä¸äº¤äº’æµç¨‹

`mcp-server` ä½œä¸º `gemini-cli` ç”Ÿæ€ç³»ç»Ÿä¸­çš„ä¸€ä¸ªç‹¬ç«‹ç»„ä»¶ï¼Œå…¶äº¤äº’æµç¨‹å¦‚ä¸‹ï¼š

1.  **é…ç½®åŠ è½½**: æœåŠ¡å™¨å¯åŠ¨æ—¶ï¼Œå®ƒä¼šåƒä¸» `gemini-cli` åº”ç”¨ä¸€æ ·ï¼ŒåŠ è½½ç”¨æˆ·å’Œå·¥ä½œåŒºçš„ `settings.json` æ–‡ä»¶ï¼Œå¹¶è¯»å–ç¯å¢ƒå˜é‡æ¥åˆå§‹åŒ–ä¸€ä¸ª `@google/gemini-cli-core` çš„ `Config` å®ä¾‹ã€‚
2.  **è®¤è¯**: æœåŠ¡å™¨**ä¸å¤„ç†**è‡ªå·±çš„è®¤è¯æµç¨‹ã€‚å®ƒå®Œå…¨ä¾èµ–äº `gemini-cli` å·²ç»å»ºç«‹çš„è®¤è¯çŠ¶æ€ï¼ˆè¯¦æƒ…è§ä¸‹ä¸€èŠ‚ï¼‰ã€‚
3.  **MCP æœåŠ¡**: å®ƒå¯åŠ¨ä¸€ä¸ª MCP æœåŠ¡å™¨ï¼Œ`gemini-cli` åœ¨éœ€è¦æ—¶å¯ä»¥è¿æ¥åˆ°è¿™ä¸ªæœåŠ¡å™¨æ¥å‘ç°å’Œæ‰§è¡Œå·¥å…·ã€‚
4.  **OpenAI æ¡¥æ¥**: å®ƒå¯åŠ¨ä¸€ä¸ª Express Web æœåŠ¡å™¨ï¼Œç›‘å¬ OpenAI æ ¼å¼çš„ API è¯·æ±‚ã€‚
5.  **è¯·æ±‚å¤„ç†**:
    -   å½“æ”¶åˆ°ä¸€ä¸ª OpenAI æ ¼å¼çš„è¯·æ±‚æ—¶ï¼ŒæœåŠ¡å™¨ä¼šå°†å…¶è½¬æ¢ä¸º `gemini-cli-core` å¯ä»¥ç†è§£çš„æ ¼å¼ã€‚
    -   å®ƒä½¿ç”¨å¤ç”¨çš„ `Config` å®ä¾‹æ¥è·å–ä¸€ä¸ª `GeminiClient`ã€‚
    -   é€šè¿‡ `GeminiClient` å°†è¯·æ±‚å‘é€ç»™ Gemini APIã€‚
    -   å¦‚æœ Gemini API çš„å“åº”æ˜¯æµå¼çš„ï¼ŒæœåŠ¡å™¨ä¼šå°†å…¶è½¬æ¢ä¸º OpenAI å…¼å®¹çš„ SSE äº‹ä»¶æµï¼›å¦‚æœæ˜¯éæµå¼çš„ï¼Œåˆ™è¿”å›ä¸€ä¸ªå®Œæ•´çš„ JSON å“åº”ã€‚

## è®¤è¯æœºåˆ¶

è‡³å…³é‡è¦çš„æ˜¯ï¼Œ`mcp-server` **ä¸ç®¡ç†è‡ªå·±çš„è®¤è¯å‡­æ®**ã€‚å®ƒä¸ä¸» `gemini-cli` å·¥å…·å…±äº«ç›¸åŒçš„è®¤è¯æœºåˆ¶ï¼Œä»¥ç¡®ä¿æ— ç¼å’Œå®‰å…¨çš„æ“ä½œã€‚

è®¤è¯å‡­æ®çš„æ¥æºéµå¾ªä¸ `gemini-cli` å®Œå…¨ç›¸åŒçš„ä¼˜å…ˆçº§å’ŒæŸ¥æ‰¾é€»è¾‘ï¼š

-   **ç¼“å­˜çš„å‡­æ®**: å¦‚æœæ‚¨ä¹‹å‰é€šè¿‡ `gemini-cli` çš„äº¤äº’å¼æµç¨‹ï¼ˆä¾‹å¦‚ `gcloud auth application-default login` æˆ– OAuth ç½‘é¡µç™»å½•ï¼‰ç™»å½•è¿‡ï¼Œ`mcp-server` ä¼šè‡ªåŠ¨ä½¿ç”¨å­˜å‚¨åœ¨ `~/.config/gcloud` æˆ– `~/.gemini` ç›®å½•ä¸‹çš„ç¼“å­˜å‡­æ®ã€‚
-   **ç¯å¢ƒå˜é‡**: æœåŠ¡å™¨ä¼šæŸ¥æ‰¾å¹¶ä½¿ç”¨æ ‡å‡†çš„ Google Cloud å’Œ Gemini ç¯å¢ƒå˜é‡ï¼Œä¾‹å¦‚ï¼š
    -   `GEMINI_API_KEY`
    -   `GOOGLE_APPLICATION_CREDENTIALS`
    -   `GOOGLE_CLOUD_PROJECT`

è¿™æ„å‘³ç€ï¼Œåªè¦æ‚¨çš„ `gemini-cli` æœ¬èº«é…ç½®æ­£ç¡®ä¸”å¯ä»¥å·¥ä½œï¼Œ`mcp-server` å°±èƒ½è‡ªåŠ¨è·å¾—æˆæƒï¼Œæ— éœ€ä»»ä½•é¢å¤–çš„è®¤è¯æ­¥éª¤ã€‚

## é…ç½®é€‰é¡¹

æ‚¨å¯ä»¥é€šè¿‡å‘½ä»¤è¡Œå‚æ•°å’Œç¯å¢ƒå˜é‡æ¥é…ç½®æœåŠ¡å™¨çš„è¡Œä¸ºã€‚

### å‘½ä»¤è¡Œå‚æ•°

-   `--port=<number>`: æŒ‡å®šæœåŠ¡å™¨ç›‘å¬çš„ç«¯å£ã€‚
    -   **é»˜è®¤å€¼**: `8765`
-   `--debug`: å¯ç”¨è¯¦ç»†çš„è°ƒè¯•æ—¥å¿—è¾“å‡ºã€‚

### ç¯å¢ƒå˜é‡

-   `GEMINI_TOOLS_DEFAULT_MODEL`: ä¸ºæœåŠ¡å™¨æ‰˜ç®¡çš„å·¥å…·ï¼ˆå¦‚ `google_web_search`ï¼‰è®¾ç½®ä¸€ä¸ªé»˜è®¤çš„ LLM æ¨¡å‹ã€‚
    -   **ç”¨é€”**: å½“ä¸€ä¸ªå·¥å…·åœ¨æ‰§è¡Œè¿‡ç¨‹ä¸­éœ€è¦è°ƒç”¨ LLMï¼ˆä¾‹å¦‚ï¼Œå¯¹æœç´¢ç»“æœè¿›è¡Œæ€»ç»“ï¼‰æ—¶ï¼Œå®ƒå°†ä½¿ç”¨æ­¤ç¯å¢ƒå˜é‡æŒ‡å®šçš„æ¨¡å‹ã€‚è¿™å…è®¸æ‚¨ä¸ºä¸»èŠå¤©å’Œå·¥å…·æ‰§è¡Œä½¿ç”¨ä¸åŒçš„æ¨¡å‹ï¼Œä»è€Œå¯èƒ½ä¼˜åŒ–æˆæœ¬å’Œé€Ÿåº¦ã€‚
    -   **ç¤ºä¾‹**: `GEMINI_TOOLS_DEFAULT_MODEL=gemini-1.5-flash`

## ä½¿ç”¨æ–¹æ³•

### 1. å®‰è£…ä¸æ„å»º

åœ¨ `gemini-cli` é¡¹ç›®çš„æ ¹ç›®å½•ä¸‹ï¼Œç¡®ä¿æ‰€æœ‰ä¾èµ–å·²å®‰è£…ï¼Œå¹¶æ„å»º `mcp-server` åŒ…ã€‚

```bash
# åœ¨é¡¹ç›®æ ¹ç›®å½•è¿è¡Œ
npm install
npm run build --workspace=@gemini-community/gemini-mcp-server
```

### 2. å¯åŠ¨æœåŠ¡å™¨

æ‚¨å¯ä»¥ä½¿ç”¨ `npm run start` å‘½ä»¤æ¥å¯åŠ¨æœåŠ¡å™¨ã€‚

```bash
# å¯åŠ¨æœåŠ¡å™¨ï¼Œä½¿ç”¨é»˜è®¤é…ç½®
npm run start --workspace=@gemini-community/gemini-mcp-server

# åœ¨ä¸åŒç«¯å£ä¸Šå¯åŠ¨ï¼Œå¹¶å¯ç”¨è°ƒè¯•æ¨¡å¼
npm run start --workspace=@gemini-community/gemini-mcp-server -- --port=9000 --debug

# ä½¿ç”¨ä¸€ä¸ªæ›´å¿«çš„æ¨¡å‹è¿›è¡Œå·¥å…·è°ƒç”¨
GEMINI_TOOLS_DEFAULT_MODEL=gemini-1.5-flash npm run start --workspace=@gemini-community/gemini-mcp-server
```

æœåŠ¡å™¨æˆåŠŸå¯åŠ¨åï¼Œæ‚¨å°†çœ‹åˆ°ç±»ä¼¼ä»¥ä¸‹çš„è¾“å‡ºï¼š

```
ğŸš€ Gemini CLI MCP Server and OpenAI Bridge are running on port 8765
   - MCP transport listening on http://localhost:8765/mcp
   - OpenAI-compatible endpoints available at http://localhost:8765/v1
âš™ï¸  Using default model for tools: gemini-2.5-pro
```

### 3. æµ‹è¯•ç«¯ç‚¹

æ‚¨å¯ä»¥ä½¿ç”¨ `curl` æˆ–ä»»ä½• API å®¢æˆ·ç«¯æ¥æµ‹è¯•æœåŠ¡å™¨ã€‚

**æµ‹è¯• OpenAI Chat Completions (æµå¼)**:

```bash
curl -N http://localhost:8765/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gemini-pro",
    "messages": [{"role": "user", "content": "Tell me a short story about a robot who learns to paint."}],
    "stream": true
  }'
```

## é¥æµ‹ã€æœåŠ¡æ¡æ¬¾å’Œéšç§

### é¥æµ‹ (Telemetry)

`@google/gemini-mcp-server` æœ¬èº«**ä¸å¼•å…¥ä»»ä½•æ–°çš„é¥æµ‹æˆ–æ•°æ®æ”¶é›†æœºåˆ¶**ã€‚

å®ƒå®Œå…¨ä¾èµ–äº `@google/gemini-cli-core` åŒ…ä¸­å†…ç½®çš„ OpenTelemetry (OTEL) ç³»ç»Ÿã€‚å› æ­¤ï¼Œæ‰€æœ‰çš„é¥æµ‹æ•°æ®ï¼ˆå¦‚æœå¯ç”¨ï¼‰éƒ½å°†éµå¾ª `gemini-cli` çš„ä¸»é…ç½®ï¼Œå¹¶è¢«å‘é€åˆ° `settings.json` æ–‡ä»¶ä¸­æŒ‡å®šçš„ç›®æ ‡ã€‚

å…³äºå¦‚ä½•é…ç½®å’Œä½¿ç”¨é¥æµ‹ï¼Œè¯·å‚é˜…[ä¸» Gemini CLI é¥æµ‹æ–‡æ¡£](../../docs/telemetry.md)ã€‚

### æœåŠ¡æ¡æ¬¾ (Terms of Service) å’Œéšç§å£°æ˜ (Privacy Notice)

æœ¬æœåŠ¡å™¨çš„ä½¿ç”¨å—åˆ¶äºæ‚¨ç”¨äºè®¤è¯çš„ `gemini-cli` è´¦æˆ·ç±»å‹æ‰€å¯¹åº”çš„æœåŠ¡æ¡æ¬¾å’Œéšç§æ”¿ç­–ã€‚`@google/gemini-mcp-server` ä½œä¸ºä¸€ä¸ªæ¡¥æ¥å·¥å…·ï¼Œæœ¬èº«ä¸æ”¶é›†ã€å­˜å‚¨æˆ–å¤„ç†æ‚¨çš„ä»»ä½•é¢å¤–æ•°æ®ã€‚

æˆ‘ä»¬å¼ºçƒˆå»ºè®®æ‚¨æŸ¥é˜…[ä¸» Gemini CLI æœåŠ¡æ¡æ¬¾å’Œéšç§å£°æ˜æ–‡æ¡£](../../docs/tos-privacy.md)ä»¥äº†è§£é€‚ç”¨äºæ‚¨è´¦æˆ·çš„è¯¦ç»†ä¿¡æ¯ã€‚

---

### å¼€å‘è€…è¯´æ˜ï¼šå…³äºåŒ…å `@google/gemini-mcp-server`

è¯·æ³¨æ„ï¼Œæœ¬åŒ…çš„åç§° `@google/gemini-mcp-server` åæ˜ äº†å®ƒä½œä¸ºå®˜æ–¹ `google-gemini/gemini-cli` çš„forké¡¹ç›®çš„ monorepo å†…éƒ¨ç»„ä»¶çš„æ¥æºã€‚

-   **å†…éƒ¨å‘½å**: åœ¨ `gemini-cli` é¡¹ç›®çš„æºä»£ç å’Œå·¥ä½œåŒºä¸­ï¼Œæ­¤å‘½åæ˜¯å†…éƒ¨ä¸€è‡´çš„ã€‚
-   **éç‹¬ç«‹å‘å¸ƒ**: æ­¤åŒ…**ä¸ä¼š**ä½œä¸ºä¸€ä¸ªç‹¬ç«‹çš„ã€ç‰ˆæœ¬åŒ–çš„åŒ…å‘å¸ƒåˆ°å…¬å…± npm registry ä¸Šã€‚å¦‚æœæ‚¨ fork æœ¬é¡¹ç›®å¹¶å¸Œæœ›ç‹¬ç«‹å‘å¸ƒæ‚¨çš„ä¿®æ”¹ç‰ˆæœ¬ï¼Œæ‚¨**å¿…é¡»**å°†åŒ…åæ›´æ”¹ä¸ºæ‚¨è‡ªå·±çš„ scopeï¼ˆä¾‹å¦‚ `@your-username/gemini-mcp-server`ï¼‰ï¼Œä»¥éµå®ˆ npm çš„åŒ…å‘½åè§„èŒƒå¹¶é¿å…æ··æ·†ã€‚
