diff --git a/.gitignore b/.gitignore
index 8afd3293..819a0556 100644
--- a/.gitignore
+++ b/.gitignore
@@ -36,3 +36,4 @@ packages/*/coverage/
 # Generated files
 packages/cli/src/generated/
 .integration-tests/
+.aider*
diff --git a/.vscode/mcp.json b/.vscode/mcp.json
new file mode 100644
index 00000000..1e7506a8
--- /dev/null
+++ b/.vscode/mcp.json
@@ -0,0 +1,7 @@
+{
+    "servers": {
+        "gemini-cli": {
+            "url": "http://localhost:8765/mcp"
+        }
+    }
+}
\ No newline at end of file
diff --git a/README-GEMINI-MCP.md b/README-GEMINI-MCP.md
new file mode 100644
index 00000000..1f115660
--- /dev/null
+++ b/README-GEMINI-MCP.md
@@ -0,0 +1,308 @@
+# Gemini CLI - MCP / OpenAI Bridge Server (`@google/gemini-mcp-server`)
+
+`@google/gemini-mcp-server` is a versatile companion application designed to serve as a powerful extension for the `gemini-cli` ecosystem. It primarily fulfills two core roles:
+
+1.  **MCP (Model-Context Protocol) Server**: It hosts and exposes `gemini-cli`'s powerful built-in tools (e.g., `google_web_search`, file system tools) via a standard, discoverable protocol. This allows the core `gemini-cli` model to invoke these tools as needed.
+
+2.  **OpenAI-Compatible API Bridge**: It provides an endpoint compatible with the OpenAI Chat Completions API. This enables any third-party tool or application that supports the OpenAI API (such as [Open WebUI](https://github.com/open-webui/open-webui)) to seamlessly interact with the underlying Gemini model of `gemini-cli`, including full support for streaming responses.
+
+## Core Design Philosophy
+
+The server is built on a principle of **minimal modification and maximum reuse**. It is not a reimplementation of `gemini-cli`'s features but is instead intelligently built on top of the `@google/gemini-cli-core` package.
+
+By reusing the `Config` and `GeminiClient` classes from the core package, the `mcp-server` inherits all of the essential business logic, tool execution capabilities, and configuration management of the main CLI. This design ensures behavioral consistency and simplifies maintenance and extension.
+
+## Features
+
+-   **Hosts Native `gemini-cli` Tools**: Exposes the built-in tools (file system operations, web fetching, web search, etc.) to the `gemini-cli` model via the MCP protocol.
+-   **OpenAI API Compatibility**: Provides `/v1/chat/completions` and `/v1/models` endpoints, allowing third-party applications to interact with the Gemini model as if it were an OpenAI service.
+-   **Streaming Support**: Fully supports streaming responses, pushing real-time generation results from the Gemini model to the client via Server-Sent Events (SSE).
+-   **Flexible Model Configuration**: Allows a separate, default LLM model to be configured via an environment variable specifically for tools hosted by the server (e.g., for summarizing search results).
+-   **Inherited Configuration & Authentication**: Automatically uses the same settings and authentication state as your main `gemini-cli` setup.
+-   **Forced YOLO Mode**: Runs in a permanent "YOLO" mode, automatically approving all tool calls for streamlined, non-interactive use by other applications.
+
+## Architecture & Interaction Flow
+
+The `mcp-server` operates as a standalone component within the `gemini-cli` ecosystem with the following interaction flow:
+
+1.  **Configuration Loading**: On startup, the server loads user and workspace `settings.json` files and reads environment variables, just like the main `gemini-cli` application, to initialize an instance of the `@google/gemini-cli-core` `Config` class.
+2.  **Authentication**: The server **does not** handle its own authentication. It relies entirely on the established authentication state of `gemini-cli` (see the next section for details).
+3.  **MCP Service**: It starts an MCP server, which the `gemini-cli` can connect to when it needs to discover and execute tools.
+4.  **OpenAI Bridge**: It starts an Express web server that listens for API requests in the OpenAI format.
+5.  **Request Handling**:
+    -   When an OpenAI-formatted request is received, the server converts it into a format that `gemini-cli-core` can understand.
+    -   It uses the reused `Config` instance to get a `GeminiClient`.
+    -   A **new, isolated `GeminiChat` session** is created for each incoming API request to prevent conversation history from leaking between different clients.
+    -   The request is sent to the Gemini API via the `GeminiClient`.
+    -   If the Gemini API's response is streaming, the server transforms it into an OpenAI-compatible SSE stream; otherwise, it returns a complete JSON response.
+
+## Authentication Mechanism
+
+Crucially, the `mcp-server` **does not manage its own credentials**. It shares the exact same authentication mechanism as the main `gemini-cli` tool to ensure seamless and secure operation.
+
+The source of authentication credentials follows the identical priority and lookup logic as `gemini-cli`:
+
+-   **Cached Credentials**: If you have previously logged in through the interactive `gemini-cli` flow (e.g., `gcloud auth application-default login` or OAuth web login), the `mcp-server` will automatically use the cached credentials stored in `~/.config/gcloud` or `~/.gemini`.
+-   **Environment Variables**: The server will look for and use standard Google Cloud and Gemini environment variables, such as:
+    -   `GEMINI_API_KEY`
+    -   `GOOGLE_APPLICATION_CREDENTIALS`
+    -   `GOOGLE_CLOUD_PROJECT`
+
+This means that as long as your `gemini-cli` itself is configured correctly and works, the `mcp-server` will be authorized automatically, with no extra authentication steps required.
+
+## Important Security Note: YOLO Mode
+
+The `mcp-server` is designed for non-interactive, programmatic use. As such, it runs with a permanent **YOLO (You Only Live Once) Mode** enabled (`approvalMode: ApprovalMode.YOLO`).
+
+This means that any tool call requested by the model (e.g., `run_shell_command`, `replace`) will be **executed immediately without user confirmation**.
+
+**Warning:** Be extremely cautious when exposing this server to a network. Any client that can reach the server will be able to execute tools with the same permissions as the user running the server process.
+
+## Configuration Options
+
+You can configure the server's behavior via command-line arguments and environment variables.
+
+### Command-Line Arguments
+
+-   `--port=<number>`: Specifies the port for the server to listen on.
+    -   **Default**: `8765`
+-   `--debug`: Enables detailed debug logging to the console.
+
+### Environment Variables
+
+-   `GEMINI_TOOLS_DEFAULT_MODEL`: Sets a default LLM model specifically for tools hosted by the server (like `google_web_search`).
+    -   **Purpose**: When a tool needs to invoke an LLM during its execution (e.g., to summarize search results), it will use the model specified by this variable. This allows you to use a different (potentially faster or cheaper) model for tool execution than for the main chat.
+    -   **Example**: `GEMINI_TOOLS_DEFAULT_MODEL=gemini-1.5-flash`
+
+## Usage
+
+### 1. Installation & Build
+
+From the root of the `gemini-cli` project, ensure all dependencies are installed and then build the `mcp-server` package.
+
+```bash
+# From the project root
+npm install
+npm run build --workspace=@google/gemini-mcp-server
+```
+
+### 2. Starting the Server
+
+You can start the server using the `npm run start` command from the root directory, targeting the workspace.
+
+```bash
+# Start the server with default configuration
+npm run start --workspace=@google/gemini-mcp-server
+
+# Start on a different port with debug mode enabled
+npm run start --workspace=@google/gemini-mcp-server -- --port=9000 --debug
+
+# Use a faster model for tool calls
+GEMINI_TOOLS_DEFAULT_MODEL=gemini-1.5-flash npm run start --workspace=@google/gemini-mcp-server
+```
+
+When the server starts successfully, you will see output similar to this:
+
+```
+ğŸš€ Gemini CLI MCP Server and OpenAI Bridge are running on port 8765
+   - MCP transport listening on http://localhost:8765/mcp
+   - OpenAI-compatible endpoints available at http://localhost:8765/v1
+âš™ï¸  Using default model for tools: gemini-2.5-pro
+```
+
+### 3. Testing the Endpoints
+
+You can use `curl` or any API client to test the server.
+
+**Test OpenAI Chat Completions (Streaming)**:
+
+```bash
+curl -N http://localhost:8765/v1/chat/completions \
+  -H "Content-Type: application/json" \
+  -d '{
+    "model": "gemini-pro",
+    "messages": [{"role": "user", "content": "Tell me a short story about a robot who learns to paint."}],
+    "stream": true
+  }'
+```
+
+**Test OpenAI Chat Completions (Non-Streaming)**:
+
+```bash
+curl http://localhost:8765/v1/chat/completions \
+  -H "Content-Type: application/json" \
+  -d '{
+    "model": "gemini-pro",
+    "messages": [{"role": "user", "content": "Why is the sky blue?"}],
+    "stream": false
+  }'
+```
+
+## Telemetry, Terms of Service, and Privacy
+
+### Telemetry
+
+The `@google/gemini-mcp-server` **does not introduce any new telemetry or data collection mechanisms**.
+
+It relies entirely on the OpenTelemetry (OTEL) system built into the `@google/gemini-cli-core` package. Therefore, all telemetry data (if enabled) will follow the main `gemini-cli` configuration and be sent to the destination specified in your `settings.json` file.
+
+For details on how to configure and use telemetry, please refer to the [main Gemini CLI telemetry documentation](../../docs/telemetry.md).
+
+### Terms of Service and Privacy Notice
+
+Your use of this server is governed by the Terms of Service and Privacy Policies corresponding to the `gemini-cli` account type you are using for authentication. As a bridge, `@google/gemini-mcp-server` does not collect, store, or process any additional data of its own.
+
+We strongly recommend you review the [main Gemini CLI Terms of Service and Privacy Notice documentation](../../docs/tos-privacy.md) for details applicable to your account.
+
+---
+
+### Developer Note: Regarding the package name `@google/gemini-mcp-server`
+
+Please note that the name of this package, `@google/gemini-mcp-server`, reflects its origin as an internal component of the official `google-gemini/gemini-cli` monorepo.
+
+-   **Internal Naming**: This naming is consistent internally within the source code and workspaces of the `gemini-cli` project.
+-   **Not for Independent Publication**: This package is **not intended to be published independently** to a public npm registry. If you fork this project and wish to publish your modified version, you **must** change the package name to your own scope (e.g., `@your-username/gemini-mcp-server`) to comply with npm's package naming policies and to avoid confusion.
+
+----
+
+# Gemini CLI - MCP / OpenAI Bridge Server (`@google/gemini-mcp-server`)
+
+`@google/gemini-mcp-server` æ˜¯ä¸€ä¸ªå¤šåŠŸèƒ½çš„æœåŠ¡å™¨åº”ç”¨ç¨‹åºï¼Œæ—¨åœ¨ä½œä¸º `gemini-cli` ç”Ÿæ€ç³»ç»Ÿçš„å¼ºå¤§æ‰©å±•ã€‚å®ƒä¸»è¦æ‰¿æ‹…ä¸¤ä¸ªæ ¸å¿ƒè§’è‰²ï¼š
+
+1.  **MCP (Model-Context Protocol) æœåŠ¡å™¨**: å®ƒä¸º `gemini-cli` æ‰˜ç®¡å’Œæš´éœ²äº†ä¸€ç³»åˆ—å¼ºå¤§çš„å†…ç½®å·¥å…·ï¼ˆä¾‹å¦‚ `google_web_search`ï¼‰ï¼Œå…è®¸ `gemini-cli` çš„æ ¸å¿ƒæ¨¡å‹é€šè¿‡ä¸€ä¸ªæ ‡å‡†çš„ã€å¯å‘ç°çš„åè®®æ¥è°ƒç”¨è¿™äº›å·¥å…·ã€‚
+
+2.  **OpenAI å…¼å®¹çš„ API æ¡¥æ¥å™¨**: å®ƒæä¾›äº†ä¸€ä¸ªä¸ OpenAI Chat Completions API å…¼å®¹çš„ç«¯ç‚¹ã€‚è¿™ä½¿å¾—ä»»ä½•æ”¯æŒ OpenAI API çš„ç¬¬ä¸‰æ–¹å·¥å…·æˆ–åº”ç”¨ç¨‹åºï¼ˆä¾‹å¦‚ [Open WebUI](https://github.com/open-webui/open-webui)ï¼‰éƒ½å¯ä»¥æ— ç¼åœ°ä¸ `gemini-cli` çš„åº•å±‚ Gemini æ¨¡å‹è¿›è¡Œäº¤äº’ï¼ŒåŒ…æ‹¬åˆ©ç”¨æµå¼å“åº”ã€‚
+
+## æ ¸å¿ƒè®¾è®¡ç†å¿µ
+
+è¿™ä¸ªæœåŠ¡å™¨çš„æ ¸å¿ƒè®¾è®¡åŸåˆ™æ˜¯ **æœ€å°åŒ–ä¿®æ”¹å’Œæœ€å¤§åŒ–å¤ç”¨**ã€‚å®ƒå¹¶ä¸æ˜¯å¯¹ `gemini-cli` åŠŸèƒ½çš„é‡æ–°å®ç°ï¼Œè€Œæ˜¯å·§å¦™åœ°æ„å»ºåœ¨ `@google/gemini-cli-core` åŒ…ä¹‹ä¸Šã€‚
+
+é€šè¿‡é‡ç”¨ `core` åŒ…ä¸­çš„ `Config` å’Œ `GeminiClient` ç±»ï¼Œ`mcp-server` ç»§æ‰¿äº† `gemini-cli` æ‰€æœ‰çš„æ ¸å¿ƒä¸šåŠ¡é€»è¾‘ã€å·¥å…·æ‰§è¡Œèƒ½åŠ›å’Œé…ç½®ç®¡ç†æœºåˆ¶ã€‚è¿™ç§è®¾è®¡ç¡®ä¿äº†è¡Œä¸ºçš„ä¸€è‡´æ€§ï¼Œå¹¶ä½¿å¾—ç»´æŠ¤å’Œæ‰©å±•å˜å¾—æ›´åŠ ç®€å•ã€‚
+
+## åŠŸèƒ½ç‰¹æ€§
+
+-   **æ‰˜ç®¡åŸç”Ÿ `gemini-cli` å·¥å…·**: é€šè¿‡ MCP åè®®ï¼Œå°† `gemini-cli` çš„å†…ç½®å·¥å…·ï¼ˆå¦‚æ–‡ä»¶ç³»ç»Ÿæ“ä½œã€ç½‘é¡µæŠ“å–ã€ç½‘ç»œæœç´¢ç­‰ï¼‰æš´éœ²ç»™ `gemini-cli` æ¨¡å‹ã€‚
+-   **OpenAI API å…¼å®¹æ€§**: æä¾› `/v1/chat/completions` å’Œ `/v1/models` ç«¯ç‚¹ï¼Œå…è®¸ç¬¬ä¸‰æ–¹åº”ç”¨ç¨‹åºåƒä¸ OpenAI å¯¹è¯ä¸€æ ·ä¸ Gemini æ¨¡å‹äº¤äº’ã€‚
+-   **æµå¼å“åº”æ”¯æŒ**: å®Œå…¨æ”¯æŒæµå¼å“åº”ï¼Œå¯ä»¥å°† Gemini æ¨¡å‹çš„å®æ—¶ç”Ÿæˆç»“æœé€šè¿‡ SSE (Server-Sent Events) æ¨é€ç»™å®¢æˆ·ç«¯ã€‚
+-   **çµæ´»çš„æ¨¡å‹é…ç½®**: å…è®¸é€šè¿‡ç¯å¢ƒå˜é‡ä¸ºæœåŠ¡å™¨æ‰˜ç®¡çš„å·¥å…·ï¼ˆå¦‚ `google_web_search`ï¼‰é…ç½®ä¸€ä¸ªç‰¹å®šçš„ã€ç‹¬ç«‹çš„é»˜è®¤ LLM æ¨¡å‹ã€‚
+
+## æ¶æ„ä¸äº¤äº’æµç¨‹
+
+`mcp-server` ä½œä¸º `gemini-cli` ç”Ÿæ€ç³»ç»Ÿä¸­çš„ä¸€ä¸ªç‹¬ç«‹ç»„ä»¶ï¼Œå…¶äº¤äº’æµç¨‹å¦‚ä¸‹ï¼š
+
+1.  **é…ç½®åŠ è½½**: æœåŠ¡å™¨å¯åŠ¨æ—¶ï¼Œå®ƒä¼šåƒä¸» `gemini-cli` åº”ç”¨ä¸€æ ·ï¼ŒåŠ è½½ç”¨æˆ·å’Œå·¥ä½œåŒºçš„ `settings.json` æ–‡ä»¶ï¼Œå¹¶è¯»å–ç¯å¢ƒå˜é‡æ¥åˆå§‹åŒ–ä¸€ä¸ª `@google/gemini-cli-core` çš„ `Config` å®ä¾‹ã€‚
+2.  **è®¤è¯**: æœåŠ¡å™¨**ä¸å¤„ç†**è‡ªå·±çš„è®¤è¯æµç¨‹ã€‚å®ƒå®Œå…¨ä¾èµ–äº `gemini-cli` å·²ç»å»ºç«‹çš„è®¤è¯çŠ¶æ€ï¼ˆè¯¦æƒ…è§ä¸‹ä¸€èŠ‚ï¼‰ã€‚
+3.  **MCP æœåŠ¡**: å®ƒå¯åŠ¨ä¸€ä¸ª MCP æœåŠ¡å™¨ï¼Œ`gemini-cli` åœ¨éœ€è¦æ—¶å¯ä»¥è¿æ¥åˆ°è¿™ä¸ªæœåŠ¡å™¨æ¥å‘ç°å’Œæ‰§è¡Œå·¥å…·ã€‚
+4.  **OpenAI æ¡¥æ¥**: å®ƒå¯åŠ¨ä¸€ä¸ª Express Web æœåŠ¡å™¨ï¼Œç›‘å¬ OpenAI æ ¼å¼çš„ API è¯·æ±‚ã€‚
+5.  **è¯·æ±‚å¤„ç†**:
+    -   å½“æ”¶åˆ°ä¸€ä¸ª OpenAI æ ¼å¼çš„è¯·æ±‚æ—¶ï¼ŒæœåŠ¡å™¨ä¼šå°†å…¶è½¬æ¢ä¸º `gemini-cli-core` å¯ä»¥ç†è§£çš„æ ¼å¼ã€‚
+    -   å®ƒä½¿ç”¨å¤ç”¨çš„ `Config` å®ä¾‹æ¥è·å–ä¸€ä¸ª `GeminiClient`ã€‚
+    -   é€šè¿‡ `GeminiClient` å°†è¯·æ±‚å‘é€ç»™ Gemini APIã€‚
+    -   å¦‚æœ Gemini API çš„å“åº”æ˜¯æµå¼çš„ï¼ŒæœåŠ¡å™¨ä¼šå°†å…¶è½¬æ¢ä¸º OpenAI å…¼å®¹çš„ SSE äº‹ä»¶æµï¼›å¦‚æœæ˜¯éæµå¼çš„ï¼Œåˆ™è¿”å›ä¸€ä¸ªå®Œæ•´çš„ JSON å“åº”ã€‚
+
+## è®¤è¯æœºåˆ¶
+
+è‡³å…³é‡è¦çš„æ˜¯ï¼Œ`mcp-server` **ä¸ç®¡ç†è‡ªå·±çš„è®¤è¯å‡­æ®**ã€‚å®ƒä¸ä¸» `gemini-cli` å·¥å…·å…±äº«ç›¸åŒçš„è®¤è¯æœºåˆ¶ï¼Œä»¥ç¡®ä¿æ— ç¼å’Œå®‰å…¨çš„æ“ä½œã€‚
+
+è®¤è¯å‡­æ®çš„æ¥æºéµå¾ªä¸ `gemini-cli` å®Œå…¨ç›¸åŒçš„ä¼˜å…ˆçº§å’ŒæŸ¥æ‰¾é€»è¾‘ï¼š
+
+-   **ç¼“å­˜çš„å‡­æ®**: å¦‚æœæ‚¨ä¹‹å‰é€šè¿‡ `gemini-cli` çš„äº¤äº’å¼æµç¨‹ï¼ˆä¾‹å¦‚ `gcloud auth application-default login` æˆ– OAuth ç½‘é¡µç™»å½•ï¼‰ç™»å½•è¿‡ï¼Œ`mcp-server` ä¼šè‡ªåŠ¨ä½¿ç”¨å­˜å‚¨åœ¨ `~/.config/gcloud` æˆ– `~/.gemini` ç›®å½•ä¸‹çš„ç¼“å­˜å‡­æ®ã€‚
+-   **ç¯å¢ƒå˜é‡**: æœåŠ¡å™¨ä¼šæŸ¥æ‰¾å¹¶ä½¿ç”¨æ ‡å‡†çš„ Google Cloud å’Œ Gemini ç¯å¢ƒå˜é‡ï¼Œä¾‹å¦‚ï¼š
+    -   `GEMINI_API_KEY`
+    -   `GOOGLE_APPLICATION_CREDENTIALS`
+    -   `GOOGLE_CLOUD_PROJECT`
+
+è¿™æ„å‘³ç€ï¼Œåªè¦æ‚¨çš„ `gemini-cli` æœ¬èº«é…ç½®æ­£ç¡®ä¸”å¯ä»¥å·¥ä½œï¼Œ`mcp-server` å°±èƒ½è‡ªåŠ¨è·å¾—æˆæƒï¼Œæ— éœ€ä»»ä½•é¢å¤–çš„è®¤è¯æ­¥éª¤ã€‚
+
+## é…ç½®é€‰é¡¹
+
+æ‚¨å¯ä»¥é€šè¿‡å‘½ä»¤è¡Œå‚æ•°å’Œç¯å¢ƒå˜é‡æ¥é…ç½®æœåŠ¡å™¨çš„è¡Œä¸ºã€‚
+
+### å‘½ä»¤è¡Œå‚æ•°
+
+-   `--port=<number>`: æŒ‡å®šæœåŠ¡å™¨ç›‘å¬çš„ç«¯å£ã€‚
+    -   **é»˜è®¤å€¼**: `8765`
+-   `--debug`: å¯ç”¨è¯¦ç»†çš„è°ƒè¯•æ—¥å¿—è¾“å‡ºã€‚
+
+### ç¯å¢ƒå˜é‡
+
+-   `GEMINI_TOOLS_DEFAULT_MODEL`: ä¸ºæœåŠ¡å™¨æ‰˜ç®¡çš„å·¥å…·ï¼ˆå¦‚ `google_web_search`ï¼‰è®¾ç½®ä¸€ä¸ªé»˜è®¤çš„ LLM æ¨¡å‹ã€‚
+    -   **ç”¨é€”**: å½“ä¸€ä¸ªå·¥å…·åœ¨æ‰§è¡Œè¿‡ç¨‹ä¸­éœ€è¦è°ƒç”¨ LLMï¼ˆä¾‹å¦‚ï¼Œå¯¹æœç´¢ç»“æœè¿›è¡Œæ€»ç»“ï¼‰æ—¶ï¼Œå®ƒå°†ä½¿ç”¨æ­¤ç¯å¢ƒå˜é‡æŒ‡å®šçš„æ¨¡å‹ã€‚è¿™å…è®¸æ‚¨ä¸ºä¸»èŠå¤©å’Œå·¥å…·æ‰§è¡Œä½¿ç”¨ä¸åŒçš„æ¨¡å‹ï¼Œä»è€Œå¯èƒ½ä¼˜åŒ–æˆæœ¬å’Œé€Ÿåº¦ã€‚
+    -   **ç¤ºä¾‹**: `GEMINI_TOOLS_DEFAULT_MODEL=gemini-1.5-flash`
+
+## ä½¿ç”¨æ–¹æ³•
+
+### 1. å®‰è£…ä¸æ„å»º
+
+åœ¨ `gemini-cli` é¡¹ç›®çš„æ ¹ç›®å½•ä¸‹ï¼Œç¡®ä¿æ‰€æœ‰ä¾èµ–å·²å®‰è£…ï¼Œå¹¶æ„å»º `mcp-server` åŒ…ã€‚
+
+```bash
+# åœ¨é¡¹ç›®æ ¹ç›®å½•è¿è¡Œ
+npm install
+npm run build --workspace=@google/gemini-mcp-server
+```
+
+### 2. å¯åŠ¨æœåŠ¡å™¨
+
+æ‚¨å¯ä»¥ä½¿ç”¨ `npm run start` å‘½ä»¤æ¥å¯åŠ¨æœåŠ¡å™¨ã€‚
+
+```bash
+# å¯åŠ¨æœåŠ¡å™¨ï¼Œä½¿ç”¨é»˜è®¤é…ç½®
+npm run start --workspace=@google/gemini-mcp-server
+
+# åœ¨ä¸åŒç«¯å£ä¸Šå¯åŠ¨ï¼Œå¹¶å¯ç”¨è°ƒè¯•æ¨¡å¼
+npm run start --workspace=@google/gemini-mcp-server -- --port=9000 --debug
+
+# ä½¿ç”¨ä¸€ä¸ªæ›´å¿«çš„æ¨¡å‹è¿›è¡Œå·¥å…·è°ƒç”¨
+GEMINI_TOOLS_DEFAULT_MODEL=gemini-1.5-flash npm run start --workspace=@google/gemini-mcp-server
+```
+
+æœåŠ¡å™¨æˆåŠŸå¯åŠ¨åï¼Œæ‚¨å°†çœ‹åˆ°ç±»ä¼¼ä»¥ä¸‹çš„è¾“å‡ºï¼š
+
+```
+ğŸš€ Gemini CLI MCP Server and OpenAI Bridge are running on port 8765
+   - MCP transport listening on http://localhost:8765/mcp
+   - OpenAI-compatible endpoints available at http://localhost:8765/v1
+âš™ï¸  Using default model for tools: gemini-2.5-pro
+```
+
+### 3. æµ‹è¯•ç«¯ç‚¹
+
+æ‚¨å¯ä»¥ä½¿ç”¨ `curl` æˆ–ä»»ä½• API å®¢æˆ·ç«¯æ¥æµ‹è¯•æœåŠ¡å™¨ã€‚
+
+**æµ‹è¯• OpenAI Chat Completions (æµå¼)**:
+
+```bash
+curl -N http://localhost:8765/v1/chat/completions \
+  -H "Content-Type: application/json" \
+  -d '{
+    "model": "gemini-pro",
+    "messages": [{"role": "user", "content": "Tell me a short story about a robot who learns to paint."}],
+    "stream": true
+  }'
+```
+
+## é¥æµ‹ã€æœåŠ¡æ¡æ¬¾å’Œéšç§
+
+### é¥æµ‹ (Telemetry)
+
+`@google/gemini-mcp-server` æœ¬èº«**ä¸å¼•å…¥ä»»ä½•æ–°çš„é¥æµ‹æˆ–æ•°æ®æ”¶é›†æœºåˆ¶**ã€‚
+
+å®ƒå®Œå…¨ä¾èµ–äº `@google/gemini-cli-core` åŒ…ä¸­å†…ç½®çš„ OpenTelemetry (OTEL) ç³»ç»Ÿã€‚å› æ­¤ï¼Œæ‰€æœ‰çš„é¥æµ‹æ•°æ®ï¼ˆå¦‚æœå¯ç”¨ï¼‰éƒ½å°†éµå¾ª `gemini-cli` çš„ä¸»é…ç½®ï¼Œå¹¶è¢«å‘é€åˆ° `settings.json` æ–‡ä»¶ä¸­æŒ‡å®šçš„ç›®æ ‡ã€‚
+
+å…³äºå¦‚ä½•é…ç½®å’Œä½¿ç”¨é¥æµ‹ï¼Œè¯·å‚é˜…[ä¸» Gemini CLI é¥æµ‹æ–‡æ¡£](../../docs/telemetry.md)ã€‚
+
+### æœåŠ¡æ¡æ¬¾ (Terms of Service) å’Œéšç§å£°æ˜ (Privacy Notice)
+
+æœ¬æœåŠ¡å™¨çš„ä½¿ç”¨å—åˆ¶äºæ‚¨ç”¨äºè®¤è¯çš„ `gemini-cli` è´¦æˆ·ç±»å‹æ‰€å¯¹åº”çš„æœåŠ¡æ¡æ¬¾å’Œéšç§æ”¿ç­–ã€‚`@google/gemini-mcp-server` ä½œä¸ºä¸€ä¸ªæ¡¥æ¥å·¥å…·ï¼Œæœ¬èº«ä¸æ”¶é›†ã€å­˜å‚¨æˆ–å¤„ç†æ‚¨çš„ä»»ä½•é¢å¤–æ•°æ®ã€‚
+
+æˆ‘ä»¬å¼ºçƒˆå»ºè®®æ‚¨æŸ¥é˜…[ä¸» Gemini CLI æœåŠ¡æ¡æ¬¾å’Œéšç§å£°æ˜æ–‡æ¡£](../../docs/tos-privacy.md)ä»¥äº†è§£é€‚ç”¨äºæ‚¨è´¦æˆ·çš„è¯¦ç»†ä¿¡æ¯ã€‚
+
+---
+
+### å¼€å‘è€…è¯´æ˜ï¼šå…³äºåŒ…å `@google/gemini-mcp-server`
+
+è¯·æ³¨æ„ï¼Œæœ¬åŒ…çš„åç§° `@google/gemini-mcp-server` åæ˜ äº†å®ƒä½œä¸ºå®˜æ–¹ `google-gemini/gemini-cli` çš„forké¡¹ç›®çš„ monorepo å†…éƒ¨ç»„ä»¶çš„æ¥æºã€‚
+
+-   **å†…éƒ¨å‘½å**: åœ¨ `gemini-cli` é¡¹ç›®çš„æºä»£ç å’Œå·¥ä½œåŒºä¸­ï¼Œæ­¤å‘½åæ˜¯å†…éƒ¨ä¸€è‡´çš„ã€‚
+-   **éç‹¬ç«‹å‘å¸ƒ**: æ­¤åŒ…**ä¸ä¼š**ä½œä¸ºä¸€ä¸ªç‹¬ç«‹çš„ã€ç‰ˆæœ¬åŒ–çš„åŒ…å‘å¸ƒåˆ°å…¬å…± npm registry ä¸Šã€‚å¦‚æœæ‚¨ fork æœ¬é¡¹ç›®å¹¶å¸Œæœ›ç‹¬ç«‹å‘å¸ƒæ‚¨çš„ä¿®æ”¹ç‰ˆæœ¬ï¼Œæ‚¨**å¿…é¡»**å°†åŒ…åæ›´æ”¹ä¸ºæ‚¨è‡ªå·±çš„ scopeï¼ˆä¾‹å¦‚ `@your-username/gemini-mcp-server`ï¼‰ï¼Œä»¥éµå®ˆ npm çš„åŒ…å‘½åè§„èŒƒå¹¶é¿å…æ··æ·†ã€‚
diff --git a/package-lock.json b/package-lock.json
index 09eb6d96..16a307cc 100644
--- a/package-lock.json
+++ b/package-lock.json
@@ -910,6 +910,10 @@
       "resolved": "packages/core",
       "link": true
     },
+    "node_modules/@google/gemini-mcp-server": {
+      "resolved": "packages/mcp-server",
+      "link": true
+    },
     "node_modules/@google/genai": {
       "version": "1.6.0",
       "resolved": "https://registry.npmjs.org/@google/genai/-/genai-1.6.0.tgz",
@@ -1227,9 +1231,9 @@
       "license": "MIT"
     },
     "node_modules/@modelcontextprotocol/sdk": {
-      "version": "1.13.1",
-      "resolved": "https://registry.npmjs.org/@modelcontextprotocol/sdk/-/sdk-1.13.1.tgz",
-      "integrity": "sha512-8q6+9aF0yA39/qWT/uaIj6zTpC+Qu07DnN/lb9mjoquCJsAh6l3HyYqc9O3t2j7GilseOQOQimLg7W3By6jqvg==",
+      "version": "1.13.2",
+      "resolved": "https://registry.npmjs.org/@modelcontextprotocol/sdk/-/sdk-1.13.2.tgz",
+      "integrity": "sha512-Vx7qOcmoKkR3qhaQ9qf3GxiVKCEu+zfJddHv6x3dY/9P6+uIwJnmuAur5aB+4FDXf41rRrDnOEGkviX5oYZ67w==",
       "license": "MIT",
       "dependencies": {
         "ajv": "^6.12.6",
@@ -2202,6 +2206,17 @@
       "license": "MIT",
       "peer": true
     },
+    "node_modules/@types/body-parser": {
+      "version": "1.19.6",
+      "resolved": "https://registry.npmjs.org/@types/body-parser/-/body-parser-1.19.6.tgz",
+      "integrity": "sha512-HLFeCYgz89uk22N5Qg3dvGvsv46B8GLvKKo1zKG4NybA8U2DiEO3w9lqGg29t/tfLRJpJ6iQxnVw4OnB7MoM9g==",
+      "dev": true,
+      "license": "MIT",
+      "dependencies": {
+        "@types/connect": "*",
+        "@types/node": "*"
+      }
+    },
     "node_modules/@types/braces": {
       "version": "3.0.5",
       "resolved": "https://registry.npmjs.org/@types/braces/-/braces-3.0.5.tgz",
@@ -2232,6 +2247,16 @@
       "integrity": "sha512-OS//b51j9uyR3zvwD04Kfs5kHpve2qalQ18JhY/ho3voGYUTPLEG90/ocfKPI48hyHH8T04f7KEEbK6Ue60oZQ==",
       "license": "MIT"
     },
+    "node_modules/@types/connect": {
+      "version": "3.4.38",
+      "resolved": "https://registry.npmjs.org/@types/connect/-/connect-3.4.38.tgz",
+      "integrity": "sha512-K6uROf1LD88uDQqJCktA4yzL1YYAK6NgfsI0v/mTgyPKWsX1CnJ0XPSDhViejru1GcRkLWb8RlzFYJRqGUbaug==",
+      "dev": true,
+      "license": "MIT",
+      "dependencies": {
+        "@types/node": "*"
+      }
+    },
     "node_modules/@types/deep-eql": {
       "version": "4.0.2",
       "resolved": "https://registry.npmjs.org/@types/deep-eql/-/deep-eql-4.0.2.tgz",
@@ -2263,6 +2288,31 @@
       "dev": true,
       "license": "MIT"
     },
+    "node_modules/@types/express": {
+      "version": "5.0.3",
+      "resolved": "https://registry.npmjs.org/@types/express/-/express-5.0.3.tgz",
+      "integrity": "sha512-wGA0NX93b19/dZC1J18tKWVIYWyyF2ZjT9vin/NRu0qzzvfVzWjs04iq2rQ3H65vCTQYlRqs3YHfY7zjdV+9Kw==",
+      "dev": true,
+      "license": "MIT",
+      "dependencies": {
+        "@types/body-parser": "*",
+        "@types/express-serve-static-core": "^5.0.0",
+        "@types/serve-static": "*"
+      }
+    },
+    "node_modules/@types/express-serve-static-core": {
+      "version": "5.0.6",
+      "resolved": "https://registry.npmjs.org/@types/express-serve-static-core/-/express-serve-static-core-5.0.6.tgz",
+      "integrity": "sha512-3xhRnjJPkULekpSzgtoNYYcTWgEZkp4myc+Saevii5JPnHNvHMRlBSHDbs7Bh1iPPoVTERHEZXyhyLbMEsExsA==",
+      "dev": true,
+      "license": "MIT",
+      "dependencies": {
+        "@types/node": "*",
+        "@types/qs": "*",
+        "@types/range-parser": "*",
+        "@types/send": "*"
+      }
+    },
     "node_modules/@types/glob": {
       "version": "8.1.0",
       "resolved": "https://registry.npmjs.org/@types/glob/-/glob-8.1.0.tgz",
@@ -2297,6 +2347,13 @@
       "integrity": "sha512-pUY3cKH/Nm2yYrEmDlPR1mR7yszjGx4DrwPjQ702C4/D5CwHuZTgZdIdwPkRbcuhs7BAh2L5rg3CL5cbRiGTCQ==",
       "license": "MIT"
     },
+    "node_modules/@types/http-errors": {
+      "version": "2.0.5",
+      "resolved": "https://registry.npmjs.org/@types/http-errors/-/http-errors-2.0.5.tgz",
+      "integrity": "sha512-r8Tayk8HJnX0FztbZN7oVqGccWgw98T/0neJphO91KkmOzug1KkofZURD4UaD5uH8AqcFLfdPErnBod0u71/qg==",
+      "dev": true,
+      "license": "MIT"
+    },
     "node_modules/@types/json-schema": {
       "version": "7.0.15",
       "resolved": "https://registry.npmjs.org/@types/json-schema/-/json-schema-7.0.15.tgz",
@@ -2321,6 +2378,13 @@
         "@types/braces": "*"
       }
     },
+    "node_modules/@types/mime": {
+      "version": "1.3.5",
+      "resolved": "https://registry.npmjs.org/@types/mime/-/mime-1.3.5.tgz",
+      "integrity": "sha512-/pyBZWSLD2n0dcHE3hq8s8ZvcETHtEuF+3E7XVt0Ig2nvsVQXdghHVcEkIWjy9A0wKfTn97a/PSDYohKIlnP/w==",
+      "dev": true,
+      "license": "MIT"
+    },
     "node_modules/@types/mime-types": {
       "version": "2.1.4",
       "resolved": "https://registry.npmjs.org/@types/mime-types/-/mime-types-2.1.4.tgz",
@@ -2349,6 +2413,20 @@
       "integrity": "sha512-37i+OaWTh9qeK4LSHPsyRC7NahnGotNuZvjLSgcPzblpHB3rrCJxAOgI5gCdKm7coonsaX1Of0ILiTcnZjbfxA==",
       "license": "MIT"
     },
+    "node_modules/@types/qs": {
+      "version": "6.14.0",
+      "resolved": "https://registry.npmjs.org/@types/qs/-/qs-6.14.0.tgz",
+      "integrity": "sha512-eOunJqu0K1923aExK6y8p6fsihYEn/BYuQ4g0CxAAgFc4b/ZLN4CrsRZ55srTdqoiLzU2B2evC+apEIxprEzkQ==",
+      "dev": true,
+      "license": "MIT"
+    },
+    "node_modules/@types/range-parser": {
+      "version": "1.2.7",
+      "resolved": "https://registry.npmjs.org/@types/range-parser/-/range-parser-1.2.7.tgz",
+      "integrity": "sha512-hKormJbkJqzQGhziax5PItDUTMAM9uE2XXQmM37dyd4hVM+5aVl7oVxMVUiVQn2oCQFN/LKCZdvSM0pFRqbSmQ==",
+      "dev": true,
+      "license": "MIT"
+    },
     "node_modules/@types/react": {
       "version": "19.1.8",
       "resolved": "https://registry.npmjs.org/@types/react/-/react-19.1.8.tgz",
@@ -2376,6 +2454,29 @@
       "dev": true,
       "license": "MIT"
     },
+    "node_modules/@types/send": {
+      "version": "0.17.5",
+      "resolved": "https://registry.npmjs.org/@types/send/-/send-0.17.5.tgz",
+      "integrity": "sha512-z6F2D3cOStZvuk2SaP6YrwkNO65iTZcwA2ZkSABegdkAh/lf+Aa/YQndZVfmEXT5vgAp6zv06VQ3ejSVjAny4w==",
+      "dev": true,
+      "license": "MIT",
+      "dependencies": {
+        "@types/mime": "^1",
+        "@types/node": "*"
+      }
+    },
+    "node_modules/@types/serve-static": {
+      "version": "1.15.8",
+      "resolved": "https://registry.npmjs.org/@types/serve-static/-/serve-static-1.15.8.tgz",
+      "integrity": "sha512-roei0UY3LhpOJvjbIP6ZZFngyLKl5dskOtDhxY5THRSpO+ZI+nzJ+m5yUMzGrp89YRa7lvknKkMYjqQFGwA7Sg==",
+      "dev": true,
+      "license": "MIT",
+      "dependencies": {
+        "@types/http-errors": "*",
+        "@types/node": "*",
+        "@types/send": "*"
+      }
+    },
     "node_modules/@types/shell-quote": {
       "version": "1.7.5",
       "resolved": "https://registry.npmjs.org/@types/shell-quote/-/shell-quote-1.7.5.tgz",
@@ -11382,7 +11483,7 @@
       "version": "0.1.8",
       "dependencies": {
         "@google/genai": "^1.4.0",
-        "@modelcontextprotocol/sdk": "^1.11.0",
+        "@modelcontextprotocol/sdk": "^1.13.2",
         "@opentelemetry/api": "^1.9.0",
         "@opentelemetry/exporter-logs-otlp-grpc": "^0.52.0",
         "@opentelemetry/exporter-metrics-otlp-grpc": "^0.52.0",
@@ -11444,6 +11545,73 @@
           "optional": true
         }
       }
+    },
+    "packages/mcp-server": {
+      "name": "@google/gemini-mcp-server",
+      "version": "0.1.8",
+      "dependencies": {
+        "@google/gemini-cli": "*",
+        "@google/gemini-cli-core": "*",
+        "@modelcontextprotocol/sdk": "^1.13.2",
+        "express": "^5.1.0",
+        "openai": "^5.8.2",
+        "zod": "^3.23.8"
+      },
+      "bin": {
+        "gemini-mcp-server": "dist/index.js"
+      },
+      "devDependencies": {
+        "@types/express": "^5.0.3",
+        "typescript": "^5.3.3",
+        "vitest": "^3.1.1"
+      },
+      "engines": {
+        "node": ">=18"
+      }
+    },
+    "packages/mcp-server/node_modules/openai": {
+      "version": "5.8.2",
+      "resolved": "https://registry.npmjs.org/openai/-/openai-5.8.2.tgz",
+      "integrity": "sha512-8C+nzoHYgyYOXhHGN6r0fcb4SznuEn1R7YZMvlqDbnCuE0FM2mm3T1HiYW6WIcMS/F1Of2up/cSPjLPaWt0X9Q==",
+      "license": "Apache-2.0",
+      "bin": {
+        "openai": "bin/cli"
+      },
+      "peerDependencies": {
+        "ws": "^8.18.0",
+        "zod": "^3.23.8"
+      },
+      "peerDependenciesMeta": {
+        "ws": {
+          "optional": true
+        },
+        "zod": {
+          "optional": true
+        }
+      }
+    },
+    "packages/mcp-server/node_modules/ws": {
+      "version": "8.18.3",
+      "resolved": "https://registry.npmjs.org/ws/-/ws-8.18.3.tgz",
+      "integrity": "sha512-PEIGCY5tSlUt50cqyMXfCzX+oOPqN0vuGqWzbcJ2xvnkzkq46oOpz7dQaTDBdfICb4N14+GARUDw2XV2N4tvzg==",
+      "license": "MIT",
+      "optional": true,
+      "peer": true,
+      "engines": {
+        "node": ">=10.0.0"
+      },
+      "peerDependencies": {
+        "bufferutil": "^4.0.1",
+        "utf-8-validate": ">=5.0.2"
+      },
+      "peerDependenciesMeta": {
+        "bufferutil": {
+          "optional": true
+        },
+        "utf-8-validate": {
+          "optional": true
+        }
+      }
     }
   }
 }
diff --git a/packages/cli/package.json b/packages/cli/package.json
index ade14e16..db5093a3 100644
--- a/packages/cli/package.json
+++ b/packages/cli/package.json
@@ -5,6 +5,10 @@
   "repository": "google-gemini/gemini-cli",
   "type": "module",
   "main": "dist/index.js",
+  "exports": {
+    ".": "./dist/index.js",
+    "./public-api": "./dist/src/public-api.js"
+  },
   "bin": {
     "gemini": "dist/index.js"
   },
diff --git a/packages/cli/src/config/config.ts b/packages/cli/src/config/config.ts
index 552a8f67..99e1dd7d 100644
--- a/packages/cli/src/config/config.ts
+++ b/packages/cli/src/config/config.ts
@@ -161,6 +161,7 @@ export async function loadHierarchicalGeminiMemory(
   );
 }
 
+
 export async function loadCliConfig(
   settings: Settings,
   extensions: Extension[],
diff --git a/packages/cli/src/public-api.ts b/packages/cli/src/public-api.ts
new file mode 100644
index 00000000..49ecfdc3
--- /dev/null
+++ b/packages/cli/src/public-api.ts
@@ -0,0 +1,5 @@
+export { loadEnvironment } from './config/config.js';
+export { loadSettings, type Settings } from './config/settings.js';
+export { loadExtensions, type Extension } from './config/extension.js';
+export { loadSandboxConfig } from './config/sandboxConfig.js';
+export { getCliVersion } from './utils/version.js';
diff --git a/packages/core/package.json b/packages/core/package.json
index e4a3a334..b831cc43 100644
--- a/packages/core/package.json
+++ b/packages/core/package.json
@@ -24,7 +24,7 @@
   ],
   "dependencies": {
     "@google/genai": "^1.4.0",
-    "@modelcontextprotocol/sdk": "^1.11.0",
+    "@modelcontextprotocol/sdk": "^1.13.2",
     "@opentelemetry/api": "^1.9.0",
     "@opentelemetry/exporter-logs-otlp-grpc": "^0.52.0",
     "@opentelemetry/exporter-metrics-otlp-grpc": "^0.52.0",
diff --git a/packages/mcp-server/package.json b/packages/mcp-server/package.json
new file mode 100644
index 00000000..e3787a64
--- /dev/null
+++ b/packages/mcp-server/package.json
@@ -0,0 +1,39 @@
+{
+  "name": "@google/gemini-mcp-server",
+  "version": "0.1.8",
+  "type": "module",
+  "main": "dist/index.js",
+  "bin": {
+    "gemini-mcp-server": "dist/index.js"
+  },
+  "scripts": {
+    "build": "node ../../scripts/build_package.js",
+    "clean": "rm -rf dist",
+    "start": "node dist/index.js",
+    "debug": "node --inspect-brk dist/index.js",
+    "lint": "eslint . --ext .ts,.tsx",
+    "format": "prettier --write .",
+    "test": "vitest run",
+    "typecheck": "tsc --noEmit",
+    "prerelease:version": "node ../../scripts/bind_package_version.js",
+    "prerelease:deps": "node ../../scripts/bind_package_dependencies.js",
+    "prepack": "npm run build",
+    "prepublishOnly": "node ../../scripts/prepublish.js"
+  },
+  "dependencies": {
+    "@google/gemini-cli": "*",
+    "@google/gemini-cli-core": "*",
+    "@modelcontextprotocol/sdk": "^1.13.2",
+    "express": "^5.1.0",
+    "openai": "^5.8.2",
+    "zod": "^3.23.8"
+  },
+  "devDependencies": {
+    "@types/express": "^5.0.3",
+    "typescript": "^5.3.3",
+    "vitest": "^3.1.1"
+  },
+  "engines": {
+    "node": ">=18"
+  }
+}
diff --git a/packages/mcp-server/src/bridge/bridge.ts b/packages/mcp-server/src/bridge/bridge.ts
new file mode 100644
index 00000000..c8e1b5b1
--- /dev/null
+++ b/packages/mcp-server/src/bridge/bridge.ts
@@ -0,0 +1,228 @@
+import express, { Request, Response, NextFunction, Application } from 'express';
+import { McpServer } from '@modelcontextprotocol/sdk/server/mcp.js';
+import { StreamableHTTPServerTransport } from '@modelcontextprotocol/sdk/server/streamableHttp.js';
+import { z } from 'zod';
+import {
+  type Config,
+  type Tool as GcliTool,
+  type ToolResult,
+  GeminiChat,
+} from '@google/gemini-cli-core';
+import {
+  type CallToolResult,
+  isInitializeRequest,
+} from '@modelcontextprotocol/sdk/types.js';
+import {
+  type PartUnion,
+  type Tool,
+  type GenerateContentConfig,
+  type Content,
+} from '@google/genai';
+import { randomUUID } from 'node:crypto';
+
+const LOG_PREFIX = '[MCP SERVER]';
+
+// NEW: æ—¥å¿—ä¸­é—´ä»¶
+const requestLogger = (req: Request, res: Response, next: NextFunction) => {
+  console.log(`${LOG_PREFIX} â¬‡ï¸  Incoming Request: ${req.method} ${req.url}`);
+  console.log(`${LOG_PREFIX}    Headers:`, JSON.stringify(req.headers, null, 2));
+  if (req.body && Object.keys(req.body).length > 0) {
+    const bodyStr = JSON.stringify(req.body);
+    console.log(
+      `${LOG_PREFIX}    Body:`,
+      bodyStr.length > 300 ? bodyStr.substring(0, 300) + '...' : bodyStr,
+    );
+  }
+  next();
+};
+
+export class GcliMcpBridge {
+  private readonly config: Config;
+  private readonly cliVersion: string;
+  private readonly mcpServer: McpServer;
+
+  constructor(config: Config, cliVersion: string) {
+    this.config = config;
+    this.cliVersion = cliVersion;
+    this.mcpServer = new McpServer(
+      {
+        name: 'gemini-cli-mcp-server',
+        version: this.cliVersion,
+      },
+      { capabilities: { tools: { listChanged: true }, logging: {} } },
+    );
+  }
+
+  public async start(app: Application) {
+    await this.registerAllGcliTools();
+
+    // NEW: ä½¿ç”¨æ—¥å¿—ä¸­é—´ä»¶
+    app.use(requestLogger);
+
+    const transports: Record<string, StreamableHTTPServerTransport> = {};
+
+    app.all('/mcp', async (req: Request, res: Response) => {
+      const sessionId = req.headers['mcp-session-id'] as string | undefined;
+      let transport = sessionId ? transports[sessionId] : undefined;
+
+      if (!transport) {
+        if (isInitializeRequest(req.body)) {
+          console.log(
+            `${LOG_PREFIX} creating new transport for initialize request.`,
+          );
+          transport = new StreamableHTTPServerTransport({
+            sessionIdGenerator: () => randomUUID(),
+            onsessioninitialized: newSessionId => {
+              console.log(
+                `${LOG_PREFIX} âœ… Session initialized with ID: ${newSessionId}`,
+              );
+              transports[newSessionId] = transport!;
+            },
+          });
+
+          transport.onclose = () => {
+            const sid = transport!.sessionId;
+            if (sid && transports[sid]) {
+              console.log(
+                `${LOG_PREFIX} ğŸšª Transport for session ${sid} closed.`,
+              );
+              delete transports[sid];
+            }
+          };
+
+          // Connect the new transport to the *existing* McpServer
+          await this.mcpServer.connect(transport);
+        } else {
+          console.error(
+            `${LOG_PREFIX} âŒ Bad Request: Missing or invalid session ID for non-initialize request.`,
+          );
+          res.status(400).json({
+            jsonrpc: '2.0',
+            error: {
+              code: -32000,
+              message: 'Bad Request: Missing or invalid session ID.',
+            },
+            id: null,
+          });
+          return;
+        }
+      } else {
+        console.log(
+          `${LOG_PREFIX}  reusing transport for session: ${sessionId}`,
+        );
+      }
+
+      try {
+        await transport.handleRequest(req, res, req.body);
+      } catch (e) {
+        console.error(`${LOG_PREFIX} ğŸ’¥ Error handling request:`, e);
+        if (!res.headersSent) {
+          res.status(500).end();
+        }
+      }
+    });
+  }
+
+  private async registerAllGcliTools() {
+    const toolRegistry = await this.config.getToolRegistry();
+    const allTools = toolRegistry.getAllTools();
+    for (const tool of allTools) {
+      this.registerGcliTool(tool);
+    }
+  }
+
+  private registerGcliTool(tool: GcliTool) {
+    const inputSchema = this.convertJsonSchemaToZod(tool.schema.parameters);
+
+    this.mcpServer.registerTool(
+      tool.name,
+      {
+        title: tool.displayName,
+        description: tool.description,
+        inputSchema: inputSchema,
+      },
+      async (args, extra) => {
+        const result = await tool.execute(args, extra.signal);
+        return this.convertGcliResultToMcpResult(result);
+      },
+    );
+  }
+
+
+  private convertJsonSchemaToZod(jsonSchema: any): any {
+    // Helper to convert a single JSON schema property to a Zod type.
+    // This is defined as an inner arrow function to recursively call itself for arrays
+    // and to call the outer function for nested objects via `this`.
+    const convertProperty = (prop: any): z.ZodTypeAny => {
+      if (!prop || !prop.type) {
+        return z.any();
+      }
+
+      switch (prop.type) {
+        case 'string':
+          return z.string().describe(prop.description || '');
+        case 'number':
+          return z.number().describe(prop.description || '');
+        case 'boolean':
+          return z.boolean().describe(prop.description || '');
+        case 'array':
+          // This is the key fix: recursively call the converter for `items`.
+          if (!prop.items) {
+            // A valid array schema MUST have `items`. Fallback to `any` if missing.
+            return z.array(z.any()).describe(prop.description || '');
+          }
+          return z
+            .array(convertProperty(prop.items))
+            .describe(prop.description || '');
+        case 'object':
+          // For nested objects, recursively call the main function to get the shape.
+          return z
+            .object(this.convertJsonSchemaToZod(prop))
+            .passthrough()
+            .describe(prop.description || '');
+        default:
+          return z.any();
+      }
+    };
+
+    // If no schema or properties, return an empty shape object.
+    if (!jsonSchema || !jsonSchema.properties) {
+      return {};
+    }
+
+    const shape: any = {};
+    for (const [key, prop] of Object.entries(jsonSchema.properties)) {
+      let fieldSchema = convertProperty(prop as any);
+
+      if (!jsonSchema.required || !jsonSchema.required.includes(key)) {
+        fieldSchema = fieldSchema.optional();
+      }
+      shape[key] = fieldSchema;
+    }
+    return shape; // Directly return the shape object.
+  }
+
+  private convertGcliResultToMcpResult(
+    gcliResult: ToolResult,
+  ): CallToolResult {
+    if (typeof gcliResult.llmContent === 'string') {
+      return { content: [{ type: 'text', text: gcliResult.llmContent }] };
+    }
+
+    const parts = Array.isArray(gcliResult.llmContent)
+      ? gcliResult.llmContent
+      : [gcliResult.llmContent];
+
+    const contentBlocks = parts.map((part: PartUnion) => {
+      if (typeof part === 'string') {
+        return { type: 'text' as const, text: part };
+      }
+      if ('text' in part && part.text) {
+        return { type: 'text' as const, text: part.text };
+      }
+      return { type: 'text' as const, text: '[Unsupported Part Type]' };
+    });
+
+    return { content: contentBlocks };
+  }
+}
diff --git a/packages/mcp-server/src/bridge/index.ts b/packages/mcp-server/src/bridge/index.ts
new file mode 100644
index 00000000..08ea4b6b
--- /dev/null
+++ b/packages/mcp-server/src/bridge/index.ts
@@ -0,0 +1 @@
+export { GcliMcpBridge } from './bridge.js';
diff --git a/packages/mcp-server/src/bridge/openai.ts b/packages/mcp-server/src/bridge/openai.ts
new file mode 100644
index 00000000..1660fe00
--- /dev/null
+++ b/packages/mcp-server/src/bridge/openai.ts
@@ -0,0 +1,139 @@
+import { Router, Request, Response } from 'express';
+import { type Config, GeminiChat } from '@google/gemini-cli-core';
+import { createOpenAIStreamTransformer } from './stream-transformer.js';
+import { type Content } from '@google/genai';
+import { WritableStream } from 'node:stream/web';
+import { randomUUID } from 'node:crypto';
+
+// å®šä¹‰ OpenAI è¯·æ±‚ä½“çš„æ¥å£
+interface OpenAIChatCompletionRequest {
+  model: string;
+  messages: Array<{ role: string; content: string }>;
+  stream?: boolean;
+}
+
+export function createOpenAIRouter(config: Config): Router {
+  const router = Router();
+  // æ³¨æ„ï¼šåŸºäº bridge.ts çš„ç°æœ‰ä»£ç ï¼Œæˆ‘ä»¬å‡è®¾ getGeminiClient å’Œ getContentGenerator æ–¹æ³•å­˜åœ¨ã€‚
+  const contentGenerator = config.getGeminiClient().getContentGenerator();
+
+  // OpenAI chat completions ç«¯ç‚¹
+  router.post(
+    '/chat/completions',
+    async (req: Request, res: Response) => {
+      const body = req.body as OpenAIChatCompletionRequest;
+
+      // ç¡®ä¿ stream é»˜è®¤ä¸º trueï¼Œé™¤éæ˜¾å¼è®¾ç½®ä¸º false
+      const stream = body.stream !== false;
+
+      if (!body.messages || body.messages.length === 0) {
+        res.status(400).json({ error: 'messages is required' });
+        return;
+      }
+
+      // å°† OpenAI æ ¼å¼çš„ messages è½¬æ¢ä¸º Gemini æ ¼å¼
+      // æ³¨æ„ï¼šè¿™é‡Œç®€åŒ–äº†å¤„ç†ï¼Œå®é™…å¯èƒ½éœ€è¦å¤„ç† system prompt ç­‰
+      const history: Content[] = body.messages.map(msg => ({
+        role: msg.role === 'assistant' ? 'model' : 'user',
+        parts: [{ text: msg.content }],
+      }));
+
+      const lastMessage = history.pop();
+      if (!lastMessage) {
+        res.status(400).json({ error: 'No message to send.' });
+        return;
+      }
+
+      try {
+        const oneShotChat = new GeminiChat(
+          config,
+          contentGenerator,
+          {}, // generationConfig
+          history,
+        );
+
+        if (stream) {
+          // --- æµå¼å“åº” ---
+          res.setHeader('Content-Type', 'text/event-stream');
+          res.setHeader('Cache-Control', 'no-cache');
+          res.setHeader('Connection', 'keep-alive');
+          res.flushHeaders(); // ç«‹å³å‘é€å¤´ä¿¡æ¯
+
+          const geminiStream = await oneShotChat.sendMessageStream({
+            message: lastMessage.parts || [],
+          });
+          const openAIStream = createOpenAIStreamTransformer(body.model);
+
+          const writer = new WritableStream({
+            write(chunk) {
+              res.write(chunk);
+            },
+          });
+
+          const readableStream = new ReadableStream({
+            async start(controller) {
+              for await (const value of geminiStream) {
+                controller.enqueue(value);
+              }
+              controller.close();
+            },
+          });
+
+          await readableStream.pipeThrough(openAIStream).pipeTo(writer);
+
+          res.end();
+        } else {
+          // --- éæµå¼å“åº”ï¼ˆä¸ºäº†å®Œæ•´æ€§ï¼‰ ---
+          const result = await oneShotChat.sendMessage({
+            message: lastMessage.parts || [],
+          });
+          const responseText =
+            result.candidates?.[0]?.content?.parts?.[0]?.text || '';
+
+          res.json({
+            id: `chatcmpl-${randomUUID()}`,
+            object: 'chat.completion',
+            created: Math.floor(Date.now() / 1000),
+            model: body.model,
+            choices: [
+              {
+                index: 0,
+                message: {
+                  role: 'assistant',
+                  content: responseText,
+                },
+                finish_reason: 'stop',
+              },
+            ],
+          });
+        }
+      } catch (error) {
+        console.error('[OpenAI Bridge] Error:', error);
+        const errorMessage =
+          error instanceof Error ? error.message : 'An unknown error occurred';
+        if (!res.headersSent) {
+          res.status(500).json({ error: errorMessage });
+        } else {
+          // å¦‚æœå¤´å·²å‘é€ï¼Œåªèƒ½å°è¯•å†™å…¥é”™è¯¯ä¿¡æ¯å¹¶ç»“æŸæµ
+          res.write(`data: ${JSON.stringify({ error: errorMessage })}\n\n`);
+          res.end();
+        }
+      }
+    },
+  );
+
+  // å¯ä»¥æ·»åŠ  /v1/models ç«¯ç‚¹
+  router.get('/models', (req, res) => {
+    // è¿™é‡Œå¯ä»¥è¿”å›ä¸€ä¸ªå›ºå®šçš„æ¨¡å‹åˆ—è¡¨ï¼Œæˆ–è€…ä» config ä¸­è·å–
+    res.json({
+      object: 'list',
+      data: [
+        { id: 'gemini-1.5-pro-latest', object: 'model', owned_by: 'google' },
+        { id: 'gemini-1.5-flash-latest', object: 'model', owned_by: 'google' },
+        { id: 'gemini-pro', object: 'model', owned_by: 'google' },
+      ],
+    });
+  });
+
+  return router;
+}
diff --git a/packages/mcp-server/src/bridge/stream-transformer.ts b/packages/mcp-server/src/bridge/stream-transformer.ts
new file mode 100644
index 00000000..e075edeb
--- /dev/null
+++ b/packages/mcp-server/src/bridge/stream-transformer.ts
@@ -0,0 +1,73 @@
+import { GenerateContentResponse } from '@google/genai';
+
+// å®šä¹‰ OpenAI æµå¼å“åº”çš„å—ç»“æ„
+interface OpenAIDelta {
+  role?: 'user' | 'assistant' | 'system';
+  content?: string;
+}
+
+interface OpenAIChoice {
+  index: number;
+  delta: OpenAIDelta;
+  finish_reason: string | null;
+}
+
+interface OpenAIChunk {
+  id: string;
+  object: 'chat.completion.chunk';
+  created: number;
+  model: string;
+  choices: OpenAIChoice[];
+}
+
+/**
+ * åˆ›å»ºä¸€ä¸ª TransformStreamï¼Œå°† Gemini çš„æµå¼å“åº”å—è½¬æ¢ä¸º OpenAI å…¼å®¹çš„ SSE äº‹ä»¶ã€‚
+ * @param model - æ­£åœ¨ä½¿ç”¨çš„æ¨¡å‹åç§°ï¼Œç”¨äºå¡«å…… OpenAI å“åº”ã€‚
+ * @returns ä¸€ä¸ª TransformStream å®ä¾‹ã€‚
+ */
+export function createOpenAIStreamTransformer(
+  model: string,
+): TransformStream<GenerateContentResponse, Uint8Array> {
+  const chatID = `chatcmpl-${crypto.randomUUID()}`;
+  const creationTime = Math.floor(Date.now() / 1000);
+  const encoder = new TextEncoder();
+  let isFirstChunk = true;
+
+  return new TransformStream({
+    transform(chunk, controller) {
+      const text = chunk.candidates?.[0]?.content?.parts?.[0]?.text;
+      if (!text) {
+        return;
+      }
+
+      const delta: OpenAIDelta = { content: text };
+      if (isFirstChunk) {
+        delta.role = 'assistant';
+        isFirstChunk = false;
+      }
+
+      const openAIChunk: OpenAIChunk = {
+        id: chatID,
+        object: 'chat.completion.chunk',
+        created: creationTime,
+        model: model,
+        choices: [
+          {
+            index: 0,
+            delta: delta,
+            finish_reason: null,
+          },
+        ],
+      };
+
+      // æŒ‰ç…§ SSE æ ¼å¼ç¼–ç 
+      const sseString = `data: ${JSON.stringify(openAIChunk)}\n\n`;
+      controller.enqueue(encoder.encode(sseString));
+    },
+    flush(controller) {
+      // æµç»“æŸæ—¶ï¼Œå‘é€ [DONE] æ¶ˆæ¯
+      const doneString = `data: [DONE]\n\n`;
+      controller.enqueue(encoder.encode(doneString));
+    },
+  });
+}
diff --git a/packages/mcp-server/src/index.ts b/packages/mcp-server/src/index.ts
new file mode 100644
index 00000000..668c7360
--- /dev/null
+++ b/packages/mcp-server/src/index.ts
@@ -0,0 +1,174 @@
+import {
+  Config,
+  ApprovalMode,
+  sessionId,
+  loadServerHierarchicalMemory,
+  FileDiscoveryService,
+  DEFAULT_GEMINI_MODEL,
+  DEFAULT_GEMINI_EMBEDDING_MODEL,
+  MCPServerConfig,
+  AuthType,
+} from '@google/gemini-cli-core';
+import {
+  loadSettings,
+  type Settings,
+  loadExtensions,
+  type Extension,
+  getCliVersion,
+  loadEnvironment,
+  loadSandboxConfig,
+} from '@google/gemini-cli/public-api';
+import { GcliMcpBridge } from './bridge/bridge.js';
+import { createOpenAIRouter } from './bridge/openai.js';
+import express from 'express';
+
+// Simple console logger for now
+const logger = {
+  // eslint-disable-next-line @typescript-eslint/no-explicit-any
+  warn: (...args: any[]) => console.warn('[WARN]', ...args),
+};
+
+function mergeMcpServers(
+  settings: Settings,
+  extensions: Extension[],
+): Record<string, MCPServerConfig> {
+  const mcpServers: Record<string, MCPServerConfig> = {
+    ...(settings.mcpServers || {}),
+  };
+  for (const extension of extensions) {
+    Object.entries(extension.config.mcpServers || {}).forEach(
+      ([key, server]) => {
+        if (mcpServers[key]) {
+          logger.warn(
+            `Skipping extension MCP config for server with key "${key}" as it already exists.`,
+          );
+          return;
+        }
+        mcpServers[key] = server;
+      },
+    );
+  }
+  return mcpServers;
+}
+
+async function startMcpServer() {
+  // 1. ç‹¬ç«‹çš„ã€ç®€å•çš„å‚æ•°è§£æ
+  const args = process.argv.slice(2);
+  const portArg = args.find(arg => arg.startsWith('--port='));
+  const port = portArg ? parseInt(portArg.split('=')[1], 10) : 8765;
+  const debugMode = args.includes('--debug');
+
+  if (isNaN(port)) {
+    console.error('Invalid port number provided. Use --port=<number>.');
+    process.exit(1);
+  }
+
+  console.log('Starting Gemini CLI in MCP Server Mode...');
+
+  // 2. å¤ç”¨é…ç½®åŠ è½½çš„æ ¸å¿ƒéƒ¨åˆ†ï¼Œä½†æ‰‹åŠ¨æ„é€  Config
+  loadEnvironment(); // åŠ è½½ .env æ–‡ä»¶
+  const workspaceRoot = process.cwd();
+  const settings = loadSettings(workspaceRoot);
+  const extensions = loadExtensions(workspaceRoot);
+  const cliVersion = await getCliVersion();
+
+  // 3. æ‰‹åŠ¨æ„é€  ConfigParametersï¼Œç»•å¼€ yargs
+  const fileDiscoveryService = new FileDiscoveryService(workspaceRoot);
+  const extensionContextFilePaths = extensions.flatMap(e => e.contextFiles);
+  const { memoryContent, fileCount } = await loadServerHierarchicalMemory(
+    workspaceRoot,
+    debugMode,
+    fileDiscoveryService,
+    extensionContextFilePaths,
+  );
+
+  const mockArgvForSandbox = {};
+  const sandboxConfig = await loadSandboxConfig(
+    settings.merged,
+    mockArgvForSandbox,
+  );
+
+  const mcpServers = mergeMcpServers(settings.merged, extensions);
+
+  const config = new Config({
+    sessionId: sessionId,
+    model: process.env.GEMINI_MODEL || DEFAULT_GEMINI_MODEL,
+    embeddingModel: DEFAULT_GEMINI_EMBEDDING_MODEL,
+    targetDir: workspaceRoot,
+    cwd: workspaceRoot,
+    debugMode: debugMode,
+    approvalMode: ApprovalMode.YOLO, // å¼ºåˆ¶ä¸º YOLO æ¨¡å¼
+    sandbox: sandboxConfig,
+    userMemory: memoryContent,
+    geminiMdFileCount: fileCount,
+    fileDiscoveryService,
+    coreTools: settings.merged.coreTools,
+    excludeTools: settings.merged.excludeTools,
+    toolDiscoveryCommand: settings.merged.toolDiscoveryCommand,
+    toolCallCommand: settings.merged.toolCallCommand,
+    mcpServers: mcpServers,
+    extensionContextFilePaths,
+    showMemoryUsage: settings.merged.showMemoryUsage,
+    accessibility: settings.merged.accessibility,
+    telemetry: settings.merged.telemetry,
+    usageStatisticsEnabled: settings.merged.usageStatisticsEnabled,
+    fileFiltering: settings.merged.fileFiltering,
+    checkpointing: settings.merged.checkpointing?.enabled,
+    proxy:
+      process.env.HTTPS_PROXY ||
+      process.env.https_proxy ||
+      process.env.HTTP_PROXY ||
+      process.env.http_proxy,
+    bugCommand: settings.merged.bugCommand,
+  });
+
+  // Initialize Auth - this is critical to initialize the tool registry and gemini client
+  let selectedAuthType = settings.merged.selectedAuthType;
+  if (!selectedAuthType && !process.env.GEMINI_API_KEY) {
+    console.error(
+      'Auth missing: Please set `selectedAuthType` in .gemini/settings.json or set the GEMINI_API_KEY environment variable.',
+    );
+    process.exit(1);
+  }
+  selectedAuthType = selectedAuthType || AuthType.USE_GEMINI;
+  await config.refreshAuth(selectedAuthType);
+  console.log(`Using authentication method: ${selectedAuthType}`);
+
+  // Check for the custom tools model environment variable
+  const toolsDefaultModel = process.env.GEMINI_TOOLS_DEFAULT_MODEL;
+  if (toolsDefaultModel && toolsDefaultModel.trim() !== '') {
+    config.setModel(toolsDefaultModel.trim());
+    console.log(`ğŸš€ Using custom model for tools: ${toolsDefaultModel.trim()}`);
+  } else {
+    // Log the default model being used if the environment variable is not set
+    console.log(`âš™ï¸  Using default model for tools: ${config.getModel()}`);
+  }
+
+  // 4. åˆå§‹åŒ–å¹¶å¯åŠ¨ MCP æ¡¥æ¥æœåŠ¡ å’Œ OpenAI æœåŠ¡
+  const mcpBridge = new GcliMcpBridge(config, cliVersion);
+
+  const app = express();
+  app.use(express.json());
+
+  // å¯åŠ¨ MCP æœåŠ¡ (è¿™æ˜¯ GcliMcpBridge çš„ä¸€éƒ¨åˆ†ï¼Œæˆ‘ä»¬éœ€è¦æŠŠå®ƒé›†æˆåˆ°ä¸» app ä¸­)
+  await mcpBridge.start(app); // ä¿®æ”¹ start æ–¹æ³•ä»¥æ¥æ”¶ express app å®ä¾‹
+
+  // å¯åŠ¨ OpenAI å…¼å®¹ç«¯ç‚¹
+  const openAIRouter = createOpenAIRouter(config);
+  app.use('/v1', openAIRouter);
+
+  app.listen(port, () => {
+    console.log(
+      `ğŸš€ Gemini CLI MCP Server and OpenAI Bridge are running on port ${port}`,
+    );
+    console.log(`   - MCP transport listening on http://localhost:${port}/mcp`);
+    console.log(
+      `   - OpenAI-compatible endpoints available at http://localhost:${port}/v1`,
+    );
+  });
+}
+
+startMcpServer().catch(error => {
+  console.error('Failed to start Gemini CLI MCP Bridge:', error);
+  process.exit(1);
+});
diff --git a/packages/mcp-server/src/mcp-test-client.ts b/packages/mcp-server/src/mcp-test-client.ts
new file mode 100644
index 00000000..2a019d32
--- /dev/null
+++ b/packages/mcp-server/src/mcp-test-client.ts
@@ -0,0 +1,202 @@
+import { Client } from '@modelcontextprotocol/sdk/client/index.js';
+import { StreamableHTTPClientTransport } from '@modelcontextprotocol/sdk/client/streamableHttp.js';
+import {
+  ListToolsResultSchema,
+  type Notification,
+} from '@modelcontextprotocol/sdk/types.js'; // <--- å¼•å…¥ Notification ç±»å‹
+import { URL } from 'url';
+import { z } from 'zod';
+import OpenAI from 'openai';
+
+// Define the schema for a text content block, as it's not exported by the SDK.
+const TextContentBlockSchema = z.object({
+  type: z.literal('text'),
+  text: z.string(),
+});
+
+// --- é…ç½® ---
+const SERVER_URL = 'http://localhost:8765/mcp';
+const LOG_PREFIX = '[TEST CLIENT]';
+// -------------
+
+function logWithPrefix(...args: unknown[]) {
+  console.log(LOG_PREFIX, ...args);
+}
+
+// --- çŒ´å­è¡¥ä¸ fetch ---
+const originalFetch = global.fetch;
+global.fetch = async (url, options) => {
+  logWithPrefix('â¡ï¸  FETCHING:', options?.method || 'GET', url.toString());
+  if (options?.headers) {
+    logWithPrefix(
+      '   Headers:',
+      JSON.stringify(
+        Object.fromEntries((options.headers as Headers).entries()),
+        null,
+        2,
+      ),
+    );
+  }
+  if (options?.body) {
+    const bodyStr = options.body.toString();
+    logWithPrefix(
+      '   Body:',
+      bodyStr.length > 300 ? bodyStr.substring(0, 300) + '...' : bodyStr,
+    );
+  }
+
+  const response = await originalFetch(url, options);
+
+  logWithPrefix(
+    'â¬…ï¸  RESPONSE:',
+    response.status,
+    response.statusText,
+    'from',
+    options?.method || 'GET',
+    url.toString(),
+  );
+  logWithPrefix(
+    '   Response Headers:',
+    JSON.stringify(Object.fromEntries(response.headers.entries()), null, 2),
+  );
+
+  const clonedResponse = response.clone();
+  clonedResponse
+    .text()
+    .then(text => {
+      if (text) {
+        logWithPrefix(
+          '   Response Body:',
+          text.length > 300 ? text.substring(0, 300) + '...' : text,
+        );
+      }
+    })
+    .catch(() => {});
+
+  return response;
+};
+// -----------------------
+
+async function runTestClient() {
+  logWithPrefix('ğŸš€ Starting MCP Test Client...');
+  logWithPrefix(`ğŸ¯ Target Server URL: ${SERVER_URL}`);
+
+  const client = new Client({
+    name: 'mcp-debug-client',
+    version: '1.0.0',
+  });
+
+  client.onerror = (error: Error) => {
+    console.error(`${LOG_PREFIX} ğŸ’¥ Client-level Error:`, error);
+  };
+
+  // --- ä¿®æ­£çš„éƒ¨åˆ† ---
+  // å°†å‚æ•°ç±»å‹ä» JSONRPCMessage æ”¹ä¸º Notification
+  client.fallbackNotificationHandler = async (notification: Notification) => {
+    logWithPrefix(
+      `ğŸ“¡ Received Unhandled Notification:`,
+      JSON.stringify(notification, null, 2),
+    );
+  };
+  // -------------------
+
+  logWithPrefix('ğŸšŒ Creating StreamableHTTPClientTransport...');
+  const transport = new StreamableHTTPClientTransport(new URL(SERVER_URL));
+
+  transport.onmessage = message => {
+    logWithPrefix('ğŸ“¥ Received Message:', JSON.stringify(message, null, 2));
+  };
+
+  try {
+    logWithPrefix('ğŸ”Œ Attempting to connect to server...');
+    await client.connect(transport);
+    logWithPrefix('âœ… Connection successful! Initialization complete.');
+    logWithPrefix('ğŸ” Server Info:', client.getServerVersion());
+    logWithPrefix('ğŸ› ï¸ Server Capabilities:', client.getServerCapabilities());
+  } catch (error) {
+    console.error(`${LOG_PREFIX} âŒ Failed to connect or initialize:`, error);
+    process.exit(1);
+  }
+
+  try {
+    logWithPrefix('ğŸ“‹ Requesting tool list...');
+    const result = await client.request(
+      { method: 'tools/list' },
+      ListToolsResultSchema,
+    );
+
+    logWithPrefix('âœ… Successfully received tool list response!');
+
+    if (result.tools && result.tools.length > 0) {
+      logWithPrefix(`ğŸ› ï¸ Discovered ${result.tools.length} tools:`);
+      result.tools.forEach((tool, index) => {
+        logWithPrefix(`  ${index + 1}. Name: ${tool.name}`);
+        logWithPrefix(`     Title: ${tool.title || 'N/A'}`);
+        logWithPrefix(`     Description: ${tool.description || 'N/A'}`);
+        logWithPrefix(
+          `     Input Schema:`,
+          JSON.stringify(tool.inputSchema, null, 2),
+        );
+      });
+    } else {
+      logWithPrefix('âš ï¸ Server returned an empty list of tools.');
+    }
+  } catch (error) {
+    console.error(`${LOG_PREFIX} âŒ Failed to list tools:`, error);
+  } finally {
+    logWithPrefix('ğŸ‘‹ Closing MCP connection...');
+    await client.close();
+    logWithPrefix('ğŸšª MCP Connection closed.');
+  }
+
+  // Now, test the OpenAI endpoint
+  await testOpenAIEndpoint();
+
+  logWithPrefix('âœ… Test finished.');
+}
+
+async function testOpenAIEndpoint() {
+  logWithPrefix('-----------------------------------');
+  logWithPrefix('ğŸš€ Testing OpenAI compatible endpoint...');
+
+  const openai = new OpenAI({
+    baseURL: 'http://localhost:8765/v1',
+    apiKey: 'not-needed', // The API key is not used by our local server
+  });
+
+  try {
+    const stream = await openai.chat.completions.create({
+      model: 'gemini-pro', // This can be any string, it's passed to the transformer
+      messages: [{ role: 'user', content: 'Why is the sky blue?' }],
+      stream: true,
+    });
+
+    let fullResponse = '';
+    logWithPrefix('âœ… Stream opened. Receiving response...');
+    for await (const chunk of stream) {
+      const content = chunk.choices[0]?.delta?.content || '';
+      fullResponse += content;
+      process.stdout.write(content);
+    }
+    console.log(''); // Newline after stream
+
+    if (fullResponse.toLowerCase().includes('scattering')) {
+      logWithPrefix('âœ… Validation successful: Response contains "scattering".');
+    } else {
+      console.error(
+        `${LOG_PREFIX} âŒ Validation failed: Response did not contain "scattering".`,
+      );
+    }
+  } catch (error) {
+    console.error(
+      `${LOG_PREFIX} âŒ Failed to call OpenAI endpoint:`,
+      error,
+    );
+  }
+  logWithPrefix('-----------------------------------');
+}
+
+runTestClient().catch(error => {
+  console.error(`${LOG_PREFIX} ğŸš¨ Unhandled top-level error:`, error);
+  process.exit(1);
+});
diff --git a/packages/mcp-server/tsconfig.json b/packages/mcp-server/tsconfig.json
new file mode 100644
index 00000000..e0fb72ce
--- /dev/null
+++ b/packages/mcp-server/tsconfig.json
@@ -0,0 +1,18 @@
+{
+  "extends": "../../tsconfig.json",
+  "compilerOptions": {
+    "outDir": "dist",
+    "rootDir": "src"
+  },
+  "include": [
+    "src/**/*.ts"
+  ],
+  "references": [
+    {
+      "path": "../core"
+    },
+    {
+      "path": "../cli"
+    }
+  ]
+}
