# Gemini CLI - MCP / OpenAI Bridge Server (`@gemini-community/gemini-mcp-server`)

`@gemini-community/gemini-mcp-server` is a versatile companion application designed to serve as a powerful extension for the `gemini-cli` ecosystem. It primarily fulfills two core roles:

1.  **MCP (Model-Context Protocol) Server**: It hosts and exposes `gemini-cli`'s powerful built-in tools (e.g., `google_web_search`, file system tools) via a standard, discoverable protocol. This allows the core `gemini-cli` model to invoke these tools as needed.

2.  **OpenAI-Compatible API Bridge**: It provides an endpoint compatible with the OpenAI Chat Completions API. This enables any third-party tool or application that supports the OpenAI API (such as [Open WebUI](https://github.com/open-webui/open-webui)) to seamlessly interact with the underlying Gemini model of `gemini-cli`, including full support for streaming responses.

## Core Design Philosophy

The server is built on a principle of **minimal modification and maximum reuse**. It is not a reimplementation of `gemini-cli`'s features but is instead intelligently built on top of the `@google/gemini-cli-core` package.

By reusing the `Config` and `GeminiClient` classes from the core package, the `mcp-server` inherits all of the essential business logic, tool execution capabilities, and configuration management of the main CLI. This design ensures behavioral consistency and simplifies maintenance and extension.

## Features

-   **Hosts Native `gemini-cli` Tools**: Exposes the built-in tools (file system operations, web fetching, web search, etc.) to the `gemini-cli` model via the MCP protocol.
-   **OpenAI API Compatibility**: Provides `/v1/chat/completions` and `/v1/models` endpoints, allowing third-party applications to interact with the Gemini model as if it were an OpenAI service.
-   **Streaming Support**: Fully supports streaming responses, pushing real-time generation results from the Gemini model to the client via Server-Sent Events (SSE).
-   **Flexible Model Configuration**: Allows a separate, default LLM model to be configured via an environment variable specifically for tools hosted by the server (e.g., for summarizing search results).
-   **Inherited Configuration & Authentication**: Automatically uses the same settings and authentication state as your main `gemini-cli` setup.
-   **Forced YOLO Mode**: Runs in a permanent "YOLO" mode, automatically approving all tool calls for streamlined, non-interactive use by other applications.

## Architecture & Interaction Flow

The `mcp-server` operates as a standalone component within the `gemini-cli` ecosystem with the following interaction flow:

1.  **Configuration Loading**: On startup, the server loads user and workspace `settings.json` files and reads environment variables, just like the main `gemini-cli` application, to initialize an instance of the `@google/gemini-cli-core` `Config` class.
2.  **Authentication**: The server **does not** handle its own authentication. It relies entirely on the established authentication state of `gemini-cli` (see the next section for details).
3.  **MCP Service**: It starts an MCP server, which the `gemini-cli` can connect to when it needs to discover and execute tools.
4.  **OpenAI Bridge**: It starts an Express web server that listens for API requests in the OpenAI format.
5.  **Request Handling**:
    -   When an OpenAI-formatted request is received, the server converts it into a format that `gemini-cli-core` can understand.
    -   It uses the reused `Config` instance to get a `GeminiClient`.
    -   A **new, isolated `GeminiChat` session** is created for each incoming API request to prevent conversation history from leaking between different clients.
    -   The request is sent to the Gemini API via the `GeminiClient`.
    -   If the Gemini API's response is streaming, the server transforms it into an OpenAI-compatible SSE stream; otherwise, it returns a complete JSON response.

## Authentication Mechanism

Crucially, the `mcp-server` **does not manage its own credentials**. It shares the exact same authentication mechanism as the main `gemini-cli` tool to ensure seamless and secure operation.

The source of authentication credentials follows the identical priority and lookup logic as `gemini-cli`:

-   **Cached Credentials**: If you have previously logged in through the interactive `gemini-cli` flow (e.g., `gcloud auth application-default login` or OAuth web login), the `mcp-server` will automatically use the cached credentials stored in `~/.config/gcloud` or `~/.gemini`.
-   **Environment Variables**: The server will look for and use standard Google Cloud and Gemini environment variables, such as:
    -   `GEMINI_API_KEY`
    -   `GOOGLE_APPLICATION_CREDENTIALS`
    -   `GOOGLE_CLOUD_PROJECT`

This means that as long as your `gemini-cli` itself is configured correctly and works, the `mcp-server` will be authorized automatically, with no extra authentication steps required.

## Important Security Note: YOLO Mode

The `mcp-server` is designed for non-interactive, programmatic use. As such, it runs with a permanent **YOLO (You Only Live Once) Mode** enabled (`approvalMode: ApprovalMode.YOLO`).

This means that any tool call requested by the model (e.g., `run_shell_command`, `replace`) will be **executed immediately without user confirmation**.

**Warning:** Be extremely cautious when exposing this server to a network. Any client that can reach the server will be able to execute tools with the same permissions as the user running the server process.

## Configuration Options

You can configure the server's behavior via command-line arguments and environment variables.

### Command-Line Arguments

-   `--port=<number>`: Specifies the port for the server to listen on.
    -   **Default**: `8765`
-   `--debug`: Enables detailed debug logging to the console.

### Environment Variables

-   `GEMINI_TOOLS_DEFAULT_MODEL`: Sets a default LLM model specifically for tools hosted by the server (like `google_web_search`).
    -   **Purpose**: When a tool needs to invoke an LLM during its execution (e.g., to summarize search results), it will use the model specified by this variable. This allows you to use a different (potentially faster or cheaper) model for tool execution than for the main chat.
    -   **Example**: `GEMINI_TOOLS_DEFAULT_MODEL=gemini-1.5-flash`

## Usage

### 1. Installation & Build

From the root of the `gemini-cli` project, ensure all dependencies are installed and then build the `mcp-server` package.

```bash
# From the project root
npm install
npm run build --workspace=@gemini-community/gemini-mcp-server
```

### 2. Starting the Server

You can start the server using the `npm run start` command from the root directory, targeting the workspace.

```bash
# Start the server with default configuration
npm run start --workspace=@gemini-community/gemini-mcp-server

# Start on a different port with debug mode enabled
npm run start --workspace=@gemini-community/gemini-mcp-server -- --port=9000 --debug

# Use a faster model for tool calls
GEMINI_TOOLS_DEFAULT_MODEL=gemini-1.5-flash npm run start --workspace=@gemini-community/gemini-mcp-server
```

When the server starts successfully, you will see output similar to this:

```
üöÄ Gemini CLI MCP Server and OpenAI Bridge are running on port 8765
   - MCP transport listening on http://localhost:8765/mcp
   - OpenAI-compatible endpoints available at http://localhost:8765/v1
‚öôÔ∏è  Using default model for tools: gemini-2.5-pro
```

### 3. Testing the Endpoints

You can use `curl` or any API client to test the server.

**Test OpenAI Chat Completions (Streaming)**:

```bash
curl -N http://localhost:8765/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gemini-pro",
    "messages": [{"role": "user", "content": "Tell me a short story about a robot who learns to paint."}],
    "stream": true
  }'
```

**Test OpenAI Chat Completions (Non-Streaming)**:

```bash
curl http://localhost:8765/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gemini-pro",
    "messages": [{"role": "user", "content": "Why is the sky blue?"}],
    "stream": false
  }'
```

## Telemetry, Terms of Service, and Privacy

### Telemetry

The `@gemini-community/gemini-mcp-server` **does not introduce any new telemetry or data collection mechanisms**.

It relies entirely on the OpenTelemetry (OTEL) system built into the `@google/gemini-cli-core` package. Therefore, all telemetry data (if enabled) will follow the main `gemini-cli` configuration and be sent to the destination specified in your `settings.json` file.

For details on how to configure and use telemetry, please refer to the [main Gemini CLI telemetry documentation](../../docs/telemetry.md).

### Terms of Service and Privacy Notice

Your use of this server is governed by the Terms of Service and Privacy Policies corresponding to the `gemini-cli` account type you are using for authentication. As a bridge, `@gemini-community/gemini-mcp-server` does not collect, store, or process any additional data of its own.

We strongly recommend you review the [main Gemini CLI Terms of Service and Privacy Notice documentation](../../docs/tos-privacy.md) for details applicable to your account.

---

### Developer Note: Regarding the package name `@gemini-community/gemini-mcp-server`

Please note that the name of this package, `@gemini-community/gemini-mcp-server`, indicates that it is a community-maintained package.

-   **Community Driven**: This package is part of the Gemini Community effort to extend the capabilities of `gemini-cli`.
-   **Publication**: This package may be published to a public npm registry under the `@gemini-community` scope. If you fork this project and wish to publish your own modified version, you **must** change the package name to your own scope (e.g., `@your-username/gemini-mcp-server`) to comply with npm's package naming policies and to avoid confusion.

----

# Gemini CLI - MCP / OpenAI Bridge Server (`@gemini-community/gemini-mcp-server`)

`@gemini-community/gemini-mcp-server` ÊòØ‰∏Ä‰∏™Â§öÂäüËÉΩÁöÑÊúçÂä°Âô®Â∫îÁî®Á®ãÂ∫èÔºåÊó®Âú®‰Ωú‰∏∫ `gemini-cli` ÁîüÊÄÅÁ≥ªÁªüÁöÑÂº∫Â§ßÊâ©Â±ï„ÄÇÂÆÉ‰∏ªË¶ÅÊâøÊãÖ‰∏§‰∏™Ê†∏ÂøÉËßíËâ≤Ôºö

1.  **MCP (Model-Context Protocol) ÊúçÂä°Âô®**: ÂÆÉ‰∏∫ `gemini-cli` ÊâòÁÆ°ÂíåÊö¥Èú≤‰∫Ü‰∏ÄÁ≥ªÂàóÂº∫Â§ßÁöÑÂÜÖÁΩÆÂ∑•ÂÖ∑Ôºà‰æãÂ¶Ç `google_web_search`ÔºâÔºåÂÖÅËÆ∏ `gemini-cli` ÁöÑÊ†∏ÂøÉÊ®°ÂûãÈÄöËøá‰∏Ä‰∏™Ê†áÂáÜÁöÑ„ÄÅÂèØÂèëÁé∞ÁöÑÂçèËÆÆÊù•Ë∞ÉÁî®Ëøô‰∫õÂ∑•ÂÖ∑„ÄÇ

2.  **OpenAI ÂÖºÂÆπÁöÑ API Ê°•Êé•Âô®**: ÂÆÉÊèê‰æõ‰∫Ü‰∏Ä‰∏™‰∏é OpenAI Chat Completions API ÂÖºÂÆπÁöÑÁ´ØÁÇπ„ÄÇËøô‰ΩøÂæó‰ªª‰ΩïÊîØÊåÅ OpenAI API ÁöÑÁ¨¨‰∏âÊñπÂ∑•ÂÖ∑ÊàñÂ∫îÁî®Á®ãÂ∫èÔºà‰æãÂ¶Ç [Open WebUI](https://github.com/open-webui/open-webui)ÔºâÈÉΩÂèØ‰ª•Êó†ÁºùÂú∞‰∏é `gemini-cli` ÁöÑÂ∫ïÂ±Ç Gemini Ê®°ÂûãËøõË°å‰∫§‰∫íÔºåÂåÖÊã¨Âà©Áî®ÊµÅÂºèÂìçÂ∫î„ÄÇ

## Ê†∏ÂøÉËÆæËÆ°ÁêÜÂøµ

Ëøô‰∏™ÊúçÂä°Âô®ÁöÑÊ†∏ÂøÉËÆæËÆ°ÂéüÂàôÊòØ **ÊúÄÂ∞èÂåñ‰øÆÊîπÂíåÊúÄÂ§ßÂåñÂ§çÁî®**„ÄÇÂÆÉÂπ∂‰∏çÊòØÂØπ `gemini-cli` ÂäüËÉΩÁöÑÈáçÊñ∞ÂÆûÁé∞ÔºåËÄåÊòØÂ∑ßÂ¶ôÂú∞ÊûÑÂª∫Âú® `@google/gemini-cli-core` ÂåÖ‰πã‰∏ä„ÄÇ

ÈÄöËøáÈáçÁî® `core` ÂåÖ‰∏≠ÁöÑ `Config` Âíå `GeminiClient` Á±ªÔºå`mcp-server` ÁªßÊâø‰∫Ü `gemini-cli` ÊâÄÊúâÁöÑÊ†∏ÂøÉ‰∏öÂä°ÈÄªËæë„ÄÅÂ∑•ÂÖ∑ÊâßË°åËÉΩÂäõÂíåÈÖçÁΩÆÁÆ°ÁêÜÊú∫Âà∂„ÄÇËøôÁßçËÆæËÆ°Á°Æ‰øù‰∫ÜË°å‰∏∫ÁöÑ‰∏ÄËá¥ÊÄßÔºåÂπ∂‰ΩøÂæóÁª¥Êä§ÂíåÊâ©Â±ïÂèòÂæóÊõ¥Âä†ÁÆÄÂçï„ÄÇ

## ÂäüËÉΩÁâπÊÄß

-   **ÊâòÁÆ°ÂéüÁîü `gemini-cli` Â∑•ÂÖ∑**: ÈÄöËøá MCP ÂçèËÆÆÔºåÂ∞Ü `gemini-cli` ÁöÑÂÜÖÁΩÆÂ∑•ÂÖ∑ÔºàÂ¶ÇÊñá‰ª∂Á≥ªÁªüÊìç‰Ωú„ÄÅÁΩëÈ°µÊäìÂèñ„ÄÅÁΩëÁªúÊêúÁ¥¢Á≠âÔºâÊö¥Èú≤Áªô `gemini-cli` Ê®°Âûã„ÄÇ
-   **OpenAI API ÂÖºÂÆπÊÄß**: Êèê‰æõ `/v1/chat/completions` Âíå `/v1/models` Á´ØÁÇπÔºåÂÖÅËÆ∏Á¨¨‰∏âÊñπÂ∫îÁî®Á®ãÂ∫èÂÉè‰∏é OpenAI ÂØπËØù‰∏ÄÊ†∑‰∏é Gemini Ê®°Âûã‰∫§‰∫í„ÄÇ
-   **ÊµÅÂºèÂìçÂ∫îÊîØÊåÅ**: ÂÆåÂÖ®ÊîØÊåÅÊµÅÂºèÂìçÂ∫îÔºåÂèØ‰ª•Â∞Ü Gemini Ê®°ÂûãÁöÑÂÆûÊó∂ÁîüÊàêÁªìÊûúÈÄöËøá SSE (Server-Sent Events) Êé®ÈÄÅÁªôÂÆ¢Êà∑Á´Ø„ÄÇ
-   **ÁÅµÊ¥ªÁöÑÊ®°ÂûãÈÖçÁΩÆ**: ÂÖÅËÆ∏ÈÄöËøáÁéØÂ¢ÉÂèòÈáè‰∏∫ÊúçÂä°Âô®ÊâòÁÆ°ÁöÑÂ∑•ÂÖ∑ÔºàÂ¶Ç `google_web_search`ÔºâÈÖçÁΩÆ‰∏Ä‰∏™ÁâπÂÆöÁöÑ„ÄÅÁã¨Á´ãÁöÑÈªòËÆ§ LLM Ê®°Âûã„ÄÇ

## Êû∂ÊûÑ‰∏é‰∫§‰∫íÊµÅÁ®ã

`mcp-server` ‰Ωú‰∏∫ `gemini-cli` ÁîüÊÄÅÁ≥ªÁªü‰∏≠ÁöÑ‰∏Ä‰∏™Áã¨Á´ãÁªÑ‰ª∂ÔºåÂÖ∂‰∫§‰∫íÊµÅÁ®ãÂ¶Ç‰∏ãÔºö

1.  **ÈÖçÁΩÆÂä†ËΩΩ**: ÊúçÂä°Âô®ÂêØÂä®Êó∂ÔºåÂÆÉ‰ºöÂÉè‰∏ª `gemini-cli` Â∫îÁî®‰∏ÄÊ†∑ÔºåÂä†ËΩΩÁî®Êà∑ÂíåÂ∑•‰ΩúÂå∫ÁöÑ `settings.json` Êñá‰ª∂ÔºåÂπ∂ËØªÂèñÁéØÂ¢ÉÂèòÈáèÊù•ÂàùÂßãÂåñ‰∏Ä‰∏™ `@google/gemini-cli-core` ÁöÑ `Config` ÂÆû‰æã„ÄÇ
2.  **ËÆ§ËØÅ**: ÊúçÂä°Âô®**‰∏çÂ§ÑÁêÜ**Ëá™Â∑±ÁöÑËÆ§ËØÅÊµÅÁ®ã„ÄÇÂÆÉÂÆåÂÖ®‰æùËµñ‰∫é `gemini-cli` Â∑≤ÁªèÂª∫Á´ãÁöÑËÆ§ËØÅÁä∂ÊÄÅÔºàËØ¶ÊÉÖËßÅ‰∏ã‰∏ÄËäÇÔºâ„ÄÇ
3.  **MCP ÊúçÂä°**: ÂÆÉÂêØÂä®‰∏Ä‰∏™ MCP ÊúçÂä°Âô®Ôºå`gemini-cli` Âú®ÈúÄË¶ÅÊó∂ÂèØ‰ª•ËøûÊé•Âà∞Ëøô‰∏™ÊúçÂä°Âô®Êù•ÂèëÁé∞ÂíåÊâßË°åÂ∑•ÂÖ∑„ÄÇ
4.  **OpenAI Ê°•Êé•**: ÂÆÉÂêØÂä®‰∏Ä‰∏™ Express Web ÊúçÂä°Âô®ÔºåÁõëÂê¨ OpenAI Ê†ºÂºèÁöÑ API ËØ∑Ê±Ç„ÄÇ
5.  **ËØ∑Ê±ÇÂ§ÑÁêÜ**:
    -   ÂΩìÊî∂Âà∞‰∏Ä‰∏™ OpenAI Ê†ºÂºèÁöÑËØ∑Ê±ÇÊó∂ÔºåÊúçÂä°Âô®‰ºöÂ∞ÜÂÖ∂ËΩ¨Êç¢‰∏∫ `gemini-cli-core` ÂèØ‰ª•ÁêÜËß£ÁöÑÊ†ºÂºè„ÄÇ
    -   ÂÆÉ‰ΩøÁî®Â§çÁî®ÁöÑ `Config` ÂÆû‰æãÊù•Ëé∑Âèñ‰∏Ä‰∏™ `GeminiClient`„ÄÇ
    -   ÈÄöËøá `GeminiClient` Â∞ÜËØ∑Ê±ÇÂèëÈÄÅÁªô Gemini API„ÄÇ
    -   Â¶ÇÊûú Gemini API ÁöÑÂìçÂ∫îÊòØÊµÅÂºèÁöÑÔºåÊúçÂä°Âô®‰ºöÂ∞ÜÂÖ∂ËΩ¨Êç¢‰∏∫ OpenAI ÂÖºÂÆπÁöÑ SSE ‰∫ã‰ª∂ÊµÅÔºõÂ¶ÇÊûúÊòØÈùûÊµÅÂºèÁöÑÔºåÂàôËøîÂõû‰∏Ä‰∏™ÂÆåÊï¥ÁöÑ JSON ÂìçÂ∫î„ÄÇ

## ËÆ§ËØÅÊú∫Âà∂

Ëá≥ÂÖ≥ÈáçË¶ÅÁöÑÊòØÔºå`mcp-server` **‰∏çÁÆ°ÁêÜËá™Â∑±ÁöÑËÆ§ËØÅÂá≠ÊçÆ**„ÄÇÂÆÉ‰∏é‰∏ª `gemini-cli` Â∑•ÂÖ∑ÂÖ±‰∫´Áõ∏ÂêåÁöÑËÆ§ËØÅÊú∫Âà∂Ôºå‰ª•Á°Æ‰øùÊó†ÁºùÂíåÂÆâÂÖ®ÁöÑÊìç‰Ωú„ÄÇ

ËÆ§ËØÅÂá≠ÊçÆÁöÑÊù•Ê∫êÈÅµÂæ™‰∏é `gemini-cli` ÂÆåÂÖ®Áõ∏ÂêåÁöÑ‰ºòÂÖàÁ∫ßÂíåÊü•ÊâæÈÄªËæëÔºö

-   **ÁºìÂ≠òÁöÑÂá≠ÊçÆ**: Â¶ÇÊûúÊÇ®‰πãÂâçÈÄöËøá `gemini-cli` ÁöÑ‰∫§‰∫íÂºèÊµÅÁ®ãÔºà‰æãÂ¶Ç `gcloud auth application-default login` Êàñ OAuth ÁΩëÈ°µÁôªÂΩïÔºâÁôªÂΩïËøáÔºå`mcp-server` ‰ºöËá™Âä®‰ΩøÁî®Â≠òÂÇ®Âú® `~/.config/gcloud` Êàñ `~/.gemini` ÁõÆÂΩï‰∏ãÁöÑÁºìÂ≠òÂá≠ÊçÆ„ÄÇ
-   **ÁéØÂ¢ÉÂèòÈáè**: ÊúçÂä°Âô®‰ºöÊü•ÊâæÂπ∂‰ΩøÁî®Ê†áÂáÜÁöÑ Google Cloud Âíå Gemini ÁéØÂ¢ÉÂèòÈáèÔºå‰æãÂ¶ÇÔºö
    -   `GEMINI_API_KEY`
    -   `GOOGLE_APPLICATION_CREDENTIALS`
    -   `GOOGLE_CLOUD_PROJECT`

ËøôÊÑèÂë≥ÁùÄÔºåÂè™Ë¶ÅÊÇ®ÁöÑ `gemini-cli` Êú¨Ë∫´ÈÖçÁΩÆÊ≠£Á°Æ‰∏îÂèØ‰ª•Â∑•‰ΩúÔºå`mcp-server` Â∞±ËÉΩËá™Âä®Ëé∑ÂæóÊéàÊùÉÔºåÊó†ÈúÄ‰ªª‰ΩïÈ¢ùÂ§ñÁöÑËÆ§ËØÅÊ≠•È™§„ÄÇ

## ÈÖçÁΩÆÈÄâÈ°π

ÊÇ®ÂèØ‰ª•ÈÄöËøáÂëΩ‰ª§Ë°åÂèÇÊï∞ÂíåÁéØÂ¢ÉÂèòÈáèÊù•ÈÖçÁΩÆÊúçÂä°Âô®ÁöÑË°å‰∏∫„ÄÇ

### ÂëΩ‰ª§Ë°åÂèÇÊï∞

-   `--port=<number>`: ÊåáÂÆöÊúçÂä°Âô®ÁõëÂê¨ÁöÑÁ´ØÂè£„ÄÇ
    -   **ÈªòËÆ§ÂÄº**: `8765`
-   `--debug`: ÂêØÁî®ËØ¶ÁªÜÁöÑË∞ÉËØïÊó•ÂøóËæìÂá∫„ÄÇ

### ÁéØÂ¢ÉÂèòÈáè

-   `GEMINI_TOOLS_DEFAULT_MODEL`: ‰∏∫ÊúçÂä°Âô®ÊâòÁÆ°ÁöÑÂ∑•ÂÖ∑ÔºàÂ¶Ç `google_web_search`ÔºâËÆæÁΩÆ‰∏Ä‰∏™ÈªòËÆ§ÁöÑ LLM Ê®°Âûã„ÄÇ
    -   **Áî®ÈÄî**: ÂΩì‰∏Ä‰∏™Â∑•ÂÖ∑Âú®ÊâßË°åËøáÁ®ã‰∏≠ÈúÄË¶ÅË∞ÉÁî® LLMÔºà‰æãÂ¶ÇÔºåÂØπÊêúÁ¥¢ÁªìÊûúËøõË°åÊÄªÁªìÔºâÊó∂ÔºåÂÆÉÂ∞Ü‰ΩøÁî®Ê≠§ÁéØÂ¢ÉÂèòÈáèÊåáÂÆöÁöÑÊ®°Âûã„ÄÇËøôÂÖÅËÆ∏ÊÇ®‰∏∫‰∏ªËÅäÂ§©ÂíåÂ∑•ÂÖ∑ÊâßË°å‰ΩøÁî®‰∏çÂêåÁöÑÊ®°ÂûãÔºå‰ªéËÄåÂèØËÉΩ‰ºòÂåñÊàêÊú¨ÂíåÈÄüÂ∫¶„ÄÇ
    -   **Á§∫‰æã**: `GEMINI_TOOLS_DEFAULT_MODEL=gemini-1.5-flash`

## ‰ΩøÁî®ÊñπÊ≥ï

### 1. ÂÆâË£Ö‰∏éÊûÑÂª∫

Âú® `gemini-cli` È°πÁõÆÁöÑÊ†πÁõÆÂΩï‰∏ãÔºåÁ°Æ‰øùÊâÄÊúâ‰æùËµñÂ∑≤ÂÆâË£ÖÔºåÂπ∂ÊûÑÂª∫ `mcp-server` ÂåÖ„ÄÇ

```bash
# Âú®È°πÁõÆÊ†πÁõÆÂΩïËøêË°å
npm install
npm run build --workspace=@gemini-community/gemini-mcp-server
```

### 2. ÂêØÂä®ÊúçÂä°Âô®

ÊÇ®ÂèØ‰ª•‰ΩøÁî® `npm run start` ÂëΩ‰ª§Êù•ÂêØÂä®ÊúçÂä°Âô®„ÄÇ

```bash
# ÂêØÂä®ÊúçÂä°Âô®Ôºå‰ΩøÁî®ÈªòËÆ§ÈÖçÁΩÆ
npm run start --workspace=@gemini-community/gemini-mcp-server

# Âú®‰∏çÂêåÁ´ØÂè£‰∏äÂêØÂä®ÔºåÂπ∂ÂêØÁî®Ë∞ÉËØïÊ®°Âºè
npm run start --workspace=@gemini-community/gemini-mcp-server -- --port=9000 --debug

# ‰ΩøÁî®‰∏Ä‰∏™Êõ¥Âø´ÁöÑÊ®°ÂûãËøõË°åÂ∑•ÂÖ∑Ë∞ÉÁî®
GEMINI_TOOLS_DEFAULT_MODEL=gemini-1.5-flash npm run start --workspace=@gemini-community/gemini-mcp-server
```

ÊúçÂä°Âô®ÊàêÂäüÂêØÂä®ÂêéÔºåÊÇ®Â∞ÜÁúãÂà∞Á±ª‰ºº‰ª•‰∏ãÁöÑËæìÂá∫Ôºö

```
üöÄ Gemini CLI MCP Server and OpenAI Bridge are running on port 8765
   - MCP transport listening on http://localhost:8765/mcp
   - OpenAI-compatible endpoints available at http://localhost:8765/v1
‚öôÔ∏è  Using default model for tools: gemini-2.5-pro
```

### 3. ÊµãËØïÁ´ØÁÇπ

ÊÇ®ÂèØ‰ª•‰ΩøÁî® `curl` Êàñ‰ªª‰Ωï API ÂÆ¢Êà∑Á´ØÊù•ÊµãËØïÊúçÂä°Âô®„ÄÇ

**ÊµãËØï OpenAI Chat Completions (ÊµÅÂºè)**:

```bash
curl -N http://localhost:8765/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gemini-pro",
    "messages": [{"role": "user", "content": "Tell me a short story about a robot who learns to paint."}],
    "stream": true
  }'
```

## ÈÅ•Êµã„ÄÅÊúçÂä°Êù°Ê¨æÂíåÈöêÁßÅ

### ÈÅ•Êµã (Telemetry)

`@google/gemini-mcp-server` Êú¨Ë∫´**‰∏çÂºïÂÖ•‰ªª‰ΩïÊñ∞ÁöÑÈÅ•ÊµãÊàñÊï∞ÊçÆÊî∂ÈõÜÊú∫Âà∂**„ÄÇ

ÂÆÉÂÆåÂÖ®‰æùËµñ‰∫é `@google/gemini-cli-core` ÂåÖ‰∏≠ÂÜÖÁΩÆÁöÑ OpenTelemetry (OTEL) Á≥ªÁªü„ÄÇÂõ†Ê≠§ÔºåÊâÄÊúâÁöÑÈÅ•ÊµãÊï∞ÊçÆÔºàÂ¶ÇÊûúÂêØÁî®ÔºâÈÉΩÂ∞ÜÈÅµÂæ™ `gemini-cli` ÁöÑ‰∏ªÈÖçÁΩÆÔºåÂπ∂Ë¢´ÂèëÈÄÅÂà∞ `settings.json` Êñá‰ª∂‰∏≠ÊåáÂÆöÁöÑÁõÆÊ†á„ÄÇ

ÂÖ≥‰∫éÂ¶Ç‰ΩïÈÖçÁΩÆÂíå‰ΩøÁî®ÈÅ•ÊµãÔºåËØ∑ÂèÇÈòÖ[‰∏ª Gemini CLI ÈÅ•ÊµãÊñáÊ°£](../../docs/telemetry.md)„ÄÇ

### ÊúçÂä°Êù°Ê¨æ (Terms of Service) ÂíåÈöêÁßÅÂ£∞Êòé (Privacy Notice)

Êú¨ÊúçÂä°Âô®ÁöÑ‰ΩøÁî®ÂèóÂà∂‰∫éÊÇ®Áî®‰∫éËÆ§ËØÅÁöÑ `gemini-cli` Ë¥¶Êà∑Á±ªÂûãÊâÄÂØπÂ∫îÁöÑÊúçÂä°Êù°Ê¨æÂíåÈöêÁßÅÊîøÁ≠ñ„ÄÇ`@google/gemini-mcp-server` ‰Ωú‰∏∫‰∏Ä‰∏™Ê°•Êé•Â∑•ÂÖ∑ÔºåÊú¨Ë∫´‰∏çÊî∂ÈõÜ„ÄÅÂ≠òÂÇ®ÊàñÂ§ÑÁêÜÊÇ®ÁöÑ‰ªª‰ΩïÈ¢ùÂ§ñÊï∞ÊçÆ„ÄÇ

Êàë‰ª¨Âº∫ÁÉàÂª∫ËÆÆÊÇ®Êü•ÈòÖ[‰∏ª Gemini CLI ÊúçÂä°Êù°Ê¨æÂíåÈöêÁßÅÂ£∞ÊòéÊñáÊ°£](../../docs/tos-privacy.md)‰ª•‰∫ÜËß£ÈÄÇÁî®‰∫éÊÇ®Ë¥¶Êà∑ÁöÑËØ¶ÁªÜ‰ø°ÊÅØ„ÄÇ

---

### ÂºÄÂèëËÄÖËØ¥ÊòéÔºöÂÖ≥‰∫éÂåÖÂêç `@google/gemini-mcp-server`

ËØ∑Ê≥®ÊÑèÔºåÊú¨ÂåÖÁöÑÂêçÁß∞ `@google/gemini-mcp-server` ÂèçÊò†‰∫ÜÂÆÉ‰Ωú‰∏∫ÂÆòÊñπ `google-gemini/gemini-cli` ÁöÑforkÈ°πÁõÆÁöÑ monorepo ÂÜÖÈÉ®ÁªÑ‰ª∂ÁöÑÊù•Ê∫ê„ÄÇ

-   **ÂÜÖÈÉ®ÂëΩÂêç**: Âú® `gemini-cli` È°πÁõÆÁöÑÊ∫ê‰ª£Á†ÅÂíåÂ∑•‰ΩúÂå∫‰∏≠ÔºåÊ≠§ÂëΩÂêçÊòØÂÜÖÈÉ®‰∏ÄËá¥ÁöÑ„ÄÇ
-   **ÈùûÁã¨Á´ãÂèëÂ∏É**: Ê≠§ÂåÖ**‰∏ç‰ºö**‰Ωú‰∏∫‰∏Ä‰∏™Áã¨Á´ãÁöÑ„ÄÅÁâàÊú¨ÂåñÁöÑÂåÖÂèëÂ∏ÉÂà∞ÂÖ¨ÂÖ± npm registry ‰∏ä„ÄÇÂ¶ÇÊûúÊÇ® fork Êú¨È°πÁõÆÂπ∂Â∏åÊúõÁã¨Á´ãÂèëÂ∏ÉÊÇ®ÁöÑ‰øÆÊîπÁâàÊú¨ÔºåÊÇ®**ÂøÖÈ°ª**Â∞ÜÂåÖÂêçÊõ¥Êîπ‰∏∫ÊÇ®Ëá™Â∑±ÁöÑ scopeÔºà‰æãÂ¶Ç `@your-username/gemini-mcp-server`ÔºâÔºå‰ª•ÈÅµÂÆà npm ÁöÑÂåÖÂëΩÂêçËßÑËåÉÂπ∂ÈÅøÂÖçÊ∑∑Ê∑Ü„ÄÇ
